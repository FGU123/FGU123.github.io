<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[RocketMQ-Consumer消息消费]]></title>
    <url>%2F2019%2F07%2F27%2FRocketMQ-Message-Consumer%2F</url>
    <content type="text"><![CDATA[消息被Producer发送到Broker后，消息消费端consumer既可请求broker拉取消息并开展消费处理。消息消费以组（Consumer Group）的模式开展，一个消费组内可以包含多个消费者，每一个Consumer Group可订阅多个Topic，Consumer Group之间有两种消费模式：广播模式（BROADCASTING）、集群模式（CLUSTERING）。 集群模式：主题下的同一条消息只允许被其中一个消费者消费。 广播模式：主题下的同一条消息将被集群内的所有消费者消费一次。 Broker与Consumer之间的消息传送方式也有两种： 拉模式：Consumer主动向Broker请求拉取消息 推模式：Broker将消息推送给Consumer，事实上推模式的实现基于拉模式，在拉模式上包装一层而已，详细实现下文展示。先来看一下，Consumer的启动过程。 Consumer启动12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485public synchronized void start() throws MQClientException &#123; switch (this.serviceState) &#123; case CREATE_JUST: log.info("the consumer [&#123;&#125;] start beginning. messageModel=&#123;&#125;, isUnitMode=&#123;&#125;", this.defaultMQPushConsumer.getConsumerGroup(), this.defaultMQPushConsumer.getMessageModel(), this.defaultMQPushConsumer.isUnitMode()); this.serviceState = ServiceState.START_FAILED; // 1. 检验consumer配置 this.checkConfig(); this.copySubscription(); if (this.defaultMQPushConsumer.getMessageModel() == MessageModel.CLUSTERING) &#123; this.defaultMQPushConsumer.changeInstanceNameToPID(); &#125; // 2. 实例化mqClientFactory this.mQClientFactory = MQClientManager.getInstance().getAndCreateMQClientInstance(this.defaultMQPushConsumer, this.rpcHook); // 3. 设置rebalance相关属性 this.rebalanceImpl.setConsumerGroup(this.defaultMQPushConsumer.getConsumerGroup()); this.rebalanceImpl.setMessageModel(this.defaultMQPushConsumer.getMessageModel()); this.rebalanceImpl.setAllocateMessageQueueStrategy(this.defaultMQPushConsumer.getAllocateMessageQueueStrategy()); this.rebalanceImpl.setmQClientFactory(this.mQClientFactory); // 4. 设置pullAPIWrapper的消息过滤钩子 this.pullAPIWrapper = new PullAPIWrapper( mQClientFactory, this.defaultMQPushConsumer.getConsumerGroup(), isUnitMode()); this.pullAPIWrapper.registerFilterMessageHook(filterMessageHookList); // 5. 设置consumer的offsetStore参数 if (this.defaultMQPushConsumer.getOffsetStore() != null) &#123; this.offsetStore = this.defaultMQPushConsumer.getOffsetStore(); &#125; else &#123; switch (this.defaultMQPushConsumer.getMessageModel()) &#123; case BROADCASTING: this.offsetStore = new LocalFileOffsetStore(this.mQClientFactory, this.defaultMQPushConsumer.getConsumerGroup()); break; case CLUSTERING: this.offsetStore = new RemoteBrokerOffsetStore(this.mQClientFactory, this.defaultMQPushConsumer.getConsumerGroup()); break; default: break; &#125; this.defaultMQPushConsumer.setOffsetStore(this.offsetStore); &#125; this.offsetStore.load(); // 6. 根据consumer设置的messageListner不同子类实例化不同的consumeMessageService,然后启动该类代表的线程 if (this.getMessageListenerInner() instanceof MessageListenerOrderly) &#123; this.consumeOrderly = true; this.consumeMessageService = new ConsumeMessageOrderlyService(this, (MessageListenerOrderly) this.getMessageListenerInner()); &#125; else if (this.getMessageListenerInner() instanceof MessageListenerConcurrently) &#123; this.consumeOrderly = false; this.consumeMessageService = new ConsumeMessageConcurrentlyService(this, (MessageListenerConcurrently) this.getMessageListenerInner()); &#125; this.consumeMessageService.start(); // 7. 注册当前的consumer boolean registerOK = mQClientFactory.registerConsumer(this.defaultMQPushConsumer.getConsumerGroup(), this); if (!registerOK) &#123; this.serviceState = ServiceState.CREATE_JUST; this.consumeMessageService.shutdown(); throw new MQClientException("The consumer group[" + this.defaultMQPushConsumer.getConsumerGroup() + "] has been created before, specify another name please." + FAQUrl.suggestTodo(FAQUrl.GROUP_NAME_DUPLICATE_URL), null); &#125; // 8. 启动各种线程任务（这里还启动了netty客户端） mQClientFactory.start(); log.info("the consumer [&#123;&#125;] start OK.", this.defaultMQPushConsumer.getConsumerGroup()); this.serviceState = ServiceState.RUNNING; break; case RUNNING: case START_FAILED: case SHUTDOWN_ALREADY: throw new MQClientException("The PushConsumer service state not OK, maybe started once, " + this.serviceState + FAQUrl.suggestTodo(FAQUrl.CLIENT_SERVICE_NOT_OK), null); default: break; &#125; this.updateTopicSubscribeInfoWhenSubscriptionChanged(); this.mQClientFactory.checkClientInBroker(); this.mQClientFactory.sendHeartbeatToAllBrokerWithLock(); this.mQClientFactory.rebalanceImmediately(); //9、直接执行reblance逻辑(也就是决定consumer的负载均衡)&#125; 具体步骤如下： 1.校验consumer的配置其实就是校验consumer设置的值是否正确，consumer重要参数如下： messageModel:消费消息的模式(广播模式和集群模式） consumeFromWhere:选择起始消费位置的方式 allocateMessageQueueStrategy:分配具体messageQuene的策略子类。（负载均衡逻辑实现的关键类） consumeThreadMin：消费消息线程池的最小核心线程数(默认20) consumeThreadMax：最大线程数（默认64） pullInterval：拉取消息的间隔，默认是0 consumeMessageBatchMaxSize：每批次消费消息的条数，默认为1 pullBatchSize：每批次拉取消息的条数，默认32 2.例化mQClientFactory我们从实例化mQClientFactory代码可以看出：一个consumer客户端只会对应一个mQClientFactory（因为factoryTable存放的mQClientFactory是以客户端作为key存放的），也就是说一个应用节点只会有一个mQClientFactory实例。1234567891011121314public MQClientInstance getAndCreateMQClientInstance(final ClientConfig clientConfig, RPCHook rpcHook) &#123; String clientId = clientConfig.buildMQClientId(); //factoryTable存放的就是client的实例，key为clientid。 MQClientInstance instance = this.factoryTable.get(clientId); if (null == instance) &#123; instance =new MQClientInstance(clientConfig.cloneClientConfig(),this.factoryIndexGenerator.getAndIncrement(), clientId, rpcHook); MQClientInstance prev = this.factoryTable.putIfAbsent(clientId, instance); if (prev != null) &#123; instance = prev; &#125; else &#123; &#125; &#125; return instance;&#125; 3.设置reblance相关属性也就是设置该consumer对应的负载均衡策略需要的相关参数，例如messageModel、allocateMessageQueueStrategy、实例化mQClientFactory等。 4.设置pullAPIWrapper的消息过滤钩子此步作用在于可以由用户自己指定consumer过滤消息的策略，只需要调用consumer的registerFilterMessageHook，将自己实现的过滤消息的FilterMessageHook设置给consumer即可。 5.设置consumer的offsetStore也就是设置consumer使用哪种处理消息消费位置offset的类。如果是广播消费模式，则选择LocalFileOffsetStore；如果是集群消费模式，则选择RemoteBrokerOffsetStore； 6.设置consumer的consumeMessageService根据consumer设置的MessageListener来决定使用具体ConsumeMessageService。如果是MessageListenerOrderly，则使用代表顺序消息消费的service：ConsumeMessageOrderlyService；如果是MessageListenerConcurrently，则使用非顺序消息service：ConsumeMessageConcurrentlyService。 PS：此步还调用了consumeMessageService的start方法，这里只是启动了一个定时线程去做cleanExpireMsg的操作，并没有启动消费消息的线程。 7.注册当前的consumer这里只是将当前consumer放到了一个缓存map中，key为consumerGroup的名称。 8.mQClientFactory.startorg.apache.rocketmq.client.impl.factory.MQClientInstance.start()12345678910111213141516171819202122232425262728293031323334public void start() throws MQClientException &#123; synchronized (this) &#123; switch (this.serviceState) &#123; case CREATE_JUST: this.serviceState = ServiceState.START_FAILED; // If not specified,looking address from name server if (null == this.clientConfig.getNamesrvAddr()) &#123; this.mQClientAPIImpl.fetchNameServerAddr(); &#125; // Start request-response channel this.mQClientAPIImpl.start(); // Start various schedule tasks this.startScheduledTask(); // Start pull service this.pullMessageService.start(); // Start rebalance service this.rebalanceService.start(); // Start push service this.defaultMQProducer.getDefaultMQProducerImpl().start(false); log.info("the client factory [&#123;&#125;] start OK", this.clientId); this.serviceState = ServiceState.RUNNING; break; case RUNNING: break; case SHUTDOWN_ALREADY: break; case START_FAILED: throw new MQClientException("The Factory object[" + this.getClientId() + "] has been created before, and failed.", null); default: break; &#125; &#125;&#125; this.mQClientAPIImpl.start()启动了netty客户端，用于处理Consumer的网络请求。this.startScheduledTask()启动了一个线程池来安排执行各种定时任务，包括以下： MQClientInstance.this.mQClientAPIImpl.fetchNameServerAddr(); MQClientInstance.this.updateTopicRouteInfoFromNameServer(); MQClientInstance.this.cleanOfflineBroker(); MQClientInstance.this.sendHeartbeatToAllBrokerWithLock(); MQClientInstance.this.persistAllConsumerOffset(); MQClientInstance.this.adjustThreadPool();至于，this.pullMessageService.start() 与 this.rebalanceService.start() 则启动了另一类独立的线程任务，分别是拉取消息及重新负载均衡。 9.触发重新负载均衡mQClientFactory.rebalanceImmediately()，点进去看，实际上是调了rebalanceService.wakeup()，唤醒第8步起的重新负载均衡线程。this.rebalanceService.start()点进去看，有一个volatile变量stopped控制rebalanceService是否进入doRebalance()操作。org.apache.rocketmq.common.ServiceThread.start()12345678910public void start() &#123; log.info("Try to start service thread:&#123;&#125; started:&#123;&#125; lastThread:&#123;&#125;", getServiceName(), started.get(), thread); if (!started.compareAndSet(false, true)) &#123; return; &#125; stopped = false; this.thread = new Thread(this, getServiceName()); this.thread.setDaemon(isDaemon); this.thread.start();&#125; org.apache.rocketmq.client.impl.consumer.RebalanceService.run()12345678910public void run() &#123; log.info(this.getServiceName() + " service started"); while (!this.isStopped()) &#123; this.waitForRunning(waitInterval); this.mqClientFactory.doRebalance(); &#125; log.info(this.getServiceName() + " service end"); &#125; 消息拉取过程PullMessageService负责从Broker拉取消息，run()的逻辑看起来很简单：一个while循环不停地从阻塞队列中获取pullRequest，然后执行pullMessage()，这里再次出现了volatile布尔变量stopped，这是一种通用的设计技巧，将stopped声明为volatile，每执行一次业务逻辑检查一下其运行状态是否为停止，可以通过其他线程将stopped设置为true从而停止该线程。this.pullRequestQueue是一个存放消息拉取请求的阻塞队列，如果PullRequestQueue为空，则线程将被阻塞，直到队列里有拉取请求可以take出来。org.apache.rocketmq.client.impl.consumer.PullMessageService.run()123456789101112131415public void run() &#123; log.info(this.getServiceName() + " service started"); while (!this.isStopped()) &#123; try &#123; PullRequest pullRequest = this.pullRequestQueue.take(); this.pullMessage(pullRequest); &#125; catch (InterruptedException ignored) &#123; &#125; catch (Exception e) &#123; log.error("Pull Message Service Run Method exception", e); &#125; &#125; log.info(this.getServiceName() + " service end");&#125; 那么问题来了，PullRequest是什么时候被放进去PullRequestQueue里边的呢？接下来详细分析过程。 PullRequestQueue这个pullRequestQueue是PullMessageService的私有属性，它存放的PullRequest又是什么结构呢？org.apache.rocketmq.client.impl.consumer.PullRequest123456789public class PullRequest &#123; private String consumerGroup; private MessageQueue messageQueue; private ProcessQueue processQueue; private long nextOffset; private boolean lockedFirst = false; ...&#125; 从PullRequest类结构看出，pullRequestQueue存放的pullRequest封装的是每一个消费者群组consumerGroup以及对应的消费队列messageQuene，还有消费队列的快照processQueue。 接着，我们跟踪一下其put方法在哪里调用。可以看到，只在PullMessageService.executePullRequestImmediately(PullRequest)方法里边有直接调用org.apache.rocketmq.client.impl.consumer.PullMessageService.executePullRequestImmediately(PullRequest)1234567public void executePullRequestImmediately(final PullRequest pullRequest) &#123; try &#123; this.pullRequestQueue.put(pullRequest); &#125; catch (InterruptedException e) &#123; log.error("executePullRequestImmediately pullRequestQueue.put", e); &#125;&#125; 那么，我们继续跟踪这个executePullRequestImmediately方法的调用链，就会发现，主要有两类调用入口：Rebalance.run()、DefaultMQPushConsumerImpl.pullMessage的结果回调PullCallback。 1. Rebalance.run()RebalanceService顾名思义，就是针对consumer端要消费哪些messageQuene来做重新负载均衡的策略。当consumer集群某个节点挂了，则要考虑重新负载均衡rebalance，将messageQuene重新按照存活的consumer节点进行分配。org.apache.rocketmq.client.impl.consumer.RebalanceImpl.updateProcessQueueTableInRebalance(String, Set, boolean)123456789101112131415161718192021222324252627282930313233343536373839private boolean updateProcessQueueTableInRebalance(final String topic, final Set&lt;MessageQueue&gt; mqSet, final boolean isOrder) &#123; ... List&lt;PullRequest&gt; pullRequestList = new ArrayList&lt;PullRequest&gt;(); for (MessageQueue mq : mqSet) &#123; if (!this.processQueueTable.containsKey(mq)) &#123; if (isOrder &amp;&amp; !this.lock(mq)) &#123; log.warn("doRebalance, &#123;&#125;, add a new mq failed, &#123;&#125;, because lock failed", consumerGroup, mq); continue; &#125; this.removeDirtyOffset(mq); ProcessQueue pq = new ProcessQueue(); long nextOffset = this.computePullFromWhere(mq); if (nextOffset &gt;= 0) &#123; ProcessQueue pre = this.processQueueTable.putIfAbsent(mq, pq); if (pre != null) &#123; log.info("doRebalance, &#123;&#125;, mq already exists, &#123;&#125;", consumerGroup, mq); &#125; else &#123; log.info("doRebalance, &#123;&#125;, add a new mq, &#123;&#125;", consumerGroup, mq); PullRequest pullRequest = new PullRequest(); pullRequest.setConsumerGroup(consumerGroup); pullRequest.setNextOffset(nextOffset); pullRequest.setMessageQueue(mq); pullRequest.setProcessQueue(pq); pullRequestList.add(pullRequest); // 先把需要重新分配的pullRequest放进一个List changed = true; &#125; &#125; else &#123; log.warn("doRebalance, &#123;&#125;, add new mq failed, &#123;&#125;", consumerGroup, mq); &#125; &#125; &#125; this.dispatchPullRequest(pullRequestList); return changed;&#125; 123456public void dispatchPullRequest(List&lt;PullRequest&gt; pullRequestList) &#123; for (PullRequest pullRequest : pullRequestList) &#123; this.defaultMQPushConsumerImpl.executePullRequestImmediately(pullRequest); // 这里遍历pullRequest，逐个把pullRequest加入到pullRequestQueue log.info("doRebalance, &#123;&#125;, add a new pull request &#123;&#125;", consumerGroup, pullRequest); &#125;&#125; 前面consumer.start过程中，RebalanceService线程会随之启动执行，那么可以理解，当consumer一启动，相应的pullRequestQueue就会存放有pullRequest对象了。 2. PullCallbackDefaultMQPushConsumerImpl.pullMessage方法里面定义了拉取结果的回调PullCallback，我们得知在PullCallback的onSuccess和onException中调用了pullRequestQueue的put方法。也就是说，RocketMQ保证了每次拉完消息之后都会调用pullRequestQueue的put逻辑。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102 PullCallback pullCallback = new PullCallback() &#123; @Override public void onSuccess(PullResult pullResult) &#123; if (pullResult != null) &#123; pullResult = DefaultMQPushConsumerImpl.this.pullAPIWrapper.processPullResult(pullRequest.getMessageQueue(), pullResult, subscriptionData); switch (pullResult.getPullStatus()) &#123; case FOUND: long prevRequestOffset = pullRequest.getNextOffset(); pullRequest.setNextOffset(pullResult.getNextBeginOffset()); long pullRT = System.currentTimeMillis() - beginTimestamp; DefaultMQPushConsumerImpl.this.getConsumerStatsManager().incPullRT(pullRequest.getConsumerGroup(), pullRequest.getMessageQueue().getTopic(), pullRT); long firstMsgOffset = Long.MAX_VALUE; if (pullResult.getMsgFoundList() == null || pullResult.getMsgFoundList().isEmpty()) &#123; DefaultMQPushConsumerImpl.this.executePullRequestImmediately(pullRequest); // 触发调用pullRequestQueue的put逻辑 &#125; else &#123; firstMsgOffset = pullResult.getMsgFoundList().get(0).getQueueOffset(); DefaultMQPushConsumerImpl.this.getConsumerStatsManager().incPullTPS(pullRequest.getConsumerGroup(), pullRequest.getMessageQueue().getTopic(), pullResult.getMsgFoundList().size()); boolean dispatchToConsume = processQueue.putMessage(pullResult.getMsgFoundList()); DefaultMQPushConsumerImpl.this.consumeMessageService.submitConsumeRequest( pullResult.getMsgFoundList(), processQueue, pullRequest.getMessageQueue(), dispatchToConsume); if (DefaultMQPushConsumerImpl.this.defaultMQPushConsumer.getPullInterval() &gt; 0) &#123; DefaultMQPushConsumerImpl.this.executePullRequestLater(pullRequest, DefaultMQPushConsumerImpl.this.defaultMQPushConsumer.getPullInterval()); // 触发调用pullRequestQueue的put逻辑 &#125; else &#123; DefaultMQPushConsumerImpl.this.executePullRequestImmediately(pullRequest); // 触发调用pullRequestQueue的put逻辑 &#125; &#125; if (pullResult.getNextBeginOffset() &lt; prevRequestOffset || firstMsgOffset &lt; prevRequestOffset) &#123; log.warn( "[BUG] pull message result maybe data wrong, nextBeginOffset: &#123;&#125; firstMsgOffset: &#123;&#125; prevRequestOffset: &#123;&#125;", pullResult.getNextBeginOffset(), firstMsgOffset, prevRequestOffset); &#125; break; case NO_NEW_MSG: pullRequest.setNextOffset(pullResult.getNextBeginOffset()); DefaultMQPushConsumerImpl.this.correctTagsOffset(pullRequest); DefaultMQPushConsumerImpl.this.executePullRequestImmediately(pullRequest); // 触发调用pullRequestQueue的put逻辑 break; case NO_MATCHED_MSG: pullRequest.setNextOffset(pullResult.getNextBeginOffset()); DefaultMQPushConsumerImpl.this.correctTagsOffset(pullRequest); DefaultMQPushConsumerImpl.this.executePullRequestImmediately(pullRequest); // 触发调用pullRequestQueue的put逻辑 break; case OFFSET_ILLEGAL: log.warn("the pull request offset illegal, &#123;&#125; &#123;&#125;", pullRequest.toString(), pullResult.toString()); pullRequest.setNextOffset(pullResult.getNextBeginOffset()); pullRequest.getProcessQueue().setDropped(true); DefaultMQPushConsumerImpl.this.executeTaskLater(new Runnable() &#123; @Override public void run() &#123; try &#123; DefaultMQPushConsumerImpl.this.offsetStore.updateOffset(pullRequest.getMessageQueue(), pullRequest.getNextOffset(), false); DefaultMQPushConsumerImpl.this.offsetStore.persist(pullRequest.getMessageQueue()); DefaultMQPushConsumerImpl.this.rebalanceImpl.removeProcessQueue(pullRequest.getMessageQueue()); log.warn("fix the pull request offset, &#123;&#125;", pullRequest); &#125; catch (Throwable e) &#123; log.error("executeTaskLater Exception", e); &#125; &#125; &#125;, 10000); break; default: break; &#125; &#125; &#125; @Override public void onException(Throwable e) &#123; if (!pullRequest.getMessageQueue().getTopic().startsWith(MixAll.RETRY_GROUP_TOPIC_PREFIX)) &#123; log.warn("execute the pull request exception", e); &#125;// 触发调用pullRequestQueue的put逻辑 DefaultMQPushConsumerImpl.this.executePullRequestLater(pullRequest, PULL_TIME_DELAY_MILLS_WHEN_EXCEPTION); &#125; 根据以上两个入口我们可以得出结论：当consumer启动时，RebalanceService使得pullRequestQueue有值，PullMessageService的线程不停地从pullRequestQueue中take messageQuene拉取消息处理，处理完之后继续往pullRequestQueue存放messageQuene，从而使得pullRequestQueue不会因为没有值而阻塞。换句话说，pullRequestQueue每次take完一次，都会再继续put messageQuene，将下一次要拉取的pullRequest再次放到pullRequestQueue中，而拉取消息实际又是一个while循环不停去拉取，这样就保证了消费消息的及时性，使得每个Consumer节点仅有一个消息拉取线程负责所有消费者的消息拉取的情况下，不会产生性能瓶颈。 OK，到此，总算完成了从阻塞队列pullRequestQueue中拿pullRequest的过程了，接下来，就是根据拿到的pullRequest来进行拉取消息pullMessage了。 this.pullMessage(pullRequest)点进去一直找到实现类的方法org.apache.rocketmq.client.impl.consumer.DefaultMQPushConsumerImpl.pullMessage(PullRequest)，这个方法篇幅比较长，下面逐一拆解此处以获取订阅信息作为拉取消息的正式开始点，那么，在此之前，其实还有两类的关于当前处理队列（processQueue）的状态校验及相关操作首先执行：1.校验并保证处理队列当前执行状态正常，2. 流控。 processQueue执行状态校验关于为什么会有ProcessQueue这个数据结构，个人理解，是因为MessageQueue的消费处理其实是并发进行的，那么我们并不能同步获取消息的处理进度，但是本次拉取消息的推进需要根据上一次消费进度来进行，于是就有了ProcessQueue作为MessageQueue的消费处理进度快照。这个类的结构，主要是一个TreeMap及一个读写锁，TreeMap里以MessageQueue的Offset作为Key，以消息内容的引用为Value（所谓快照），保存所有从MessageQueue获取到，但是还未被处理的消息；读写锁的作用是控制多线程下对TreeMap对象的并发访问。从pullRequest中获取ProcessQueue，如果processQueue当前状态未被丢弃，则更新ProcessQueue的lastPullTimestamp为当前时间戳；如果当前消费者被挂起，则将拉取任务延迟指定时间（PULL_TIME_DELAY_MILLS_WHEN_SUSPEND，1s）后再次放入到PullMessageService的拉取任务队列中，结束本次消息拉取。org.apache.rocketmq.client.impl.consumer.DefaultMQPushConsumerImpl.pullMessage(PullRequest)123456789101112131415161718192021final ProcessQueue processQueue = pullRequest.getProcessQueue();if (processQueue.isDropped()) &#123; log.info("the pull request[&#123;&#125;] is dropped.", pullRequest.toString()); return; &#125; pullRequest.getProcessQueue().setLastPullTimestamp(System.currentTimeMillis()); try &#123; this.makeSureStateOK(); &#125; catch (MQClientException e) &#123; log.warn("pullMessage exception, consumer state not ok", e); this.executePullRequestLater(pullRequest, PULL_TIME_DELAY_MILLS_WHEN_EXCEPTION); return; &#125; if (this.isPause()) &#123; log.warn("consumer was paused, execute pull request later. instanceName=&#123;&#125;, group=&#123;&#125;", this.defaultMQPushConsumer.getInstanceName(), this.defaultMQPushConsumer.getConsumerGroup()); this.executePullRequestLater(pullRequest, PULL_TIME_DELAY_MILLS_WHEN_SUSPEND); return; &#125; 流控RocketMQ的消息拉取过程的流量控制，是让consumer根据自身的消息处理速度调整获取消息的操作速度，采取的流控处理做法是作延迟一段时间(默认50ms)后消费处理，流控主要从3种维度进行：消息消费数量、消息大小、偏移量间隔。org.apache.rocketmq.client.impl.consumer.DefaultMQPushConsumerImpl.pullMessage(PullRequest)12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455long cachedMessageCount = processQueue.getMsgCount().get(); long cachedMessageSizeInMiB = processQueue.getMsgSize().get() / (1024 * 1024); if (cachedMessageCount &gt; this.defaultMQPushConsumer.getPullThresholdForQueue()) &#123; this.executePullRequestLater(pullRequest, PULL_TIME_DELAY_MILLS_WHEN_FLOW_CONTROL); if ((queueFlowControlTimes++ % 1000) == 0) &#123; log.warn( "the cached message count exceeds the threshold &#123;&#125;, so do flow control, minOffset=&#123;&#125;, maxOffset=&#123;&#125;, count=&#123;&#125;, size=&#123;&#125; MiB, pullRequest=&#123;&#125;, flowControlTimes=&#123;&#125;", this.defaultMQPushConsumer.getPullThresholdForQueue(), processQueue.getMsgTreeMap().firstKey(), processQueue.getMsgTreeMap().lastKey(), cachedMessageCount, cachedMessageSizeInMiB, pullRequest, queueFlowControlTimes); &#125; return; &#125; if (cachedMessageSizeInMiB &gt; this.defaultMQPushConsumer.getPullThresholdSizeForQueue()) &#123; this.executePullRequestLater(pullRequest, PULL_TIME_DELAY_MILLS_WHEN_FLOW_CONTROL); if ((queueFlowControlTimes++ % 1000) == 0) &#123; log.warn( "the cached message size exceeds the threshold &#123;&#125; MiB, so do flow control, minOffset=&#123;&#125;, maxOffset=&#123;&#125;, count=&#123;&#125;, size=&#123;&#125; MiB, pullRequest=&#123;&#125;, flowControlTimes=&#123;&#125;", this.defaultMQPushConsumer.getPullThresholdSizeForQueue(), processQueue.getMsgTreeMap().firstKey(), processQueue.getMsgTreeMap().lastKey(), cachedMessageCount, cachedMessageSizeInMiB, pullRequest, queueFlowControlTimes); &#125; return; &#125; if (!this.consumeOrderly) &#123; if (processQueue.getMaxSpan() &gt; this.defaultMQPushConsumer.getConsumeConcurrentlyMaxSpan()) &#123; this.executePullRequestLater(pullRequest, PULL_TIME_DELAY_MILLS_WHEN_FLOW_CONTROL); if ((queueMaxSpanFlowControlTimes++ % 1000) == 0) &#123; log.warn( "the queue's messages, span too long, so do flow control, minOffset=&#123;&#125;, maxOffset=&#123;&#125;, maxSpan=&#123;&#125;, pullRequest=&#123;&#125;, flowControlTimes=&#123;&#125;", processQueue.getMsgTreeMap().firstKey(), processQueue.getMsgTreeMap().lastKey(), processQueue.getMaxSpan(), pullRequest, queueMaxSpanFlowControlTimes); &#125; return; &#125; &#125; else &#123; if (processQueue.isLocked()) &#123; if (!pullRequest.isLockedFirst()) &#123; final long offset = this.rebalanceImpl.computePullFromWhere(pullRequest.getMessageQueue()); boolean brokerBusy = offset &lt; pullRequest.getNextOffset(); log.info("the first time to pull message, so fix offset from broker. pullRequest: &#123;&#125; NewOffset: &#123;&#125; brokerBusy: &#123;&#125;", pullRequest, offset, brokerBusy); if (brokerBusy) &#123; log.info("[NOTIFYME]the first time to pull message, but pull request offset larger than broker consume offset. pullRequest: &#123;&#125; NewOffset: &#123;&#125;", pullRequest, offset); &#125; pullRequest.setLockedFirst(true); pullRequest.setNextOffset(offset); &#125; &#125; else &#123; this.executePullRequestLater(pullRequest, PULL_TIME_DELAY_MILLS_WHEN_EXCEPTION); log.info("pull message later because not locked in broker, &#123;&#125;", pullRequest); return; &#125; &#125; 消息消费数量：当前消息处理总数如果超出了指定阈值（1000条）：cachedMessageCount &gt; this.defaultMQPushConsumer.getPullThresholdForQueue()， 将触发流控，放弃本次拉取任务，并且指定该队列的下一次拉取任务时间间隔为PULL_TIME_DELAY_MILLS_WHEN_FLOW_CONTROL（50ms），每触发1000次流控后输出流控日志。 消息大小：当前消息处理的大小如果超出了指定大小阈值（100MB）：cachedMessageSizeInMiB &gt; this.defaultMQPushConsumer.getPullThresholdSizeForQueue()，将触发流控，流控处理与上述的消息消费数量维度的流控处理一致。 偏移量间隔：就是ProcessQueue中最大偏移量与最小偏移量的间距（processQueue.getMaxSpan()，等于maxOffset - minOffset，如下图），当这个间距超过指定阈值（2000）：processQueue.getMaxSpan() &gt; this.defaultMQPushConsumer.getConsumeConcurrentlyMaxSpan()，就会触发流控，流控处理与上述两种维度的流控处理一致。这个维度的流控设置目的，是为了避免因为一条消息堵塞导致重复消费（下面会细述），不同的是，这里有个前置条件，就是这里只针对非顺序消费模式进行此维度的流控(!this.consumeOrderly)。 如图，假如3109为本批次消息消费后的最大偏移量（maxOffset），1093为最小偏移量（minOffset），那么maxSpan=maxOffset-minOffset=3109-1093=2016，大于阈值（2000），则会触发流控，但可能出现上述的这种情况，就是在minOffset后面的很大部分消息实际上已被消费成功，因为下一次的消费偏移量（nextOffset）即为本次minOffset，所以，如果多次消费都是被同一minOffset位置的这个消息阻塞，那么就会引发大量消息重复被消费。而consumeConcurrentlyMaxSpan在这里只是为达到流控目的而把这个间距值限定在一个合适范围而已，但对于解决重复消费的问题，这个作用其实很有限。 考虑到要是碰上这种极端的情况，一批消息超2K条，实际绝大部分消息都被消费成功，而堵塞前进消费的minOffset那条消息，假设它的消费本身是存在问题的，可能是死循环之类的，那么一直没法成功消费，这就会导致进度一直卡在这条消息这里。其实RocketMQ有现成提供这类问题的解决方案，就是把因为消费超时卡住批量消费进度的消息定义为ExpireMsg，起一个计划线程池，定时执行清掉这些ExpireMsg，清除的具体操作就是把消息重发回去broker，作为延时消息（delayLevel=3，10s）再次存储并后续下发consumer消费。（下文会提到延时消息） 1.获取订阅信息拉取该主题的订阅信息，如果为空，则结束本次消息拉取，同时设置下一次拉取任务的延时为PULL_TIME_DELAY_MILLS_WHEN_EXCEPTION（3s）。org.apache.rocketmq.client.impl.consumer.DefaultMQPushConsumerImpl.pullMessage(PullRequest)12345678... final SubscriptionData subscriptionData = this.rebalanceImpl.getSubscriptionInner().get(pullRequest.getMessageQueue().getTopic()); if (null == subscriptionData) &#123; this.executePullRequestLater(pullRequest, PULL_TIME_DELAY_MILLS_WHEN_EXCEPTION); log.warn("find the consumer's subscription failed, &#123;&#125;", pullRequest); return; &#125;... 2.构建PullCallback构建PullCallback是为了后面请求拉取消息的结果响应处理，其中上一步获取到的订阅信息就是在成功拉取消息后用于处理拉取请求。org.apache.rocketmq.client.impl.consumer.DefaultMQPushConsumerImpl.pullMessage(PullRequest)12345678910111213141516...PullCallback pullCallback = new PullCallback() &#123; @Override public void onSuccess(PullResult pullResult) &#123; if (pullResult != null) &#123; pullResult = DefaultMQPushConsumerImpl.this.pullAPIWrapper.processPullResult(pullRequest.getMessageQueue(), pullResult, subscriptionData); switch (pullResult.getPullStatus()) &#123; case FOUND: long prevRequestOffset = pullRequest.getNextOffset(); pullRequest.setNextOffset(pullResult.getNextBeginOffset()); long pullRT = System.currentTimeMillis() - beginTimestamp; DefaultMQPushConsumerImpl.this.getConsumerStatsManager().incPullRT(pullRequest.getConsumerGroup(), pullRequest.getMessageQueue().getTopic(), pullRT);... 3.取得要从哪台broker拉取消息的broker地址org.apache.rocketmq.client.impl.consumer.DefaultMQPushConsumerImpl.pullMessage(PullRequest)123456789101112131415161718192021222324252627282930313233 ... try &#123; this.pullAPIWrapper.pullKernelImpl( pullRequest.getMessageQueue(), subExpression, subscriptionData.getExpressionType(), subscriptionData.getSubVersion(), pullRequest.getNextOffset(), this.defaultMQPushConsumer.getPullBatchSize(), sysFlag, commitOffsetValue, BROKER_SUSPEND_MAX_TIME_MILLIS, CONSUMER_TIMEOUT_MILLIS_WHEN_SUSPEND, CommunicationMode.ASYNC, pullCallback ); &#125; catch (Exception e) &#123; log.error("pullKernelImpl exception", e); this.executePullRequestLater(pullRequest, PULL_TIME_DELAY_MILLS_WHEN_EXCEPTION); &#125; ...``` ``` JAVA org.apache.rocketmq.client.impl.consumer.PullAPIWrapper.pullKernelImpl(MessageQueue, String, String, long, long, int, int, long, long, long, CommunicationMode, PullCallback) FindBrokerResult findBrokerResult = this.mQClientFactory.findBrokerAddressInSubscribe(mq.getBrokerName(), this.recalculatePullFromWhichNode(mq), false); if (null == findBrokerResult) &#123; this.mQClientFactory.updateTopicRouteInfoFromNameServer(mq.getTopic()); findBrokerResult = this.mQClientFactory.findBrokerAddressInSubscribe(mq.getBrokerName(), this.recalculatePullFromWhichNode(mq), false); &#125; 4.构建要拉取消息的网络请求头PullAPIWrapper.pullKernelImpl(…)里边调用MQClientAPIImpl.pullMessage(…)，为拉取消息的实际网络请求作准备org.apache.rocketmq.client.impl.MQClientAPIImpl.pullMessage(String, PullMessageRequestHeader, long, CommunicationMode, PullCallback)12345678910111213141516171819202122232425public PullResult pullMessage( final String addr, final PullMessageRequestHeader requestHeader, final long timeoutMillis, final CommunicationMode communicationMode, final PullCallback pullCallback) throws RemotingException, MQBrokerException, InterruptedException &#123; RemotingCommand request = RemotingCommand.createRequestCommand(RequestCode.PULL_MESSAGE, requestHeader); switch (communicationMode) &#123; case ONEWAY: assert false; return null; case ASYNC: this.pullMessageAsync(addr, request, timeoutMillis, pullCallback); return null; case SYNC: return this.pullMessageSync(addr, request, timeoutMillis); default: assert false; break; &#125; return null;&#125; 5.执行网络层请求broker的代码，根据结果执行对应的回调处理深入到网络的调用过程，可以发现本质是交给了netty的work线程去向broker请求拉取消息，拉取到消息之后异步回调拉取的结果。入口代码如下：org.apache.rocketmq.client.impl.MQClientAPIImpl.pullMessageAsync(String, RemotingCommand, long, PullCallback)12345678910111213141516171819202122232425262728293031private void pullMessageAsync( final String addr, final RemotingCommand request, final long timeoutMillis, final PullCallback pullCallback ) throws RemotingException, InterruptedException &#123; this.remotingClient.invokeAsync(addr, request, timeoutMillis, new InvokeCallback() &#123; @Override public void operationComplete(ResponseFuture responseFuture) &#123; RemotingCommand response = responseFuture.getResponseCommand(); if (response != null) &#123; try &#123; PullResult pullResult = MQClientAPIImpl.this.processPullResponse(response); assert pullResult != null; pullCallback.onSuccess(pullResult); &#125; catch (Exception e) &#123; pullCallback.onException(e); &#125; &#125; else &#123; if (!responseFuture.isSendRequestOK()) &#123; pullCallback.onException(new MQClientException("send request failed to " + addr + ". Request: " + request, responseFuture.getCause())); &#125; else if (responseFuture.isTimeout()) &#123; pullCallback.onException(new MQClientException("wait response from " + addr + " timeout :" + responseFuture.getTimeoutMillis() + "ms" + ". Request: " + request, responseFuture.getCause())); &#125; else &#123; pullCallback.onException(new MQClientException("unknown reason. addr: " + addr + ", timeoutMillis: " + timeoutMillis + ". Request: " + request, responseFuture.getCause())); &#125; &#125; &#125; &#125;); &#125; org.apache.rocketmq.client.impl.MQClientAPIImpl.pullMessageSync(String, RemotingCommand, long)123456789private PullResult pullMessageSync( final String addr, final RemotingCommand request, final long timeoutMillis) throws RemotingException, InterruptedException, MQBrokerException &#123; RemotingCommand response = this.remotingClient.invokeSync(addr, request, timeoutMillis); assert response != null; return this.processPullResponse(response);&#125; 6.执行第一步构建的PullCallback的onSuccess/onException逻辑其中onSuccess中，有根据broker响应的不同结果做不同的逻辑处理：org.apache.rocketmq.client.impl.consumer.DefaultMQPushConsumerImpl.pullMessage(...).new PullCallback() &#123;...&#125;.onSuccess(PullResult)、org.apache.rocketmq.client.impl.consumer.DefaultMQPushConsumerImpl.pullMessage(...).new PullCallback() &#123;...&#125;.onException(Throwable)12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970@Overridepublic void onSuccess(PullResult pullResult) &#123; if (pullResult != null) &#123; pullResult = DefaultMQPushConsumerImpl.this.pullAPIWrapper.processPullResult(pullRequest.getMessageQueue(), pullResult, subscriptionData); switch (pullResult.getPullStatus()) &#123; case FOUND: long prevRequestOffset = pullRequest.getNextOffset(); // 本次拉取消息的offset pullRequest.setNextOffset(pullResult.getNextBeginOffset()); // 设置下一次拉取消息的offset long pullRT = System.currentTimeMillis() - beginTimestamp; DefaultMQPushConsumerImpl.this.getConsumerStatsManager().incPullRT(pullRequest.getConsumerGroup(), pullRequest.getMessageQueue().getTopic(), pullRT); long firstMsgOffset = Long.MAX_VALUE; if (pullResult.getMsgFoundList() == null || pullResult.getMsgFoundList().isEmpty()) &#123; DefaultMQPushConsumerImpl.this.executePullRequestImmediately(pullRequest); // 如果没有发现新拉到的消息，将pullRequest放到pullRequestQueue中 &#125; else &#123; firstMsgOffset = pullResult.getMsgFoundList().get(0).getQueueOffset(); DefaultMQPushConsumerImpl.this.getConsumerStatsManager().incPullTPS(pullRequest.getConsumerGroup(), pullRequest.getMessageQueue().getTopic(), pullResult.getMsgFoundList().size()); boolean dispatchToConsume = processQueue.putMessage(t.getMsgFoundList(),processQueue,pullRequest getMessageQueue(),dispatchToConsume); // 把拉取到的消息丢给 processQueue DefaultMQPushConsumerImpl.this.consumeMessageService.submitConsumeRequest( pullResult.getMsgFoundList(), processQueue, pullRequest.getMessageQueue(), dispatchToConsume); // 把processQueue丢给ConsumeMessageService，提交消费任务从而让拉取到的消息进行消费 if (DefaultMQPushConsumerImpl.this.defaultMQPushConsumer.getPullInterval() &gt; 0) &#123; //判断是否有设置拉取消息的时间间隔，有则走间隔拉取消息的逻辑 DefaultMQPushConsumerImpl.this.executePullRequestLater(pullRequest, DefaultMQPushConsumerImpl.this.defaultMQPushConsumer.getPullInterval()); &#125; else &#123; // 否则将pullRequest放到pullRequestQueue中 DefaultMQPushConsumerImpl.this.executePullRequestImmediately(pullRequest); &#125; &#125; if (pullResult.getNextBeginOffset() &lt; prevRequestOffset || firstMsgOffset &lt; prevRequestOffset) &#123; log.warn( "[BUG] pull message result maybe data wrong, nextBeginOffset: &#123;&#125; firstMsgOffset: &#123;&#125; prevRequestOffset: &#123;&#125;", pullResult.getNextBeginOffset(), firstMsgOffset, prevRequestOffset); &#125; break; case NO_NEW_MSG: ... case NO_MATCHED_MSG: ... case OFFSET_ILLEGAL: ... default: break; &#125; &#125;&#125;@Overridepublic void onException(Throwable e) &#123; if (!pullRequest.getMessageQueue().getTopic().startsWith(MixAll.RETRY_GROUP_TOPIC_PREFIX)) &#123; log.warn("execute the pull request exception", e); &#125; DefaultMQPushConsumerImpl.this.executePullRequestLater(pullRequest, PULL_TIME_DELAY_MILLS_WHEN_EXCEPTION);&#125; 7.将拉取到的消息交给consumeMessageService如上一步的部分代码所示，就是交给consumeMessageService代表的消费消息线程池处理，由于消费消息的方式有两种，提交线程池的入口也有两个：org.apache.rocketmq.client.impl.consumer.ConsumeMessageConcurrentlyService.submitConsumeRequest(List, ProcessQueue, MessageQueue, boolean)12345678910111213141516171819202122232425262728293031323334353637public void submitConsumeRequest( final List&lt;MessageExt&gt; msgs, final ProcessQueue processQueue, final MessageQueue messageQueue, final boolean dispatchToConsume) &#123; final int consumeBatchSize = this.defaultMQPushConsumer.getConsumeMessageBatchMaxSize(); if (msgs.size() &lt;= consumeBatchSize) &#123; ConsumeRequest consumeRequest = new ConsumeRequest(msgs, processQueue, messageQueue); try &#123; this.consumeExecutor.submit(consumeRequest); &#125; catch (RejectedExecutionException e) &#123; this.submitConsumeRequestLater(consumeRequest); &#125; &#125; else &#123; for (int total = 0; total &lt; msgs.size(); ) &#123; List&lt;MessageExt&gt; msgThis = new ArrayList&lt;MessageExt&gt;(consumeBatchSize); for (int i = 0; i &lt; consumeBatchSize; i++, total++) &#123; if (total &lt; msgs.size()) &#123; msgThis.add(msgs.get(total)); &#125; else &#123; break; &#125; &#125; ConsumeRequest consumeRequest = new ConsumeRequest(msgThis, processQueue, messageQueue); try &#123; this.consumeExecutor.submit(consumeRequest); &#125; catch (RejectedExecutionException e) &#123; for (; total &lt; msgs.size(); total++) &#123; msgThis.add(msgs.get(total)); &#125; this.submitConsumeRequestLater(consumeRequest); &#125; &#125; &#125;&#125; org.apache.rocketmq.client.impl.consumer.ConsumeMessageOrderlyService.submitConsumeRequest(List, ProcessQueue, MessageQueue, boolean)12345678910public void submitConsumeRequest( final List&lt;MessageExt&gt; msgs, final ProcessQueue processQueue, final MessageQueue messageQueue, final boolean dispathToConsume) &#123; if (dispathToConsume) &#123; ConsumeRequest consumeRequest = new ConsumeRequest(processQueue, messageQueue); this.consumeExecutor.submit(consumeRequest); &#125;&#125; 也就是说，拉到消息后接下来便是消费消息的过程了。当然，在这之前，还有一个指定动作就是将下一次需要拉取的pullRequest再次放到pullRequestQueue中，其用意在上文已有提及。 消息拉取过程小结一个consumer客户端会分配一个拉取消息线程（PullMessageService），不停地从存放了messageQuene的阻塞队列中take需要拉取消息的messagequene，最后通过调用通知网络层发起拉取消息拉取的网络请求（实际就是交给netty的worker线程拉消息），netty的worker线程拉取到消息后调用处理PullCallback处理拉取的结果。 由于从broker拉取消息的网络请求交给了netty的worker线程处理，并且work线程处理完之后再异步通知拉取结果处理，我们可以知道pullmessage本身并没有太重的操作，同时每次请求broker拉取消息是批量拉取（默认值是每批32条），因此即使一个consuemr客户端只会有一个线程负责所有consumerGroup，也不会有太慢以及太大的性能瓶颈。 消息消费过程ConsumeMessageService是消息消费接口，有两个实现类，分别是顺序消费（ConsumeMessageOrderlyService）及普通消费（ConsumeMessageConcurrentlyService）。无论ConsumeMessageOrderlyService还是ConsumeMessageConcurrentlyService，在核心方法ConsumeMessageService.submitConsumeRequest(…)的实现里都有一个核心逻辑，就是将代表消息实际消费的任务ConsumeRequest，提交给了一个名为ConsumeMessageThread的线程池去异步执行。org.apache.rocketmq.client.impl.consumer.ConsumeMessageConcurrentlyService.ConsumeMessageConcurrentlyService(DefaultMQPushConsumerImpl, MessageListenerConcurrently)1234567891011121314151617181920public ConsumeMessageConcurrentlyService(DefaultMQPushConsumerImpl defaultMQPushConsumerImpl, MessageListenerConcurrently messageListener) &#123; this.defaultMQPushConsumerImpl = defaultMQPushConsumerImpl; this.messageListener = messageListener; this.defaultMQPushConsumer = this.defaultMQPushConsumerImpl.getDefaultMQPushConsumer(); this.consumerGroup = this.defaultMQPushConsumer.getConsumerGroup(); this.consumeRequestQueue = new LinkedBlockingQueue&lt;Runnable&gt;(); this.consumeExecutor = new ThreadPoolExecutor( this.defaultMQPushConsumer.getConsumeThreadMin(), this.defaultMQPushConsumer.getConsumeThreadMax(), 1000 * 60, TimeUnit.MILLISECONDS, this.consumeRequestQueue, new ThreadFactoryImpl("ConsumeMessageThread_")); this.scheduledExecutorService = Executors.newSingleThreadScheduledExecutor(new ThreadFactoryImpl("ConsumeMessageScheduledThread_")); this.cleanExpireMsgExecutors = Executors.newSingleThreadScheduledExecutor(new ThreadFactoryImpl("CleanExpireMsgScheduledThread_"));&#125; 那么，重点就在于ConsumeRequest线程任务的run方法了，而无论ConsumeMessageOrderlyService还是ConsumeMessageConcurrentlyService，核心的消费逻辑基本一致：取得业务方法实现的messageListener，调用其consumeMessage方法，得到处理结果。org.apache.rocketmq.client.impl.consumer.ConsumeMessageConcurrentlyService.ConsumeRequest.run()12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182// 判断消息是否已被删除 if (this.processQueue.isDropped()) &#123; log.info("the message queue not be able to consume, because it's dropped. group=&#123;&#125; &#123;&#125;", ConsumeMessageConcurrentlyService.this.consumerGroup, this.messageQueue); return; &#125;// 得到messageListener，也就是实际的消息消费业务实现类 MessageListenerConcurrently listener = ConsumeMessageConcurrentlyService.this.messageListener; ConsumeConcurrentlyContext context = new ConsumeConcurrentlyContext(messageQueue); ConsumeConcurrentlyStatus status = null; defaultMQPushConsumerImpl.resetRetryAndNamespace(msgs, defaultMQPushConsumer.getConsumerGroup());// 准备消息消费的上下文 ConsumeMessageContext consumeMessageContext = null; if (ConsumeMessageConcurrentlyService.this.defaultMQPushConsumerImpl.hasHook()) &#123; consumeMessageContext = new ConsumeMessageContext(); consumeMessageContext.setNamespace(defaultMQPushConsumer.getNamespace()); consumeMessageContext.setConsumerGroup(defaultMQPushConsumer.getConsumerGroup()); consumeMessageContext.setProps(new HashMap&lt;String, String&gt;()); consumeMessageContext.setMq(messageQueue); consumeMessageContext.setMsgList(msgs); consumeMessageContext.setSuccess(false); ConsumeMessageConcurrentlyService.this.defaultMQPushConsumerImpl.executeHookBefore(consumeMessageContext); &#125; long beginTimestamp = System.currentTimeMillis(); boolean hasException = false; ConsumeReturnType returnType = ConsumeReturnType.SUCCESS; try &#123; if (msgs != null &amp;&amp; !msgs.isEmpty()) &#123; for (MessageExt msg : msgs) &#123; MessageAccessor.setConsumeStartTimeStamp(msg, String.valueOf(System.currentTimeMillis())); &#125; &#125; status = listener.consumeMessage(Collections.unmodifiableList(msgs), context); // 调用消息消费的业务逻辑 &#125; catch (Throwable e) &#123; log.warn("consumeMessage exception: &#123;&#125; Group: &#123;&#125; Msgs: &#123;&#125; MQ: &#123;&#125;", RemotingHelper.exceptionSimpleDesc(e), ConsumeMessageConcurrentlyService.this.consumerGroup, msgs, messageQueue); hasException = true; &#125; long consumeRT = System.currentTimeMillis() - beginTimestamp; // 业务消费代码处理时长 if (null == status) &#123; if (hasException) &#123; returnType = ConsumeReturnType.EXCEPTION; &#125; else &#123; returnType = ConsumeReturnType.RETURNNULL; &#125; &#125; else if (consumeRT &gt;= defaultMQPushConsumer.getConsumeTimeout() * 60 * 1000) &#123; // 如果消费超时，则有相应处理 returnType = ConsumeReturnType.TIME_OUT; &#125; else if (ConsumeConcurrentlyStatus.RECONSUME_LATER == status) &#123; returnType = ConsumeReturnType.FAILED; &#125; else if (ConsumeConcurrentlyStatus.CONSUME_SUCCESS == status) &#123; returnType = ConsumeReturnType.SUCCESS; &#125; if (ConsumeMessageConcurrentlyService.this.defaultMQPushConsumerImpl.hasHook()) &#123; consumeMessageContext.getProps().put(MixAll.CONSUME_CONTEXT_TYPE, returnType.name()); &#125; if (null == status) &#123; log.warn("consumeMessage return null, Group: &#123;&#125; Msgs: &#123;&#125; MQ: &#123;&#125;", ConsumeMessageConcurrentlyService.this.consumerGroup, msgs, messageQueue); status = ConsumeConcurrentlyStatus.RECONSUME_LATER; &#125; if (ConsumeMessageConcurrentlyService.this.defaultMQPushConsumerImpl.hasHook()) &#123; consumeMessageContext.setStatus(status.toString()); consumeMessageContext.setSuccess(ConsumeConcurrentlyStatus.CONSUME_SUCCESS == status); ConsumeMessageConcurrentlyService.this.defaultMQPushConsumerImpl.executeHookAfter(consumeMessageContext); &#125; ConsumeMessageConcurrentlyService.this.getConsumerStatsManager() .incConsumeRT(ConsumeMessageConcurrentlyService.this.consumerGroup, messageQueue.getTopic(), consumeRT);// 处理消费消息的结果 if (!processQueue.isDropped()) &#123; ConsumeMessageConcurrentlyService.this.processConsumeResult(status, context, this); &#125; else &#123; log.warn("processQueue is dropped without process consume result. messageQueue=&#123;&#125;, msgs=&#123;&#125;", messageQueue, msgs); &#125; 接下来就看下，messageListener得到消费结果后做的处理：org.apache.rocketmq.client.impl.consumer.ConsumeMessageConcurrentlyService.processConsumeResult(ConsumeConcurrentlyStatus, ConsumeConcurrentlyContext, ConsumeRequest)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162 public void processConsumeResult( final ConsumeConcurrentlyStatus status, final ConsumeConcurrentlyContext context, final ConsumeRequest consumeRequest ) &#123; int ackIndex = context.getAckIndex(); if (consumeRequest.getMsgs().isEmpty()) return;// 以下为统计数据记录的操作 switch (status) &#123; case CONSUME_SUCCESS: if (ackIndex &gt;= consumeRequest.getMsgs().size()) &#123; ackIndex = consumeRequest.getMsgs().size() - 1; &#125; int ok = ackIndex + 1; int failed = consumeRequest.getMsgs().size() - ok; this.getConsumerStatsManager().incConsumeOKTPS(consumerGroup, consumeRequest.getMessageQueue().getTopic(), ok); this.getConsumerStatsManager().incConsumeFailedTPS(consumerGroup, consumeRequest.getMessageQueue().getTopic(), failed); break; case RECONSUME_LATER: ackIndex = -1; this.getConsumerStatsManager().incConsumeFailedTPS(consumerGroup, consumeRequest.getMessageQueue().getTopic(), consumeRequest.getMsgs().size()); break; default: break; &#125;// 以下为针对不同消费模式对消费失败的消息做不同的处理 switch (this.defaultMQPushConsumer.getMessageModel()) &#123; case BROADCASTING: // 广播模式下的消费失败消息处理比较简单，就是遍历失败消息，拿出来之后打个log，默认直接丢弃失败消息就完了 for (int i = ackIndex + 1; i &lt; consumeRequest.getMsgs().size(); i++) &#123; MessageExt msg = consumeRequest.getMsgs().get(i); log.warn("BROADCASTING, the message consume failed, drop it, &#123;&#125;", msg.toString()); &#125; break; case CLUSTERING: // 集群模式下的失败消息处理 List&lt;MessageExt&gt; msgBackFailed = new ArrayList&lt;MessageExt&gt;(consumeRequest.getMsgs().size()); for (int i = ackIndex + 1; i &lt; consumeRequest.getMsgs().size(); i++) &#123; MessageExt msg = consumeRequest.getMsgs().get(i); boolean result = this.sendMessageBack(msg, context); // 失败的消息，直接重新发回broker if (!result) &#123; // 如果发回broker的操作结果还是失败，则放到msgBackFailed列表，下一步继续作处理 msg.setReconsumeTimes(msg.getReconsumeTimes() + 1); msgBackFailed.add(msg); &#125; &#125; // 如果msgBackFailed列表不为空，说明上一步有消息在发回broker时发送失败，则走另外的方式处理消费失败的消息：也就是晚一些再重试消费 if (!msgBackFailed.isEmpty()) &#123; consumeRequest.getMsgs().removeAll(msgBackFailed); this.submitConsumeRequestLater(msgBackFailed, consumeRequest.getProcessQueue(), consumeRequest.getMessageQueue()); &#125; break; default: break; &#125;// 以下为更新offset操作，可以看到，不管消息消费成功与否，都会更新consumerGroup消费到的offset long offset = consumeRequest.getProcessQueue().removeMessage(consumeRequest.getMsgs()); if (offset &gt;= 0 &amp;&amp; !consumeRequest.getProcessQueue().isDropped()) &#123; this.defaultMQPushConsumerImpl.getOffsetStore().updateOffset(consumeRequest.getMessageQueue(), offset, true); // 这里实际上只是更新RemoteBrokerOffsetStore.offsetTable里所存储的offset值，在实现上是通过定时线程发网络请求提交到broker，详见org.apache.rocketmq.client.impl.factory.MQClientInstance.persistAllConsumerOffset() &#125; &#125; 由以上的消费结果处理基本可以得知： 调用业务实现的消费消息逻辑后，得到消费结果，即使消费超时，也最终会根据messageListener执行返回的结果来决定是否重新消费消息。 根据不同的消费模式会对消费失败的结果做不同的处理（实际上按是否顺序消费来划分的两种消息消费模型来看，两者都有不同的消费结果状态定义ConsumeOrderlyStatus、ConsumeConcurrentlyStatus）。对于广播模式，失败消息的处理是直接丢弃；集群模式则会重新消费消息，相应的处理为 1.把消息重新发回Broker，后续作重试处理 2.若发回broker失败，后续作重试消费 关于offset的更新，其实是不管消息消费成功与否，都会有更新consumerGroup所消费到的offset，因为消费失败的消息会作重试处理，其实并不影响offset的更新 接下来，便是更新offset及重试消费消息的过程分析了。 offset更新无论是LocalFileOffsetStore还是RemoteBrokerOffsetStore，offset更新的逻辑都是一致的：实际上就是每个messageQueue消费到的offset，存放到一个名为offsetTable的内存缓存map里。12345678910111213141516public void updateOffset(MessageQueue mq, long offset, boolean increaseOnly) &#123; if (mq != null) &#123; AtomicLong offsetOld = this.offsetTable.get(mq); if (null == offsetOld) &#123; offsetOld = this.offsetTable.putIfAbsent(mq, new AtomicLong(offset)); &#125; if (null != offsetOld) &#123; if (increaseOnly) &#123; MixAll.compareAndIncreaseOnly(offsetOld, offset); &#125; else &#123; offsetOld.set(offset); &#125; &#125; &#125;&#125; 那么，之后是怎样把这offset更新提交给broker这边的呢？找回consumer启动过程可以定位到，offset的更新，其实是由一个定时线程提交给broker的。就在 org.apache.rocketmq.client.impl.factory.MQClientInstance.start()这个方法里，this.startScheduledTask()启动的若干个定时线程池里，其中有一个就是定时持久化所有consumer的offset变更1234567891011this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() &#123; @Override public void run() &#123; try &#123; MQClientInstance.this.persistAllConsumerOffset(); &#125; catch (Exception e) &#123; log.error("ScheduledTask persistAllConsumerOffset exception", e); &#125; &#125; &#125;, 1000 * 10, this.clientConfig.getPersistConsumerOffsetInterval(), TimeUnit.MILLISECONDS); 这个定时任务，最终会调用到RemoteBrokerOffsetStore.persistAll(Set)方法，把offset变更发回Broker1234567891011121314151617181920212223242526272829303132333435public void persistAll(Set&lt;MessageQueue&gt; mqs) &#123; if (null == mqs || mqs.isEmpty()) return; final HashSet&lt;MessageQueue&gt; unusedMQ = new HashSet&lt;MessageQueue&gt;(); if (!mqs.isEmpty()) &#123; for (Map.Entry&lt;MessageQueue, AtomicLong&gt; entry : this.offsetTable.entrySet()) &#123; MessageQueue mq = entry.getKey(); AtomicLong offset = entry.getValue(); if (offset != null) &#123; if (mqs.contains(mq)) &#123; try &#123; this.updateConsumeOffsetToBroker(mq, offset.get()); // 将offset的更新，发到broker log.info("[persistAll] Group: &#123;&#125; ClientId: &#123;&#125; updateConsumeOffsetToBroker &#123;&#125; &#123;&#125;", this.groupName, this.mQClientFactory.getClientId(), mq, offset.get()); &#125; catch (Exception e) &#123; log.error("updateConsumeOffsetToBroker exception, " + mq.toString(), e); &#125; &#125; else &#123; unusedMQ.add(mq); &#125; &#125; &#125; &#125; if (!unusedMQ.isEmpty()) &#123; for (MessageQueue mq : unusedMQ) &#123; this.offsetTable.remove(mq); log.info("remove unused mq, &#123;&#125;, &#123;&#125;", mq, this.groupName); &#125; &#125;&#125; 由于是先消费消息，再提交offset更新，这里有可能存在消费完消息之后，提交offset失败的情况，尽管这种可能性极低，因为提交offset操作实际上只是做了内存的操作，并没有什么重的慢的操作。倒是另外一种情况导致offset更新的丢失的可能性会大很多，因为offset是先存在内存，再通过定时任务间隔数秒走网络请求提交给broker的，这里可能存在譬如这数秒内consumer突然宕机、网络请求失败等因素导致没有成功提交offset到broker，那么在consumer宕机后重启服务，就会出现重复消费消息。综上，在consumer的业务消费代码务必要保证幂等性。 而事实上，offset提交broker的操作不是仅仅依赖定时任务完成，在consumer关闭退出时，也会有一次主动触发持久化offset到broker的方法调用（DefaultMQPushConsumerImpl里的shutdown方法也是调的DefaultMQPullConsumerImpl.shutdown()方法）：org.apache.rocketmq.client.impl.consumer.DefaultMQPullConsumerImpl.shutdown()1234567891011121314151617public synchronized void shutdown() &#123; switch (this.serviceState) &#123; case CREATE_JUST: break; case RUNNING: this.persistConsumerOffset(); // 在consumer关闭时把offset持久化到broker this.mQClientFactory.unregisterConsumer(this.defaultMQPullConsumer.getConsumerGroup()); this.mQClientFactory.shutdown(); log.info("the consumer [&#123;&#125;] shutdown OK", this.defaultMQPullConsumer.getConsumerGroup()); this.serviceState = ServiceState.SHUTDOWN_ALREADY; break; case SHUTDOWN_ALREADY: break; default: break; &#125;&#125; 这样做，是为了让应用正常退出时，让consumer实例也正常关闭，保证此时也能触发offset更新正确提交到broker。 消费消息重试在上文中提到，在消息消费失败后，失败消息会重新发回broker。若重新发送给broker也失败了，那么失败消息会被交给定时任务重新尝试消费。这里看一下broker到底是如何处理消费失败的消息的。在consumerSendMessageBack方法里看到，失败消息重新发回broker使用到的远程请求类型是RequestCode.CONSUMER_SEND_MSG_BACKorg.apache.rocketmq.client.impl.MQClientAPIImpl.consumerSendMessageBack(String, MessageExt, String, int, long, int)1234567891011 public void consumerSendMessageBack( final String addr, final MessageExt msg, final String consumerGroup, final int delayLevel, final long timeoutMillis, final int maxConsumeRetryTimes ) throws RemotingException, MQBrokerException, InterruptedException &#123; ConsumerSendMsgBackRequestHeader requestHeader = new ConsumerSendMsgBackRequestHeader(); RemotingCommand request = RemotingCommand.createRequestCommand(RequestCode.CONSUMER_SEND_MSG_BACK, requestHeader);... 根据这个类型的请求，找到broker处理消费失败的消息的入口：SendMessageProcessor.processRequest(ChannelHandlerContext, RemotingCommand)org.apache.rocketmq.broker.processor.SendMessageProcessor.processRequest(ChannelHandlerContext, RemotingCommand)1234567 public RemotingCommand processRequest(ChannelHandlerContext ctx, RemotingCommand request) throws RemotingCommandException &#123; SendMessageContext mqtraceContext; switch (request.getCode()) &#123; case RequestCode.CONSUMER_SEND_MSG_BACK: return this.consumerSendMsgBack(ctx, request); // 处理重新发回broker消息的请求... org.apache.rocketmq.broker.processor.SendMessageProcessor.consumerSendMsgBack(ChannelHandlerContext, RemotingCommand)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154private RemotingCommand consumerSendMsgBack(final ChannelHandlerContext ctx, final RemotingCommand request) throws RemotingCommandException &#123; final RemotingCommand response = RemotingCommand.createResponseCommand(null); final ConsumerSendMsgBackRequestHeader requestHeader = (ConsumerSendMsgBackRequestHeader)request.decodeCommandCustomHeader(ConsumerSendMsgBackRequestHeader.class); String namespace = NamespaceUtil.getNamespaceFromResource(requestHeader.getGroup()); if (this.hasConsumeMessageHook() &amp;&amp; !UtilAll.isBlank(requestHeader.getOriginMsgId())) &#123; ConsumeMessageContext context = new ConsumeMessageContext(); context.setNamespace(namespace); context.setConsumerGroup(requestHeader.getGroup()); context.setTopic(requestHeader.getOriginTopic()); context.setCommercialRcvStats(BrokerStatsManager.StatsType.SEND_BACK); context.setCommercialRcvTimes(1); context.setCommercialOwner(request.getExtFields().get(BrokerStatsManager.COMMERCIAL_OWNER)); this.executeConsumeMessageHookAfter(context); &#125; SubscriptionGroupConfig subscriptionGroupConfig = this.brokerController.getSubscriptionGroupManager().findSubscriptionGroupConfig(requestHeader.getGroup()); if (null == subscriptionGroupConfig) &#123; response.setCode(ResponseCode.SUBSCRIPTION_GROUP_NOT_EXIST); response.setRemark("subscription group not exist, " + requestHeader.getGroup() + " " + FAQUrl.suggestTodo(FAQUrl.SUBSCRIPTION_GROUP_NOT_EXIST)); return response; &#125; if (!PermName.isWriteable(this.brokerController.getBrokerConfig().getBrokerPermission())) &#123; response.setCode(ResponseCode.NO_PERMISSION); response.setRemark("the broker[" + this.brokerController.getBrokerConfig().getBrokerIP1() + "] sending message is forbidden"); return response; &#125; if (subscriptionGroupConfig.getRetryQueueNums() &lt;= 0) &#123; response.setCode(ResponseCode.SUCCESS); response.setRemark(null); return response; &#125; String newTopic = MixAll.getRetryTopic(requestHeader.getGroup()); int queueIdInt = Math.abs(this.random.nextInt() % 99999999) % subscriptionGroupConfig.getRetryQueueNums(); int topicSysFlag = 0; if (requestHeader.isUnitMode()) &#123; topicSysFlag = TopicSysFlag.buildSysFlag(false, true); &#125; TopicConfig topicConfig = this.brokerController.getTopicConfigManager().createTopicInSendMessageBackMethod( newTopic, subscriptionGroupConfig.getRetryQueueNums(), PermName.PERM_WRITE | PermName.PERM_READ, topicSysFlag); if (null == topicConfig) &#123; response.setCode(ResponseCode.SYSTEM_ERROR); response.setRemark("topic[" + newTopic + "] not exist"); return response; &#125; if (!PermName.isWriteable(topicConfig.getPerm())) &#123; response.setCode(ResponseCode.NO_PERMISSION); response.setRemark(String.format("the topic[%s] sending message is forbidden", newTopic)); return response; &#125; MessageExt msgExt = this.brokerController.getMessageStore().lookMessageByOffset(requestHeader.getOffset()); if (null == msgExt) &#123; response.setCode(ResponseCode.SYSTEM_ERROR); response.setRemark("look message by offset failed, " + requestHeader.getOffset()); return response; &#125; final String retryTopic = msgExt.getProperty(MessageConst.PROPERTY_RETRY_TOPIC); if (null == retryTopic) &#123; MessageAccessor.putProperty(msgExt, MessageConst.PROPERTY_RETRY_TOPIC, msgExt.getTopic()); &#125; msgExt.setWaitStoreMsgOK(false); int delayLevel = requestHeader.getDelayLevel(); int maxReconsumeTimes = subscriptionGroupConfig.getRetryMaxTimes(); if (request.getVersion() &gt;= MQVersion.Version.V3_4_9.ordinal()) &#123; maxReconsumeTimes = requestHeader.getMaxReconsumeTimes(); &#125; if (msgExt.getReconsumeTimes() &gt;= maxReconsumeTimes || delayLevel &lt; 0) &#123; newTopic = MixAll.getDLQTopic(requestHeader.getGroup()); queueIdInt = Math.abs(this.random.nextInt() % 99999999) % DLQ_NUMS_PER_GROUP; topicConfig = this.brokerController.getTopicConfigManager().createTopicInSendMessageBackMethod(newTopic, DLQ_NUMS_PER_GROUP, PermName.PERM_WRITE, 0 ); if (null == topicConfig) &#123; response.setCode(ResponseCode.SYSTEM_ERROR); response.setRemark("topic[" + newTopic + "] not exist"); return response; &#125; &#125; else &#123; if (0 == delayLevel) &#123; delayLevel = 3 + msgExt.getReconsumeTimes(); &#125; msgExt.setDelayTimeLevel(delayLevel); &#125; MessageExtBrokerInner msgInner = new MessageExtBrokerInner(); msgInner.setTopic(newTopic); msgInner.setBody(msgExt.getBody()); msgInner.setFlag(msgExt.getFlag()); MessageAccessor.setProperties(msgInner, msgExt.getProperties()); msgInner.setPropertiesString(MessageDecoder.messageProperties2String(msgExt.getProperties())); msgInner.setTagsCode(MessageExtBrokerInner.tagsString2tagsCode(null, msgExt.getTags())); msgInner.setQueueId(queueIdInt); msgInner.setSysFlag(msgExt.getSysFlag()); msgInner.setBornTimestamp(msgExt.getBornTimestamp()); msgInner.setBornHost(msgExt.getBornHost()); msgInner.setStoreHost(this.getStoreHost()); msgInner.setReconsumeTimes(msgExt.getReconsumeTimes() + 1); String originMsgId = MessageAccessor.getOriginMessageId(msgExt); MessageAccessor.setOriginMessageId(msgInner, UtilAll.isBlank(originMsgId) ? msgExt.getMsgId() : originMsgId); PutMessageResult putMessageResult = this.brokerController.getMessageStore().putMessage(msgInner); if (putMessageResult != null) &#123; switch (putMessageResult.getPutMessageStatus()) &#123; case PUT_OK: String backTopic = msgExt.getTopic(); String correctTopic = msgExt.getProperty(MessageConst.PROPERTY_RETRY_TOPIC); if (correctTopic != null) &#123; backTopic = correctTopic; &#125; this.brokerController.getBrokerStatsManager().incSendBackNums(requestHeader.getGroup(), backTopic); response.setCode(ResponseCode.SUCCESS); response.setRemark(null); return response; default: break; &#125; response.setCode(ResponseCode.SYSTEM_ERROR); response.setRemark(putMessageResult.getPutMessageStatus().name()); return response; &#125; response.setCode(ResponseCode.SYSTEM_ERROR); response.setRemark("putMessageResult is null"); return response;&#125; 这里可以看到，broker在接收到消费端consumer发回来的失败消息后，会转为延时消息存放起来（因为重试消息是有时间间隔的），利用/的功能，broker端到了延迟的时间点，再将该/转换为重试消息（Topic名转为%RETRY%+consumerGroup），此时consumer端对这些消息重新可见，从而会拉取到该重试消息，达到延迟重复消费的目的。 延时消息延时消息的使用只需要在发送前，指定message的DelayTimeLevel即可。123Message msg = new Message("topic","msg content".getBytes(RemotingHelper.DEFAULT_CHARSET));msg.setDelayTimeLevel(3); // 延迟10sSendResult sendResult = producer.send(msg); 目前RocketMQ支持的延迟时间有： 延时 1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h，对应的延迟级别（delayTimeLevel）依次为1,2,3,4,5… 实现原理延迟消息在Producer发送后，在Broker被存放在单独一个Topic：SCHEDULE_TOPIC_XXXX，每一个延迟级别对应该Topic下的一个消费队列，当延迟时间到之时，由定时调度任务（DeliverDelayedMessageTimerTask）读取消息并转换为普通的消息存取到真实指定的Topic下，此时对于consumer端此消息才可见，从而被consumer消费。 具体实现123456789101112131415161718192021222324252627282930 public void start() &#123;// 1. 根据支持的各种延迟级别，添加不同延迟时间的TimeTask if (started.compareAndSet(false, true)) &#123; this.timer = new Timer("ScheduleMessageTimerThread", true); for (Map.Entry&lt;Integer, Long&gt; entry : this.delayLevelTable.entrySet()) &#123; Integer level = entry.getKey(); Long timeDelay = entry.getValue(); Long offset = this.offsetTable.get(level); // 获取每个延迟级别在普通消费队列中的offset if (null == offset) &#123; offset = 0L; &#125; if (timeDelay != null) &#123; this.timer.schedule(new DeliverDelayedMessageTimerTask(level, offset), FIRST_DELAY_TIME); &#125; &#125; // 添加一个专门执行延迟消息持久化的定时任务 this.timer.scheduleAtFixedRate(new TimerTask() &#123; @Override public void run() &#123; try &#123; if (started.get()) ScheduleMessageService.this.persist(); &#125; catch (Throwable e) &#123; log.error("scheduleAtFixedRate flush exception", e); &#125; &#125; &#125;, 10000, this.defaultMessageStore.getMessageStoreConfig().getFlushDelayOffsetInterval()); &#125; &#125; 核心实现在DeliverDelayedMessageTimerTask类，其主要逻辑在于扫描延迟消息SCHEDULE_TOPIC_XXXX的队列下的消息，将延迟消息转换成指定Topic的消息123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111 public void executeOnTimeup() &#123; ConsumeQueue cq = ScheduleMessageService.this.defaultMessageStore.findConsumeQueue(SCHEDULE_TOPIC, delayLevel2QueueId(delayLevel)); //读取队列SCHEDULE_TOPIC_XXXX，其中不同的延迟级别对应不同的队列id（queueId=delayLevel-1） long failScheduleOffset = offset; if (cq != null) &#123; SelectMappedBufferResult bufferCQ = cq.getIndexBuffer(this.offset); if (bufferCQ != null) &#123; try &#123; long nextOffset = offset; int i = 0; ConsumeQueueExt.CqExtUnit cqExtUnit = new ConsumeQueueExt.CqExtUnit(); for (; i &lt; bufferCQ.getSize(); i += ConsumeQueue.CQ_STORE_UNIT_SIZE) &#123; // 循环读取延迟消息 long offsetPy = bufferCQ.getByteBuffer().getLong(); int sizePy = bufferCQ.getByteBuffer().getInt(); long tagsCode = bufferCQ.getByteBuffer().getLong(); if (cq.isExtAddr(tagsCode)) &#123; if (cq.getExt(tagsCode, cqExtUnit)) &#123; tagsCode = cqExtUnit.getTagsCode(); &#125; else &#123; //can't find ext content.So re compute tags code. log.error("[BUG] can't find consume queue extend file content!addr=&#123;&#125;, offsetPy=&#123;&#125;, sizePy=&#123;&#125;", tagsCode, offsetPy, sizePy); long msgStoreTime = defaultMessageStore.getCommitLog().pickupStoreTimestamp(offsetPy, sizePy); tagsCode = computeDeliverTimestamp(delayLevel, msgStoreTime); &#125; &#125; long now = System.currentTimeMillis(); long deliverTimestamp = this.correctDeliverTimestamp(now, tagsCode); nextOffset = offset + (i / ConsumeQueue.CQ_STORE_UNIT_SIZE); long countdown = deliverTimestamp - now; if (countdown &lt;= 0) &#123; // 只有当延迟消息发送的消息到达了才处理 MessageExt msgExt = ScheduleMessageService.this.defaultMessageStore.lookMessageByOffset( offsetPy, sizePy); // 根据offset值读取SCHEDULE_TOPIC_XXXX队列的消息 if (msgExt != null) &#123; try &#123; MessageExtBrokerInner msgInner = this.messageTimeup(msgExt); // 将读取的消息转换为真实topic的消息（也就是普通消息） PutMessageResult putMessageResult = ScheduleMessageService.this.writeMessageStore .putMessage(msgInner); // 存放该消息 if (putMessageResult != null &amp;&amp; putMessageResult.getPutMessageStatus() == PutMessageStatus.PUT_OK) &#123; continue; &#125; else &#123; // XXX: warn and notify me log.error( "ScheduleMessageService, a message time up, but reput it failed, topic: &#123;&#125; msgId &#123;&#125;", msgExt.getTopic(), msgExt.getMsgId()); ScheduleMessageService.this.timer.schedule( new DeliverDelayedMessageTimerTask(this.delayLevel, nextOffset), DELAY_FOR_A_PERIOD); ScheduleMessageService.this.updateOffset(this.delayLevel, nextOffset); return; &#125; &#125; catch (Exception e) &#123; /* * XXX: warn and notify me */ log.error( "ScheduleMessageService, messageTimeup execute error, drop it. msgExt=" + msgExt + ", nextOffset=" + nextOffset + ",offsetPy=" + offsetPy + ",sizePy=" + sizePy, e); &#125; &#125; &#125; else &#123; ScheduleMessageService.this.timer.schedule( new DeliverDelayedMessageTimerTask(this.delayLevel, nextOffset), countdown); ScheduleMessageService.this.updateOffset(this.delayLevel, nextOffset); return; &#125; &#125; // end of for// 计算下一次读取延迟队列的offset，是定时任务下一次从该位置读取延时消息 nextOffset = offset + (i / ConsumeQueue.CQ_STORE_UNIT_SIZE); ScheduleMessageService.this.timer.schedule(new DeliverDelayedMessageTimerTask( this.delayLevel, nextOffset), DELAY_FOR_A_WHILE); // 将下一次读取延迟队列的offset存放到一个缓存map中 ScheduleMessageService.this.updateOffset(this.delayLevel, nextOffset); return; &#125; finally &#123; bufferCQ.release(); &#125; &#125; // end of if (bufferCQ != null) else &#123; long cqMinOffset = cq.getMinOffsetInQueue(); if (offset &lt; cqMinOffset) &#123; failScheduleOffset = cqMinOffset; log.error("schedule CQ offset invalid. offset=" + offset + ", cqMinOffset=" + cqMinOffset + ", queueId=" + cq.getQueueId()); &#125; &#125; &#125; // end of if (cq != null) ScheduleMessageService.this.timer.schedule(new DeliverDelayedMessageTimerTask(this.delayLevel, failScheduleOffset), DELAY_FOR_A_WHILE); &#125; 这里的流程，大致为读取延迟队列的消息，取得消息的开始位置offset，将延迟消息转换为指定topic的普通消息并存起来，修改下一次读取的offset，改的是在内存中的offset而非文件中的，并指定下一次转换延迟消息的TimeTask。 至于持久化offset，在另一个定时任务ScheduleMessageService的persist()方法中实现1234567891011public synchronized void persist() &#123; String jsonString = this.encode(true); if (jsonString != null) &#123; String fileName = this.configFilePath(); try &#123; MixAll.string2File(jsonString, fileName); &#125; catch (IOException e) &#123; log.error("persist file " + fileName + " exception", e); &#125; &#125;&#125; 综上，延迟消息的核心实现比之普通消息，只是多了一道将从延迟消息Topic的队列中取出延迟消息以转储到普通消息Topic下的处理，其余逻辑与普通消息无异。 总结消息消费部分是实现过程细节众多的一个模块，当中包括消息消费方式、队列重新负载、消息拉取、消费处理、进度存储及同步、定时消息（延迟消息）、保证顺序消费等等。RocketMQ的消息消费方式包括集群模式与广播模式，不同模式有不同的处理逻辑。在拉取消息时会因应不同的消息处理情况作出不同的处理，譬如引发队列的重新负载，根据当前消费组内消费者个数与主题队列数量按照某一种负载算法进行队列分配，分配原则为同一个消费者可以分配多个消息消费队列，同一个消息消费队列同一时间只会分配给一个消费者。消息拉取由 PullMessageService 线程根据 RebalanceService线程创建的拉取任务进行拉取，默认一批拉取32条消息，提交给消费者消费线程池后继续下一次的消息拉取。如果消息消费过慢产生消息堆积会触发消息消费拉取流控，流控针对的维度目前分别有：消息消费数量、消息大小、偏移量间隔。消息消费是在消费线程池中并发地对同一消息消费队列的消息进行消费，消费成功后，取出消息处理队列中最小的消息偏移量作为消息消费进度偏移量存在于消息消费进度存储文件中，集群模式消息进度存储在 Broker，广播模式消息进度存储在Consumer端。如果业务方返回 RECONSUME_LATER ，则 RocketMQ 启用消息消费重试机制，将原消息的主题与队列存储在消息属性中，将消息存储在主题名为SCHEDULE_TOPIC_XXXX的消息消费队列中，等待指定时间后，RocketMQ自动将该消息重新拉取并再次将消息存储在commitlog进而转发到普通消息消费队列供消费者消费，消息消费重试主题为%RETRY%消费者组名。延迟消息并不支持任意精度的延迟时间调度，只支持定义好的延迟级别，可通过在broker配置文件中设置messageDelayLevel。延迟消息更多地使用在辅助消费的场景下。]]></content>
  </entry>
  <entry>
    <title><![CDATA[RocketMQ消息存储]]></title>
    <url>%2F2019%2F06%2F02%2FRocketMQ-Message-Storage%2F</url>
    <content type="text"><![CDATA[存储概要设计每个Broker都对应有一个MessageStore，专门用来存储发送到它的消息，不过MessageStore本身不是文件，只是存储的一个抽象，MessageStore 中保存着一个 CommitLog，CommitLog 维护了一个 MappedFileQueue，而MappedFileQueue 中又维护了多个 MappedFile，每个MappedFile都会映射到文件系统中一个文件，这些文件才是真正的存储消息的地方，MappedFile的文件名为它记录的第一条消息的全局物理偏移量。 消息接收Broker对于每个请求 Code，都注册了对应的处理类，其中用于接收消息的处理类为：org.apache.rocketmq.broker.processor.SendMessageProcessor。 123this.remotingServer.registerProcessor(RequestCode.SEND_MESSAGE, sendProcessor, this.sendMessageExecutor);this.remotingServer.registerProcessor(RequestCode.SEND_MESSAGE_V2, sendProcessor, this.sendMessageExecutor);this.remotingServer.registerProcessor(RequestCode.SEND_BATCH_MESSAGE, sendProcessor, this.sendMessageExecutor); SendMessageProcessor实现了 org.apache.rocketmq.remoting.netty.NettyRequestProcessor 接口，并在接口方法processRequest()中处理接收到的请求，SendMessageProcessor在processRequest()中调用了sendMessage()方法来进行消息处理。 org.apache.rocketmq.broker.processor.SendMessageProcessor.processRequest(ChannelHandlerContext, RemotingCommand)1234567891011121314151617181920212223242526public RemotingCommand processRequest(ChannelHandlerContext ctx, RemotingCommand request) throws RemotingCommandException &#123; SendMessageContext mqtraceContext; switch (request.getCode()) &#123; case RequestCode.CONSUMER_SEND_MSG_BACK: return this.consumerSendMsgBack(ctx, request); default: SendMessageRequestHeader requestHeader = parseRequestHeader(request); if (requestHeader == null) &#123; return null; &#125; mqtraceContext = buildMsgContext(ctx, requestHeader); this.executeSendMessageHookBefore(ctx, request, mqtraceContext); RemotingCommand response; if (requestHeader.isBatch()) &#123; response = this.sendBatchMessage(ctx, request, mqtraceContext, requestHeader); &#125; else &#123; response = this.sendMessage(ctx, request, mqtraceContext, requestHeader); &#125; this.executeSendMessageHookAfter(response, mqtraceContext); return response; &#125;&#125; 消息存储SendMessageProcessor-sendMessage()接收到消息请求后，就要处理请求了，上面调用了sendMessage()来处理消息SendMessageProcessor 中 sendMessage() 中主要分为下面几步： 根据收到请求，封装成内部消息结构：MessageExtBrokerInner。 org.apache.rocketmq.broker.processor.SendMessageProcessor.sendMessage(ChannelHandlerContext, RemotingCommand, SendMessageContext, SendMessageRequestHeader)1234567891011121314151617final byte[] body = request.getBody();...MessageExtBrokerInner msgInner = new MessageExtBrokerInner();msgInner.setTopic(requestHeader.getTopic());msgInner.setQueueId(queueIdInt); ...msgInner.setBody(body);msgInner.setFlag(requestHeader.getFlag());MessageAccessor.setProperties(msgInner, MessageDecoder.string2messageProperties(requestHeader.getProperties()));msgInner.setPropertiesString(requestHeader.getProperties());msgInner.setBornTimestamp(requestHeader.getBornTimestamp());msgInner.setBornHost(ctx.channel().remoteAddress());msgInner.setStoreHost(this.getStoreHost());msgInner.setReconsumeTimes(requestHeader.getReconsumeTimes() == null ? 0 : requestHeader.getReconsumeTimes()); 调用 Broker 中 的MessageStore的putMessage() 方法，将消息放入MessageStore中。 org.apache.rocketmq.broker.processor.SendMessageProcessor.sendMessage(ChannelHandlerContext, RemotingCommand, SendMessageContext, SendMessageRequestHeader)1putMessageResult = this.brokerController.getMessageStore().putMessage(msgInner); 正如前面所述，每个Broker都有一个MessageStore实例，MessageStore本身是一个接口，定义了一些用来存储消息的接口协议，RocketMQ中MessageStore默认的实现类为DefaultMessageStore，Broker在其初始化方法initialize()中便会初始化好DefaultMessageStore。 DefaultMessageStore-putMessage()DefaultMessageStore 中 putMessage() 逻辑又分为下面几步：1). 检查当前Broker是否可以存储消息，比如 MessageStore 被关闭、Broker 状态为 Slave 都会拒绝存储。 org.apache.rocketmq.store.DefaultMessageStore.putMessage(MessageExtBrokerInner)123456789101112131415...if (this.shutdown) &#123; log.warn("message store has shutdown, so putMessage is forbidden"); return new PutMessageResult(PutMessageStatus.SERVICE_NOT_AVAILABLE, null); &#125; if (BrokerRole.SLAVE == this.messageStoreConfig.getBrokerRole()) &#123; long value = this.printTimes.getAndIncrement(); if ((value % 50000) == 0) &#123; log.warn("message store is slave mode, so putMessage is forbidden "); &#125; return new PutMessageResult(PutMessageStatus.SERVICE_NOT_AVAILABLE, null); &#125;... 2). 检查消息合法性，比如消息的Topic长度和内容长度是否超出限制。这种情况下也会拒绝存储。org.apache.rocketmq.store.DefaultMessageStore.putMessage(MessageExtBrokerInner)123456789if (msg.getTopic().length() &gt; Byte.MAX_VALUE) &#123; log.warn("putMessage message topic length too long " + msg.getTopic().length()); return new PutMessageResult(PutMessageStatus.MESSAGE_ILLEGAL, null);&#125;if (msg.getPropertiesString() != null &amp;&amp; msg.getPropertiesString().length() &gt; Short.MAX_VALUE) &#123; log.warn("putMessage message properties length too long " + msg.getPropertiesString().length()); return new PutMessageResult(PutMessageStatus.PROPERTIES_SIZE_EXCEEDED, null);&#125; 3). 如果消息通过了上面的重重考验，便会被提交给 MessageStore 中的 CommitLog，进行下一步处理。org.apache.rocketmq.store.DefaultMessageStore.putMessage(MessageExtBrokerInner)1PutMessageResult result = this.commitLog.putMessage(msg); 消息到了CommitLog后，便要开始进入存储逻辑了。我们来看看CommitLog中是如何处理消息的。 CommitLog-PutMessage() 获取写锁，保证同一时刻只处理一条消息的存储操作。 1putMessageLock.lock(); 从CommitLog的Message 中获取最新的MappedFile，追加消息。 12MappedFile mappedFile = this.mappedFileQueue.getLastMappedFile();result = mappedFile.appendMessage(msg, this.appendMessageCallback); 前面介绍到，CommitLog 中保存了一个MappedFileQueue，MappedFileQueue 初始化的时候配置了消息文件MappedFile的存储路径以及单个MappedFile文件的大小，当某个消息文件写满后，便会生成一个新的MappedFile继续写入消息，所以MappedFileQueue中会按照消息写入时间顺序，维护多个MappedFile。 消息追加结束后，释放写锁1putMessageLock.unlock(); 上面这几步中，重点关注的是第2步，即将消息追加到当前最新的MappedFile中。 上面追加消息调用的是MappedFile.appendMessage()方法，此方法最终调用到MappedFile.appendMessagesInner() 中：org.apache.rocketmq.store.MappedFile.appendMessagesInner(MessageExt, AppendMessageCallback)12345678910111213141516171819202122assert messageExt != null; assert cb != null; int currentPos = this.wrotePosition.get(); if (currentPos &lt; this.fileSize) &#123; ByteBuffer byteBuffer = writeBuffer != null ? writeBuffer.slice() : this.mappedByteBuffer.slice(); byteBuffer.position(currentPos); AppendMessageResult result = null; if (messageExt instanceof MessageExtBrokerInner) &#123; result = cb.doAppend(this.getFileFromOffset(), byteBuffer, this.fileSize - currentPos, (MessageExtBrokerInner) messageExt); &#125; else if (messageExt instanceof MessageExtBatch) &#123; result = cb.doAppend(this.getFileFromOffset(), byteBuffer, this.fileSize - currentPos, (MessageExtBatch) messageExt); &#125; else &#123; return new AppendMessageResult(AppendMessageStatus.UNKNOWN_ERROR); &#125; this.wrotePosition.addAndGet(result.getWroteBytes()); this.storeTimestamp = result.getStoreTimestamp(); return result; &#125; log.error("MappedFile.appendMessage return null, wrotePosition: &#123;&#125; fileSize: &#123;&#125;", currentPos, this.fileSize); return new AppendMessageResult(AppendMessageStatus.UNKNOWN_ERROR); 获取MappedFile中的 writeBuffer，如果 writeBuffer 为空，则获取mappedByteBuffer。 在MessageStore初始化的时候，会初始化一个Buffer缓存池：TransientStorePool，TransientStorePool在初始化时会初始化若干DirectBuffer，放入一个Deque中，默认池子容量为5。MappedFile的writeBuffer就是从这个池子中获取的。而 mappedByteBuffer 类型为MappedByteBuffer，前面说到每个MappedFile都会映射到文件系统中的一个文件，mappedByteBuffer 即为该文件在内存中的映射。当追加消息到MappedFile中，会优先追加到 writeBuffer中。 调用 cb.doAppend()追加消息，调用该方法时，传入了下面几个参数 this.getFileFromOffset()：MappedFile的全局消息物理偏移量（即MappedFile中第一个消息全局物理偏移量，也是MappedFile的文件名）。byteBuffer：即MappedFile的内存缓冲区，也即是 1 中的writeBuffer或mappedByteBuffer。this.fileSize - currentPos：fileSize为单个文件的额定大小，默认为1GB，currentPos为当前文件中已经写到什么位置，两个相减即为当前文件剩余容量。(MessageExtBrokerInner) messageExt：这个没什么好说的，就是内部封装好的消息 cb 从哪来的呢？前面CommitLog在调用appendMessagesInner()时，传入的 cb 为：this.appendMessageCallback，它的类型为 DefaultAppendMessageCallback，实现了AppendMessageCallback接口。所以我们接下来就要看看DefaultAppendMessageCallback中对于doAppend()的实现即可。 doAppend() 主要逻辑如下： 计算消息存储的各个属性，如消息长度，消息在消息队列中的长度等。 12345678910111213141516String msgId = MessageDecoder.createMessageId(this.msgIdMemory, msgInner.getStoreHostBytes(hostHolder), wroteOffset);// Record ConsumeQueue informationkeyBuilder.setLength(0);keyBuilder.append(msgInner.getTopic());keyBuilder.append('-');keyBuilder.append(msgInner.getQueueId());String key = keyBuilder.toString();Long queueOffset = CommitLog.this.topicQueueTable.get(key);......final byte[] topicData = msgInner.getTopic().getBytes(MessageDecoder.CHARSET_UTF8);final int topicLength = topicData.length;final int bodyLength = msgInner.getBody() == null ? 0 : msgInner.getBody().length;final int msgLen = calMsgLength(bodyLength, topicLength, propertiesLength); 判断消息追加后是否超过单个MappedFile大小，如果超出，则返回状态码：AppendMessageStatus.END_OF_FILE 1234567891011121314// Determines whether there is sufficient free spaceif ((msgLen + END_FILE_MIN_BLANK_LENGTH) &gt; maxBlank) &#123; this.resetByteBuffer(this.msgStoreItemMemory, maxBlank); // 1 TOTALSIZE this.msgStoreItemMemory.putInt(maxBlank); // 2 MAGICCODE this.msgStoreItemMemory.putInt(CommitLog.BLANK_MAGIC_CODE); // 3 The remaining space may be any value // Here the length of the specially set maxBlank final long beginTimeMills = CommitLog.this.defaultMessageStore.now(); byteBuffer.put(this.msgStoreItemMemory.array(), 0, maxBlank); return new AppendMessageResult(AppendMessageStatus.END_OF_FILE, wroteOffset, maxBlank, msgId, msgInner.getStoreTimestamp(), queueOffset, CommitLog.this.defaultMessageStore.now() - beginTimeMills);&#125; 此时 CommitLog 会新创建一个MappedFile，重新追加消息。 123456789101112131415switch (result.getStatus()) &#123; ...... case END_OF_FILE: unlockMappedFile = mappedFile; // Create a new file, re-write the message mappedFile = this.mappedFileQueue.getLastMappedFile(0); if (null == mappedFile) &#123; // XXX: warn and notify me log.error("create mapped file2 error, topic: " + msg.getTopic() + " clientAddr: " + msg.getBornHostString()); beginTimeInLock = 0; return new PutMessageResult(PutMessageStatus.CREATE_MAPEDFILE_FAILED, result); &#125; result = mappedFile.appendMessage(msg, this.appendMessageCallback); break;&#125; 序列化消息内容，存储到内存缓存区中 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748// Initialization of storage spacethis.resetByteBuffer(msgStoreItemMemory, msgLen);// 1 TOTALSIZEthis.msgStoreItemMemory.putInt(msgLen);// 2 MAGICCODEthis.msgStoreItemMemory.putInt(CommitLog.MESSAGE_MAGIC_CODE);// 3 BODYCRCthis.msgStoreItemMemory.putInt(msgInner.getBodyCRC());// 4 QUEUEIDthis.msgStoreItemMemory.putInt(msgInner.getQueueId());// 5 FLAGthis.msgStoreItemMemory.putInt(msgInner.getFlag());// 6 QUEUEOFFSETthis.msgStoreItemMemory.putLong(queueOffset);// 7 PHYSICALOFFSETthis.msgStoreItemMemory.putLong(fileFromOffset + byteBuffer.position());// 8 SYSFLAGthis.msgStoreItemMemory.putInt(msgInner.getSysFlag());// 9 BORNTIMESTAMPthis.msgStoreItemMemory.putLong(msgInner.getBornTimestamp());// 10 BORNHOSTthis.resetByteBuffer(hostHolder, 8);this.msgStoreItemMemory.put(msgInner.getBornHostBytes(hostHolder));// 11 STORETIMESTAMPthis.msgStoreItemMemory.putLong(msgInner.getStoreTimestamp());// 12 STOREHOSTADDRESSthis.resetByteBuffer(hostHolder, 8);this.msgStoreItemMemory.put(msgInner.getStoreHostBytes(hostHolder));//this.msgBatchMemory.put(msgInner.getStoreHostBytes());// 13 RECONSUMETIMESthis.msgStoreItemMemory.putInt(msgInner.getReconsumeTimes());// 14 Prepared Transaction Offsetthis.msgStoreItemMemory.putLong(msgInner.getPreparedTransactionOffset());// 15 BODYthis.msgStoreItemMemory.putInt(bodyLength);if (bodyLength &gt; 0) this.msgStoreItemMemory.put(msgInner.getBody());// 16 TOPICthis.msgStoreItemMemory.put((byte) topicLength);this.msgStoreItemMemory.put(topicData);// 17 PROPERTIESthis.msgStoreItemMemory.putShort((short) propertiesLength);if (propertiesLength &gt; 0) this.msgStoreItemMemory.put(propertiesData);final long beginTimeMills = CommitLog.this.defaultMessageStore.now();// Write messages to the queue bufferbyteBuffer.put(this.msgStoreItemMemory.array(), 0, msgLen); 返回追加成功的结果 12AppendMessageResult result = new AppendMessageResult(AppendMessageStatus.PUT_OK, wroteOffset, msgLen, msgId,msgInner.getStoreTimestamp(), queueOffset, CommitLog.this.defaultMessageStore.now() - beginTimeMills); 文件存储及刷盘策略RocketMQ支持的刷盘策略有两种： 同步刷盘同步的意思就是说当消息追加到内存后，就立即刷到文件中存储。 异步刷盘当消息追加到内存中，并不是理解刷到文件中，而是在后台任务中进行异步操作。 RocketMQ默认采用异步刷盘策略。 当CommitLog在putMessage()中收到MappedFile成功追加消息到内存的结果后，便会调用handleDiskFlush()方法进行刷盘，将消息存储到文件中。handleDiskFlush() 便会根据两种刷盘策略，调用不同的刷盘服务。 org.apache.rocketmq.store.CommitLog.CommitLog(DefaultMessageStore)12345if (FlushDiskType.SYNC_FLUSH == defaultMessageStore.getMessageStoreConfig().getFlushDiskType()) &#123; this.flushCommitLogService = new GroupCommitService();&#125; else &#123; this.flushCommitLogService = new FlushRealTimeService();&#125; 同步刷盘同步刷盘的服务为GroupCommitService，主要逻辑如下： handleDiskFlush()中提交刷盘请求 1234final GroupCommitService service = (GroupCommitService) this.flushCommitLogService;GroupCommitRequest request = new GroupCommitRequest(result.getWroteOffset() + result.getWroteBytes());service.putRequest(request); 同步等待刷盘结果，刷盘失败也会标志消息存储失败，返回 FLUSH_DISK_TIMEOUT 123456boolean flushOK = request.waitForFlush(this.defaultMessageStore.getMessageStoreConfig().getSyncFlushTimeout());if (!flushOK) &#123; log.error("do groupcommit, wait for flush failed, topic: " + messageExt.getTopic() + " tags: " + messageExt.getTags() + " client address: " + messageExt.getBornHostString()); putMessageResult.setPutMessageStatus(PutMessageStatus.FLUSH_DISK_TIMEOUT);&#125; 进行同步刷盘的服务为 GroupCommitService，当请求被提交给GroupCommitService后，GroupCommitService并不是立即处理，而是先放到内部的一个请求队列中，并利用waitPoint通知新请求到来。 12345678public synchronized void putRequest(final GroupCommitRequest request) &#123; synchronized (this.requestsWrite) &#123; this.requestsWrite.add(request); &#125; if (hasNotified.compareAndSet(false, true)) &#123; waitPoint.countDown(); // notify &#125;&#125; 当 GroupCommitService 被唤醒后，便会将 requestsWrite 中的请求交换到 requestsRead中，避免产生锁竞争。12345private void swapRequests() &#123; List&lt;GroupCommitRequest&gt; tmp = this.requestsWrite; this.requestsWrite = this.requestsRead; this.requestsRead = tmp;&#125; GroupCommitService 在启动后会在死循环中调用doCommit()方法，而doCommit()则不断遍历requestsRead中的请求，进行处理： 12345678910111213141516171819202122232425262728293031private void doCommit() &#123; synchronized (this.requestsRead) &#123; if (!this.requestsRead.isEmpty()) &#123; for (GroupCommitRequest req : this.requestsRead) &#123; // There may be a message in the next file, so a maximum of // two times the flush boolean flushOK = false; for (int i = 0; i &lt; 2 &amp;&amp; !flushOK; i++) &#123; flushOK = CommitLog.this.mappedFileQueue.getFlushedWhere() &gt;= req.getNextOffset(); if (!flushOK) &#123; CommitLog.this.mappedFileQueue.flush(0); &#125; &#125; req.wakeupCustomer(flushOK); &#125; long storeTimestamp = CommitLog.this.mappedFileQueue.getStoreTimestamp(); if (storeTimestamp &gt; 0) &#123; CommitLog.this.defaultMessageStore.getStoreCheckpoint().setPhysicMsgTimestamp(storeTimestamp); &#125; this.requestsRead.clear(); &#125; else &#123; // Because of individual messages is set to not sync flush, it // will come to this process CommitLog.this.mappedFileQueue.flush(0); &#125; &#125;&#125; 可见这里最终调用了CommitLog.this.mappedFileQueue.flush(0) 来进行刷盘。 同步刷盘的任务虽然也是在异步线程中执行，但是消息存储的主流程中会同步等待刷盘结果，所以本质上还是同步操作。 异步刷盘同步刷盘的服务为FlushRealTimeService，不过当内存缓存池TransientStorePool 可用时，消息会先提交到TransientStorePool 中的WriteBuffer内部，再提交到MappedFile的FileChannle中，此时异步刷盘服务就是 CommitRealTimeService，它继承自 FlushRealTimeService。 我们别管那么多，先看看FlushRealTimeService中的主要逻辑吧： handleDiskFlush()中直接唤醒异步刷盘服务 1flushCommitLogService.wakeup(); FlushRealTimeService 在启动后，会在死循环中周期性的进行刷盘操作，主要逻辑如下。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253while (!this.isStopped()) &#123; // 休眠策略，为 true 时，调用 Thread.sleep()休眠，为false时，调用wait()休眠，默认 false boolean flushCommitLogTimed = CommitLog.this.defaultMessageStore.getMessageStoreConfig().isFlushCommitLogTimed(); // 获取刷盘周期，默认为 500 ms int interval = CommitLog.this.defaultMessageStore.getMessageStoreConfig().getFlushIntervalCommitLog(); // 每次刷盘至少要刷多少页内容，每页大小为 4 k，默认每次要刷 4 页 int flushPhysicQueueLeastPages = CommitLog.this.defaultMessageStore.getMessageStoreConfig().getFlushCommitLogLeastPages(); // 两次刷写之间的最大时间间隔，默认 10 s int flushPhysicQueueThoroughInterval = CommitLog.this.defaultMessageStore.getMessageStoreConfig().getFlushCommitLogThoroughInterval(); boolean printFlushProgress = false; // Print flush progress long currentTimeMillis = System.currentTimeMillis(); // 判断当前时间距离上次刷盘时间是否已经超出设置的两次刷盘最大间隔 if (currentTimeMillis &gt;= (this.lastFlushTimestamp + flushPhysicQueueThoroughInterval)) &#123; this.lastFlushTimestamp = currentTimeMillis; // 如果已经超时，则将flushPhysicQueueLeastPages设置为0，表明将所有内存缓存全部刷到文件中 flushPhysicQueueLeastPages = 0; printFlushProgress = (printTimes++ % 10) == 0; &#125; try &#123; // 根据不同休眠策略，进行休眠等待 if (flushCommitLogTimed) &#123; Thread.sleep(interval); &#125; else &#123; this.waitForRunning(interval); &#125; if (printFlushProgress) &#123; this.printFlushProgress(); &#125; long begin = System.currentTimeMillis(); // 休眠结束，开始执行刷盘操作 CommitLog.this.mappedFileQueue.flush(flushPhysicQueueLeastPages); long storeTimestamp = CommitLog.this.mappedFileQueue.getStoreTimestamp(); if (storeTimestamp &gt; 0) &#123; CommitLog.this.defaultMessageStore.getStoreCheckpoint().setPhysicMsgTimestamp(storeTimestamp); &#125; long past = System.currentTimeMillis() - begin; if (past &gt; 500) &#123; log.info("Flush data to disk costs &#123;&#125; ms", past); &#125; &#125; catch (Throwable e) &#123; CommitLog.log.warn(this.getServiceName() + " service has exception. ", e); this.printFlushProgress(); &#125;&#125; 通过上面这段逻辑可知，异步刷盘就在异步线程中，周期性的将内存缓冲区的内容刷到文件中，在消息主流程中，只会唤醒异步刷盘线程，而不会同步等待刷盘结果，所以称为异步刷盘。 MappedFile的刷盘两种刷盘策略，最终都调用了下面这个方法进行刷盘1CommitLog.this.mappedFileQueue.flush(flushPhysicQueueLeastPages); 是时候看看mappedFileQueue.flush()中做了什么了。 从mappedFileQueue保存的所有MappedFile中，找出所要刷盘的MappedFile 1MappedFile mappedFile = this.findMappedFileByOffset(this.flushedWhere, this.flushedWhere == 0); flushedWhere 记录了最后一条被刷到文件的内容的全局物理偏移量。所以此次刷盘就要根据偏移量，找到本次要刷盘的起始点位于哪个MappedFile。 如果找到了对应的MappedFile，则对该MappedFile中的内容执行刷盘操作，并更新flushedWhere。 12345678910if (mappedFile != null) &#123; long tmpTimeStamp = mappedFile.getStoreTimestamp(); int offset = mappedFile.flush(flushLeastPages); long where = mappedFile.getFileFromOffset() + offset; result = where == this.flushedWhere; this.flushedWhere = where; if (0 == flushLeastPages) &#123; this.storeTimestamp = tmpTimeStamp; &#125;&#125; 刷盘的终极目的地就在MappedFile的flush()方法中，具体也分为下面几步： 判断是否满足刷盘条件1if (this.isAbleToFlush(flushLeastPages)) isAbleToFlush()其实就是判断当前剩余未刷盘内容长度，是否超过最小刷盘长度：flushLeastPages，避免不必要的刷盘操作。 1234567891011121314private boolean isAbleToFlush(final int flushLeastPages) &#123; int flush = this.flushedPosition.get(); int write = getReadPosition(); if (this.isFull()) &#123; return true; &#125; if (flushLeastPages &gt; 0) &#123; return ((write / OS_PAGE_SIZE) - (flush / OS_PAGE_SIZE)) &gt;= flushLeastPages; &#125; return write &gt; flush;&#125; 如果满足刷盘条件，则将内存中的内容刷到文件中。1234567// 如果writeBuffer不为空，则表明消息是先提交到writeBuffer中，已经从writeBuffer提交到fileChannel，直接调用fileChannel.force()if (writeBuffer != null || this.fileChannel.position() != 0) &#123; this.fileChannel.force(false);&#125; else &#123; // 反之，消息是直接存储在文件内存映射缓冲区mappedByteBuffer中，直接调用它的force()即可 this.mappedByteBuffer.force();&#125; 到这儿，消息就成功的从内存中存储到文件内部了。 消息索引ConsumerQueueRocketMQ为了保证消息发送的高吞吐量，采用单一文件存储（CommitLog）所有Topic的消息，从而保证消息存储是完全的顺序写，这是写消息的高性能所在，但是这样给文件读取带来了不便，试想一下如果消费者直接从CommitLog中去遍历查找所订阅的Topic下的消息，效率将极其低下。为了适应消息消费的检索需求，RocketMQ设计了消息消费队列文件（ConsumeQueue），ConsumeQueue可以看成是CommitLog关于消息消费的“索引”文件，ConsumeQueue的第一级目录为Topic，第二级目录为Topic底下的消息队列（MessageQueue）。 为了加速 ConsumeQueue 消息条目的检索速度与节省磁盘空间，每一个 ConsumeQueue 条目不会存储消息的全量信息，它存储自己所属Topic的消息在CommitLog中的偏移量，这样当消费者从Broker拉取消息的时候，就可以快速根据偏移量定位到消息，其存储格式如图所示 单个 ConsumeQueue 文件默认包含30万个条目，也就是说单个文件的长度为30W x 20 字节。单个ConsumeQueue文件可以看作是一个 ConsumeQueue 条目的数组，数组下标为 ConsumeQueue 的逻辑偏移量，消息消费进度存储的偏移量即逻辑偏移量。 ConsumeQueue 构建机制是当消息到达 Commitlog 文件后 由专门的线程产生消息转发任务，从而构建消息消费队列文件与下文提到的索引文件。 ConsumeQueue本身同样是利用MappedFileQueue进行记录偏移量信息的，可见MappedFileQueue的设计多么美妙，它没有与消息进行耦合，而是设计成一个通用的存储功能。 先来看一下ConsumeQueue根据消息逻辑偏移量、时间戳查找消息的实现。 根据消息逻辑偏移量查找消息org.apache.rocketmq.store.ConsumeQueue.getIndexBuffer(long)123456789101112public SelectMappedBufferResult getIndexBuffer(final long startIndex) &#123; int mappedFileSize = this.mappedFileSize; long offset = startIndex * CQ_STORE_UNIT_SIZE; if (offset &gt;= this.getMinLogicOffset()) &#123; MappedFile mappedFile = this.mappedFileQueue.findMappedFileByOffset(offset); if (mappedFile != null) &#123; SelectMappedBufferResult result = mappedFile.selectMappedBuffer((int) (offset % mappedFileSize)); return result; &#125; &#125; return null; &#125; 根据 startIndex 获取消息消费队列条目。首先 startIndex * 20 算得在 consumeQueue 中的物理偏移 ，如果该 offset 小于 minLogicOffset，则返回 null，说明该消息已被删除；如果大于 minLogicOffset，则根据偏移量定位到具体的物理文件，然后通过 offset 与物理文大小取模获取在该文件的偏移，从而从偏移量开始连续读取 20个字节即可。 根据时间戳查找消息由于方法代码篇幅较长，以下分段进行解析 org.apache.rocketmq.store.ConsumeQueue.getOffsetInQueueByTime(long)1234567891011121314public long getOffsetInQueueByTime(final long timestamp) &#123; MappedFile mappedFile = this.mappedFileQueue.getMappedFileByTime(timestamp); if (mappedFile != null) &#123; long offset = 0; int low = minLogicOffset &gt; mappedFile.getFileFromOffset() ? (int) (minLogicOffset - mappedFile.getFileFromOffset()) : 0; int high = 0; int midOffset = -1, targetOffset = -1, leftOffset = -1, rightOffset = -1; long leftIndexValue = -1L, rightIndexValue = -1L; long minPhysicOffset = this.defaultMessageStore.getMinPhyOffset(); SelectMappedBufferResult sbr = mappedFile.selectMappedBuffer(0); if (null != sbr) &#123; ByteBuffer byteBuffer = sbr.getByteBuffer(); high = byteBuffer.limit() - CQ_STORE_UNIT_SIZE;... 首先根据时间戳定位到物理文件，其具体实现就是从首个文件开始找，直到找到首次出现更新时间大于该时间戳的文件。 org.apache.rocketmq.store.ConsumeQueue.getOffsetInQueueByTime(long)12345678910111213141516171819202122232425262728293031... try &#123; while (high &gt;= low) &#123; midOffset = (low + high) / (2 * CQ_STORE_UNIT_SIZE) * CQ_STORE_UNIT_SIZE; byteBuffer.position(midOffset); long phyOffset = byteBuffer.getLong(); int size = byteBuffer.getInt(); if (phyOffset &lt; minPhysicOffset) &#123; low = midOffset + CQ_STORE_UNIT_SIZE; leftOffset = midOffset; continue; &#125; long storeTime = this.defaultMessageStore.getCommitLog().pickupStoreTimestamp(phyOffset, size); if (storeTime &lt; 0) &#123; return 0; &#125; else if (storeTime == timestamp) &#123; targetOffset = midOffset; break; &#125; else if (storeTime &gt; timestamp) &#123; high = midOffset - CQ_STORE_UNIT_SIZE; rightOffset = midOffset; rightIndexValue = storeTime; &#125; else &#123; low = midOffset + CQ_STORE_UNIT_SIZE; leftOffset = midOffset; leftIndexValue = storeTime; &#125; &#125;... 采用二分查找来加速检索。首先计算最低查找偏移量，取消息队列最小偏移量与该文件最小偏移量二者中的较小者作为low，获得当前存储文件中有效的最小消息物理偏移量minPhysicOffset，如果查找到消息偏移量小于该物理偏移量，则结束该查找过程。二分查找的常规退出循环的条件为（ low &gt; high ），首先查找中间的偏移量 midOffset，将 ConsumeQueue 文件对应的 ByteBuffer 定位到 midOffset ，然后读取4个字节获取该消息的物理偏移量 offset。1) 如果得到的物理偏移量小于当前的最小物理偏移量，说明待查找的物理偏移量肯定大于 midOffset，所以将 low 设置为 midOffset ，然后继续折半查找；2) 如果 offset 大于最小物理偏移，说明该消息是有效消息，则根据消息偏移量和消息长度获取消息的存储时间戳；3) 如果存储时间小于0，消息为无效消息，直接返回0；4) 如果存储时间戳等于待查找时间戳，说明查找到匹配消息，设置 targetOffset 并跳出循环；5) 如果存储时间戳大于待查找时间戳，说明待查找信息小于 midOffset ，则设置 high 为 midOffset 并设置 rightlndexValue 等于 midOffset；6) 如果存储时间小于待查找时间戳，说明待查找消息在大于 midOffset ，则设置 low 为 midOffset ，并设置 leftIndexValue 等于 midOffset。 org.apache.rocketmq.store.ConsumeQueue.getOffsetInQueueByTime(long) 12345678910111213141516171819202122232425 if (targetOffset != -1) &#123; offset = targetOffset; &#125; else &#123; if (leftIndexValue == -1) &#123; offset = rightOffset; &#125; else if (rightIndexValue == -1) &#123; offset = leftOffset; &#125; else &#123; offset = Math.abs(timestamp - leftIndexValue) &gt; Math.abs(timestamp - rightIndexValue) ? rightOffset : leftOffset; &#125; &#125; return (mappedFile.getFileFromOffset() + offset) / CQ_STORE_UNIT_SIZE; &#125; finally &#123; sbr.release(); &#125; &#125; &#125; return 0;&#125; 如果 targetOffset 不等于-1，表示找到了存储时间戳等于待查找时间的消息；如果 leftIndexValue 等于-1，表示返回比当前时间戳大并且最接近待查找时间的偏移量；如果 rightIndexValue 等于-1，表示返回的消息比待查找时间戳小并且最接近查找的偏移量。 索引文件因为ConsumeQueue中没有存储消息ID，如果我们需要根据消息ID来查找消息，那么，在不采取其它措施来查找消息的情况下，就又得遍历CommitLog文件了，于是，为了满足这个需求，RocketMQ采用了索引文件（IndexFile）。 RocketMQ引入了Hash索引机制为消息建立索引，HashMap的设计，包含两个基本点：Hash槽与解决Hash冲突的链表结构。IndexFile布局如图 消息的索引信息是存放在磁盘上的，文件以时间戳命名的，默认存放在 $ROCKETMQ_HOME/store/index 目录下。由上图来看，一个索引文件的结构被分成了三部分: 前 40 个字节存放固定的索引头(IndexHeader)信息，包含了存放在这个索引文件中的消息的最小/大存储时间、最小/大偏移量等状况 中间一段存储了 500 万个哈希槽位，每个槽内部存储的是索引文件的地址 (索引槽) 最后一段存储了 2000 万个索引内容信息，是实际的索引信息存储的地方。每一个槽位存储了这条消息的键哈希值、存储偏移量、存储时间戳与下一个索引槽地址 事实上，RocketMQ 在内存中还维护了一个索引文件列表，对于每一个索引文件，前一个文件的最大存储时间是下一个文件的最小存储时间，前一个文件的最大偏移量是下一个文件的最小偏移量。每一个索引文件都索引了在某个时间段内、某个偏移量段内的所有消息，当文件满了，就会用前一个文件的最大偏移量和最大存储时间作为起始值，创建下一个索引文件: IndexFile文件的数据写入org.apache.rocketmq.store.index.IndexFile.putKey(String, long, long)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596/** * * @param key topic + uniqKey * @param phyOffset 物理偏移量 * @param storeTimestamp * @return */ public boolean putKey(final String key, final long phyOffset, final long storeTimestamp) &#123; //1、判断index是否已满，返回失败 if (this.indexHeader.getIndexCount() &lt; this.indexNum) &#123; //2、计算key的非负数hashCode int keyHash = indexKeyHashMethod(key); //3、key应该存放的slot keyHash % 500W int slotPos = keyHash % this.hashSlotNum; //3、slot的数据存放位置 40 + keyHash %（500W）* 4 int absSlotPos = IndexHeader.INDEX_HEADER_SIZE + slotPos * hashSlotSize; FileLock fileLock = null; try &#123; // fileLock = this.fileChannel.lock(absSlotPos, hashSlotSize, // false); //5、如果存在hash冲突，获取这个slot存的前一个index的计数，如果没有则值为0 int slotValue = this.mappedByteBuffer.getInt(absSlotPos); if (slotValue &lt;= invalidIndex || slotValue &gt; this.indexHeader.getIndexCount()) &#123; slotValue = invalidIndex; &#125; //6、计算当前msg的存储时间和第一条msg相差秒数 long timeDiff = storeTimestamp - this.indexHeader.getBeginTimestamp(); //这里为了节约空间；直接timestamp是8位 timeDiff = timeDiff / 1000; if (this.indexHeader.getBeginTimestamp() &lt;= 0) &#123; timeDiff = 0; &#125; else if (timeDiff &gt; Integer.MAX_VALUE) &#123; timeDiff = Integer.MAX_VALUE; &#125; else if (timeDiff &lt; 0) &#123; timeDiff = 0; &#125; //7、获取该条index实际存储position //40 + 500W * 4 + index的顺序数 * 40； int absIndexPos = IndexHeader.INDEX_HEADER_SIZE + this.hashSlotNum * hashSlotSize + this.indexHeader.getIndexCount() * indexSize; //8、Index Linked list //topic+message key的hash值 this.mappedByteBuffer.putInt(absIndexPos, keyHash); //消息在CommitLog的物理文件地址, 可以直接查询到该消息(索引的核心机制) this.mappedByteBuffer.putLong(absIndexPos + 4, phyOffset); //消息的落盘时间与header里的beginTimestamp的差值(为了节省存储空间，如果直接存message的落盘时间就得8bytes) this.mappedByteBuffer.putInt(absIndexPos + 4 + 8, (int) timeDiff); //9、记录该slot上一个index //hash冲突处理的关键之处, 相同hash值上一个消息索引的index(如果当前消息索引是该hash值的第一个索引，则prevIndex=0, 也是消息索引查找时的停止条件)，每个slot位置的第一个消息的prevIndex就是0的 this.mappedByteBuffer.putInt(absIndexPos + 4 + 8 + 4, slotValue); //Slot Table //4字节 //10、记录该slot当前index，如果hash冲突（即absSlotPos一致）作为下一次该slot新增的前置index this.mappedByteBuffer.putInt(absSlotPos, this.indexHeader.getIndexCount()); //11、如果是第一条消息，更新header中的起始offset和起始time if (this.indexHeader.getIndexCount() &lt;= 1) &#123; this.indexHeader.setBeginPhyOffset(phyOffset); this.indexHeader.setBeginTimestamp(storeTimestamp); &#125; //12、累计indexHeader this.indexHeader.incHashSlotCount(); this.indexHeader.incIndexCount(); this.indexHeader.setEndPhyOffset(phyOffset); this.indexHeader.setEndTimestamp(storeTimestamp); return true; &#125; catch (Exception e) &#123; log.error("putKey exception, Key: " + key + " KeyHashCode: " + key.hashCode(), e); &#125; finally &#123; if (fileLock != null) &#123; try &#123; fileLock.release(); &#125; catch (IOException e) &#123; log.error("Failed to release the lock", e); &#125; &#125; &#125; &#125; else &#123; log.warn("Over index file capacity: index count = " + this.indexHeader.getIndexCount() + "; index max num = " + this.indexNum); &#125; return false; &#125; 其中，第5步判断slot的值是否存在，如存在则hash冲突，则在第9步把value设置为当前index的前一个index，到第10步将slot的值设置为当前index，这里有点类似HashMap的链表操作。值得注意的一点是，IndexFile 条目中存储的不是消息索引 key 而是消息属性 key 的 HashCode ，在 根据 key 查找时需要根据消息物理偏移量找到消息进而再验证消息 key 的值，之所以只存储 Hash Code 而不存储具体的 key 是为了将 Index 目设计为定长结构，才 能方便地检索 与定位条目。 索引文件检索索引生成的key是topic#MessageKey，所以两者作为参数传递进去查询消息org.apache.rocketmq.store.index.IndexService.queryOffset(String, String, int, long, long)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/** * * @param topic 按topic维度来查询消息，因为索引生成的时候key是用的topic#MessageKey * @param key MessageKey * @param maxNum 最多返回的消息数，因为key是由用户设置的，并不保证唯一，所以可能取到多个消息；同时index中只存储了hash，所以hash相同的消息也会取出来 * @param begin 起始时间 * @param end 结束时间 * @return */ public QueryOffsetResult queryOffset(String topic, String key, int maxNum, long begin, long end) &#123; List&lt;Long&gt; phyOffsets = new ArrayList&lt;Long&gt;(maxNum); long indexLastUpdateTimestamp = 0; long indexLastUpdatePhyoffset = 0; //不会超过64条 maxNum = Math.min(maxNum, this.defaultMessageStore.getMessageStoreConfig().getMaxMsgsNumBatch()); try &#123; this.readWriteLock.readLock().lock(); if (!this.indexFileList.isEmpty()) &#123; //1、从最后一个文件开始往前查找，最后一个文件是最新的 for (int i = this.indexFileList.size(); i &gt; 0; i--) &#123; IndexFile f = this.indexFileList.get(i - 1); boolean lastFile = i == this.indexFileList.size(); if (lastFile) &#123; indexLastUpdateTimestamp = f.getEndTimestamp(); indexLastUpdatePhyoffset = f.getEndPhyOffset(); &#125; //2、判断index文件的时间包含了begin和end的全部或者部分 if (f.isTimeMatched(begin, end)) &#123; //3、从index文件中获取offset f.selectPhyOffset(phyOffsets, buildKey(topic, key), maxNum, begin, end, lastFile); &#125; if (f.getBeginTimestamp() &lt; begin) &#123; break; &#125; if (phyOffsets.size() &gt;= maxNum) &#123; break; &#125; &#125; &#125; &#125; catch (Exception e) &#123; log.error("queryMsg exception", e); &#125; finally &#123; this.readWriteLock.readLock().unlock(); &#125; return new QueryOffsetResult(phyOffsets, indexLastUpdateTimestamp, indexLastUpdatePhyoffset); &#125; 物理偏移量查找org.apache.rocketmq.store.index.IndexFile.selectPhyOffset(List, String, int, long, long, boolean)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586public void selectPhyOffset(final List&lt;Long&gt; phyOffsets, final String key, final int maxNum, final long begin, final long end, boolean lock) &#123; if (this.mappedFile.hold()) &#123; //1、计算key的非负数hashCode int keyHash = indexKeyHashMethod(key); //2、key应该存放的slot keyHash % 500W int slotPos = keyHash % this.hashSlotNum; //3、slot的数据存放位置 40 + keyHash %（500W）* 4 int absSlotPos = IndexHeader.INDEX_HEADER_SIZE + slotPos * hashSlotSize; FileLock fileLock = null; try &#123; if (lock) &#123; // fileLock = this.fileChannel.lock(absSlotPos, // hashSlotSize, true); &#125; //4、获取slot最后存储的index位置进行回溯 int slotValue = this.mappedByteBuffer.getInt(absSlotPos); // if (fileLock != null) &#123; // fileLock.release(); // fileLock = null; // &#125; if (slotValue &lt;= invalidIndex || slotValue &gt; this.indexHeader.getIndexCount() || this.indexHeader.getIndexCount() &lt;= 1) &#123; &#125; else &#123; for (int nextIndexToRead = slotValue; ; ) &#123; //5、查询条目满足则返回 if (phyOffsets.size() &gt;= maxNum) &#123; break; &#125; //6、获取该条index实际存储position int absIndexPos = IndexHeader.INDEX_HEADER_SIZE + this.hashSlotNum * hashSlotSize + nextIndexToRead * indexSize; int keyHashRead = this.mappedByteBuffer.getInt(absIndexPos); //7、物理偏移量即commitLog的offset long phyOffsetRead = this.mappedByteBuffer.getLong(absIndexPos + 4); //当前msg的存储时间和第一条msg相差秒数 long timeDiff = (long) this.mappedByteBuffer.getInt(absIndexPos + 4 + 8); int prevIndexRead = this.mappedByteBuffer.getInt(absIndexPos + 4 + 8 + 4); if (timeDiff &lt; 0) &#123; break; &#125; timeDiff *= 1000L; long timeRead = this.indexHeader.getBeginTimestamp() + timeDiff; boolean timeMatched = (timeRead &gt;= begin) &amp;&amp; (timeRead &lt;= end); //8、hash一致并且时间在begin和end之间，加入结果集中 if (keyHash == keyHashRead &amp;&amp; timeMatched) &#123; phyOffsets.add(phyOffsetRead); &#125; //9、读取到0，说明没数据可读 if (prevIndexRead &lt;= invalidIndex || prevIndexRead &gt; this.indexHeader.getIndexCount() || prevIndexRead == nextIndexToRead || timeRead &lt; begin) &#123; break; &#125; //10、前一条不等于0，继续读取前一条，往前回溯 nextIndexToRead = prevIndexRead; &#125; &#125; &#125; catch (Exception e) &#123; log.error("selectPhyOffset exception ", e); &#125; finally &#123; if (fileLock != null) &#123; try &#123; fileLock.release(); &#125; catch (IOException e) &#123; log.error("Failed to release the lock", e); &#125; &#125; this.mappedFile.release(); &#125; &#125;&#125; 实时更新消息消费队列和索引文件消息消费队列文件（ConsumeQueue）和消息属性索引文件（IndexFile）都是基于CommitLog文件构建的，当Producer提交消息存储到CommitLog文件中，consumeQueue和IndexFile需要及时更新，保证消息消费的及时性。而RocketMQ是通过开启一个线程ReputMessageService来做到准实时转发CommitLog文件更新的事件请求，而监听事件变化的相应的任务处理器根据转发的消息及时更新ConsumeQueue与IndexFile。更新流程如下： 由org.apache.rocketmq.store.DefaultMessageStore.start()得知，每个Broker在启动时，会启动ReputMessageService线程，并初始化一个非常关键的参数reputFromOffset。该参数的含义是ReputMessageService从哪个物理偏移量开始转发消息给ConsumeQueue和IndexFile。如果允许重复转发，reputFromOffset设置为CommitLog的提交指针；如果不允许重复转发，reputFromOffset设置为CommitLog在内存中的最大偏移量。 org.apache.rocketmq.store.DefaultMessageStore.start()12this.reputMessageService.setReputFromOffset(maxPhysicalPosInLogicQueue);this.reputMessageService.start(); 如上图所示，ReputMessageService线程每执行1次任务推送后，休眠1ms即继续尝试推送更新任务到ConsumeQueue及IndexFile，任务转发的核心实现在doReput(…)方法中实现，其主要逻辑主要如下： 返回reputFromOffset偏移量开始的全部有效数据（commitlog 文件），然后循环读取每一条消息。 从SelectMappedBufferResult返回的ByteBuffer中循环读取消息，一次读取一条，创建DispatchRequest对象，如果消息长度大于0，则调用doDispatch方法，最终分别调用CommitLogDispatcherBuildConsumeQueue（构建消息消费队列）、CommitLogDispatcherBuildlndex（构建索引文件）。接下来看下这两者的具体实现。 根据消息更新ConsumeQueueConsumeQueue转发任务实现类为 CommitLogDispatcherBuildConsumeQueue，内部终将调用 putMessagePositionInfo(DispatchRequest) 方法 org.apache.rocketmq.store.DefaultMessageStore.putMessagePositionInfo(DispatchRequest)1234public void putMessagePositionInfo(DispatchRequest dispatchRequest) &#123; ConsumeQueue cq = this.findConsumeQueue(dispatchRequest.getTopic(), dispatchRequest.getQueueId()); cq.putMessagePositionInfoWrapper(dispatchRequest);&#125; 根据Topic及队列ID，先获取对应的ConsumeQueue文件，其逻辑比较简单，因为每个Topic对应一个ConsumeQueue目录，然后Topic下每个消息队列对应一个文件夹，然后Topic下每个消息队列对应一个文件夹，然后取出该文件夹最后的ConsumeQueue文件即可。 12345678910111213this.byteBufferIndex.flip();this.byteBufferIndex.limit(CQ_STORE_UNIT_SIZE);this.byteBufferIndex.putLong(offset);this.byteBufferIndex.putInt(size);this.byteBufferIndex.putLong(tagsCode);final long expectLogicOffset = cqOffset * CQ_STORE_UNIT_SIZE;MappedFile mappedFile = this.mappedFileQueue.getLastMappedFile(expectLogicOffset);if (mappedFile != null) &#123; ... return mappedFile.appendMessage(this.byteBufferIndex.array());&#125; 依次将消息偏移量、消息长度、 tag_hashcode 写入到 ByteBuffer 中，并根据consumeQueueOffset计算ConsumeQueue中的物理地址，将内容追加到ConsumeQueue的内存映射文件（MappedFile，但本操作只追加并不刷盘），ConsumeQueue的刷盘方式固定为异步刷盘模式。 根据消息更新IndexFileHash 索引文件转发任务实现类为 CommitLogDispatcherBuildIndex org.apache.rocketmq.store.DefaultMessageStore.CommitLogDispatcherBuildIndex.dispatch(DispatchRequest)12345public void dispatch(DispatchRequest request) &#123; if (DefaultMessageStore.this.messageStoreConfig.isMessageIndexEnable()) &#123; DefaultMessageStore.this.indexService.buildIndex(request); &#125;&#125; 由以上代码可知，支持由配置方式，透过设置messageIndexEnable为true，来决定是否调用重建索引服务，也就是IndexService.buildIndex方法。 org.apache.rocketmq.store.index.IndexService.buildIndex(DispatchRequest)12345678910IndexFile indexFile = retryGetAndCreateIndexFile(); if (indexFile != null) &#123; long endPhyOffset = indexFile.getEndPhyOffset(); DispatchRequest msg = req; String topic = msg.getTopic(); String keys = msg.getKeys(); if (msg.getCommitLogOffset() &lt; endPhyOffset) &#123; return; &#125;... 获取或创建 IndexFile 文件并获取所有文件最大的物理偏移量 如果该消息的物理偏移量小于索引文件中的物理偏移，则说明是重复数据，忽略本次索引构建。 org.apache.rocketmq.store.index.IndexService.buildIndex(DispatchRequest)1234567if (req.getUniqKey() != null) &#123; indexFile = putKey(indexFile, msg, buildKey(topic, req.getUniqKey())); if (indexFile == null) &#123; log.error("putKey error commitlog &#123;&#125; uniqkey &#123;&#125;", req.getCommitLogOffset(), req.getUniqKey()); return; &#125;&#125; 如果消息的唯一键不为空，则添加到 Hash 索引中，以便加速根据唯一键检索消息。 org.apache.rocketmq.store.index.IndexService.buildIndex(DispatchRequest)12345678910111213if (keys != null &amp;&amp; keys.length() &gt; 0) &#123; String[] keyset = keys.split(MessageConst.KEY_SEPARATOR); for (int i = 0; i &lt; keyset.length; i++) &#123; String key = keyset[i]; if (key.length() &gt; 0) &#123; indexFile = putKey(indexFile, msg, buildKey(topic, key)); if (indexFile == null) &#123; log.error("putKey error commitlog &#123;&#125; uniqkey &#123;&#125;", req.getCommitLogOffset(), req.getUniqKey()); return; &#125; &#125; &#125; &#125; 构建索引键， RocketMQ 支持为同一个消息建立多个索引，多个索引键空格分开。 小结RocketMQ 主要存储文件包含消息文件（Commitlog）、消息消费队列文件（ConsumeQueue）、 Hash 索引文件 （IndexFile ）等。单个消息存储文件、消息消费队列文件、Hash索引文件长度固定以便使用内存映射机制进行文件的读写操作。RocketMQ 组织文件以文件的起始偏移量来命名文件，这样根据偏移量能快速定位到真实的物理文件。RocketMQ基于内存映射文件机制提供了同步刷盘与异步刷盘两种机制，异步刷盘是指在消息存储时先追加到内存映射文件，然后启动专门的刷盘线程定时将内存中的数据刷写到磁盘。当消息到达 Commitlog 文件后，会通过 ReputMessageService线程接近实时地将消息转发给消息消费队列文件与索引文件。大神总结的RocketMQ消息存储的总体架构图如下： RocketMQ采用的这种存储结构，我们可以理解成混合型存储结构，即为Broker单个实例下所有的队列共用一个日志数据文件（即为CommitLog）来存储。而Kafka采用的是独立型的存储结构，每个队列一个文件。这种混合型结构的缺点在于，随机读的操作较多，降低读消息的效率，于是RocketMQ使用ConsumeQueue文件及IndexFile文件辅助消息读取、查找，而这样也要付出一定的开销及维护代价。为此，RocketMQ为提高读写性能，把系统中所有I/O请求，都通过Page Cache机制实现，Page Cache本身可以对数据文件进行预读取。另外，RocketMQ主要通过MappedByteBuffer对文件进行读写操作。其中，利用了NIO中的FileChannel模型直接将磁盘上的物理文件直接映射到用户态的内存地址中（这种Mmap的方式减少了传统IO将磁盘文件数据在操作系统内核地址空间的缓冲区和用户应用程序地址空间的缓冲区之间来回进行拷贝的性能开销），将对文件的操作转化为直接对内存地址进行操作，从而极大地提高了文件的读写效率（这里需要注意的是，采用MappedByteBuffer这种内存映射的方式有几个限制，其中之一是一次只能映射1.5~2G 的文件至用户态的虚拟内存，这也是为何RocketMQ默认设置单个CommitLog日志数据文件为1G的原因了）。 最后，附上大神总结的RocketMQ文件存储模型结构图： Reference《RocketMQ技术内幕》丁威RocketMQ源码分析之ConsumeQueue消息中间件—RocketMQ消息存储（一）]]></content>
  </entry>
  <entry>
    <title><![CDATA[RocketMQ-Producer消息发送]]></title>
    <url>%2F2019%2F05%2F30%2FRocketMQ-Producer-Send-Message%2F</url>
    <content type="text"><![CDATA[RocketMQ发送消息主流程RocketMQ发送消息主要流程：Producer在发送消息的时候，会根据消息的Topic，选出对应的路由信息，再挑选出具体某个队列，将消息发送至队列对应的Broker。 RocketMQ消息发送方式RocketMQ支持3种消息发送方式：同步（Sync）、异步（Async）、单向（Oneway）。 同步：发送者向MQ 执行发送消息API 时，同步等待， 直到消息服务器返回发送结果。 异步：发送者向MQ 执行发送消息API 时，指定消息发送成功后的回调函数，然后调用消息发送API 后，立即返回，消息发送者线程不阻塞，直到运行结束。待消息发送成功或失败的时候，回调任务在一个新的线程中执行。*单向：消息发送者向MQ执行发送消息API时，直接返回，不等待消息服务器的结果，也不注册回调函数，简单地说，就是只管发，不在乎消息是否成功存储在消息服务器上。 RocketMQ 消息发送考虑的问题 消息队列如何负载？ 消息发送如何实现高可用？ 批量消息发送如何实现一致性？ RocketMQ消息结构RocketMQ 消息封装类是 org.apache.rocketmq.common.message.Message，其类设计如下 Message类的全属性构造函数org.apache.rocketmq.common.message.Message.Message(String, String, String, int, byte[], boolean)12345678910111213public Message(String topic, String tags, String keys, int flag, byte[] body, boolean waitStoreMsgOK) &#123; this.topic = topic; // topic：消息所在的topic通道，主要属性 this.flag = flag; this.body = body; // body：消息的真实内容，主要属性 if (tags != null &amp;&amp; tags.length() &gt; 0) this.setTags(tags); // tags：消息标签，用于消息过滤 if (keys != null &amp;&amp; keys.length() &gt; 0) this.setKeys(keys); // keys：Message索引键，多个则用空格隔开，RocketMQ可以根据这些Key快速检索到消息 this.setWaitStoreMsgOK(waitStoreMsgOK); // waitStoreMsgOK：消息发送时是否等消息存储完成后再返回 &#125; Message 的基础属性主要包括消息所属主题topic ， 消息Flag(RocketMQ 不做处理）、扩展属性（properties）、消息体（body）、事务ID（transactionId，用于分布式事务）。 其中，RocketMQ Message的一些扩展属性properties还包含： delayTimeLevel：消息延迟级别，用于定时消息或消息重试 buyerId： 买家ID（这个字段一看就带有很浓重的电商气息） 透过这些属性的set方法可以知道，这些扩展属性存储在Message的Map类型的properties变量中。 生产者的启动流程消息生产者的代码都在client 模块中，相对于RocketMQ 来说，它就是客户端，也是消息的提供者，我们在应用系统中初始化生产者的一个实例即可使用它来发消息。 DefaultMQProducer（默认的消息发送者）消息生产者的启动流程，我们可以从org.apache.rocketmq.client.producer.DefaultMQProducer.start()入口开始看进去，默认实现是org.apache.rocketmq.client.impl.producer.DefaultMQProducerImpl.start(boolean)12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849//启动的简易入口 public void start() throws MQClientException &#123; this.start(true); &#125; //程序的真实启动入口 public void start(final boolean startFactory) throws MQClientException &#123; switch (this.serviceState) &#123; case CREATE_JUST: this.serviceState = ServiceState.START_FAILED; // 设置默认的状态是失败 // 验证配置，主要是验证group配置，不能为默认group this.checkConfig(); // 将group的的名称设置为当前线程的后缀id if (!this.defaultMQProducer.getProducerGroup().equals(MixAll.CLIENT_INNER_PRODUCER_GROUP)) &#123; this.defaultMQProducer.changeInstanceNameToPID(); &#125; // 获得发送客户端工厂，该工程是复用设计，内部是client的配置 this.mQClientFactory = MQClientManager.getInstance().getAndCreateMQClientInstance(this.defaultMQProducer, rpcHook); // 注册当前的消息发送者，确保每个group都是唯一的，否则报错 boolean registerOK = mQClientFactory.registerProducer(this.defaultMQProducer.getProducerGroup(), this); if (!registerOK) &#123; this.serviceState = ServiceState.CREATE_JUST; throw new MQClientException("The producer group[" + this.defaultMQProducer.getProducerGroup() + "] has been created before, specify another name please." + FAQUrl.suggestTodo(FAQUrl.GROUP_NAME_DUPLICATE_URL), null); &#125; // topic的发送消息管理，发生自动创建topic的请求 this.topicPublishInfoTable.put(this.defaultMQProducer.getCreateTopicKey(), new TopicPublishInfo()); //如果是静态工程启动，需要手动的启动 if (startFactory) &#123; mQClientFactory.start(); &#125; log.info("the producer [&#123;&#125;] start OK. sendMessageWithVIPChannel=&#123;&#125;", this.defaultMQProducer.getProducerGroup(), this.defaultMQProducer.isSendMessageWithVIPChannel()); this.serviceState = ServiceState.RUNNING; break; case RUNNING: case START_FAILED: case SHUTDOWN_ALREADY: throw new MQClientException("The producer service state not OK, maybe started once, " + this.serviceState + FAQUrl.suggestTodo(FAQUrl.CLIENT_SERVICE_NOT_OK), null); default: break; &#125; // 启动成功后，发送心跳 this.mQClientFactory.sendHeartbeatToAllBrokerWithLock(); &#125; 接下来，重点看一下MQClientManager.getInstance().getAndCreateMQClientInstance(this.defaultMQProducer, rpcHook)的操作12345678910111213141516171819// 基于本地缓存的客户端管理 public MQClientInstance getAndCreateMQClientInstance(final ClientConfig clientConfig, RPCHook rpcHook) &#123; String clientId = clientConfig.buildMQClientId(); //id为当前服务器的id MQClientInstance instance = this.factoryTable.get(clientId); //是否有可复用的通信客户端，资源占用比较大，可以复用 if (null == instance) &#123; instance = new MQClientInstance(clientConfig.cloneClientConfig(), this.factoryIndexGenerator.getAndIncrement(), clientId, rpcHook); //初始化客户端请求实例 MQClientInstance prev = this.factoryTable.putIfAbsent(clientId, instance); //设置请求对象，如果存在则返回原先的值，同时不覆盖，用原来的请求 if (prev != null) &#123; instance = prev; log.warn("Returned Previous MQClientInstance for clientId:[&#123;&#125;]", clientId); &#125; else &#123; log.info("Created new MQClientInstance for clientId:[&#123;&#125;]", clientId); &#125; &#125; return instance; &#125; 继续跟进去看初始化MQClientInstance的构造，最终的操作都会围绕该类进行操作和整合 12345678910111213141516171819202122232425262728293031323334//初始化客户端请求实例 public MQClientInstance(ClientConfig clientConfig, int instanceIndex, String clientId, RPCHook rpcHook) &#123; this.clientConfig = clientConfig; //mq的核心配置信息 this.instanceIndex = instanceIndex; //当前进程内的唯一标识，升序数值 this.nettyClientConfig = new NettyClientConfig(); //netty通信的客户端配置 this.nettyClientConfig.setClientCallbackExecutorThreads(clientConfig.getClientCallbackExecutorThreads()); this.nettyClientConfig.setUseTLS(clientConfig.isUseTLS()); this.clientRemotingProcessor = new ClientRemotingProcessor(this); //解析客户端请求，封装的事件处理 this.mQClientAPIImpl = new MQClientAPIImpl(this.nettyClientConfig, this.clientRemotingProcessor, rpcHook, clientConfig); //客户端实例的实际实现，网络通信的核心，只是初始化了通信框架，具体的链接后面根据不同的地址再进行链接操作 //设置核心的nameserv地址 if (this.clientConfig.getNamesrvAddr() != null) &#123; this.mQClientAPIImpl.updateNameServerAddressList(this.clientConfig.getNamesrvAddr()); log.info("user specified name server address: &#123;&#125;", this.clientConfig.getNamesrvAddr()); &#125; this.clientId = clientId; this.mQAdminImpl = new MQAdminImpl(this); //mq管理 this.pullMessageService = new PullMessageService(this); //拉取消息的实现 this.rebalanceService = new RebalanceService(this); //负载均衡的实现，可能有相关的机器增加删除，需要定期的进行重负载操作 this.defaultMQProducer = new DefaultMQProducer(MixAll.CLIENT_INNER_PRODUCER_GROUP); this.defaultMQProducer.resetClientConfig(clientConfig); this.consumerStatsManager = new ConsumerStatsManager(this.scheduledExecutorService); //消费端的状态管理 log.info("Created a new client Instance, InstanceIndex:&#123;&#125;, ClientID:&#123;&#125;, ClientConfig:&#123;&#125;, ClientVersion:&#123;&#125;, SerializerType:&#123;&#125;", this.instanceIndex, this.clientId, this.clientConfig, MQVersion.getVersionDesc(MQVersion.CURRENT_VERSION), RemotingCommand.getSerializeTypeConfigInThisServer()); &#125; 继续跟进去网络通信的构造方法 org.apache.rocketmq.client.impl.MQClientAPIImpl.MQClientAPIImpl(NettyClientConfig, ClientRemotingProcessor, RPCHook, ClientConfig) 12345678910111213141516171819202122// 客户端网络通信 public MQClientAPIImpl(final NettyClientConfig nettyClientConfig, final ClientRemotingProcessor clientRemotingProcessor, RPCHook rpcHook, final ClientConfig clientConfig) &#123; this.clientConfig = clientConfig; //核心的配置 topAddressing = new TopAddressing(MixAll.getWSAddr(), clientConfig.getUnitName()); //该功能主要是判断如果namesrv为空，则从约定的服务上去拉取 this.remotingClient = new NettyRemotingClient(nettyClientConfig, null); //通信客户端的核心实现，底层基于netty的链接 this.clientRemotingProcessor = clientRemotingProcessor; //请求事件封装处理 //注册rpc调用的钩子方法，并将事件处理绑定到上层传递过来的事件处理封装类上 this.remotingClient.registerRPCHook(rpcHook); this.remotingClient.registerProcessor(RequestCode.CHECK_TRANSACTION_STATE, this.clientRemotingProcessor, null); this.remotingClient.registerProcessor(RequestCode.NOTIFY_CONSUMER_IDS_CHANGED, this.clientRemotingProcessor, null); this.remotingClient.registerProcessor(RequestCode.RESET_CONSUMER_CLIENT_OFFSET, this.clientRemotingProcessor, null); this.remotingClient.registerProcessor(RequestCode.GET_CONSUMER_STATUS_FROM_CLIENT, this.clientRemotingProcessor, null); this.remotingClient.registerProcessor(RequestCode.GET_CONSUMER_RUNNING_INFO, this.clientRemotingProcessor, null); this.remotingClient.registerProcessor(RequestCode.CONSUME_MESSAGE_DIRECTLY, this.clientRemotingProcessor, null); &#125; 至此，初始化配置的操作已经完成。接下来，就是继续调用mqClientFactory.start()方法，org.apache.rocketmq.client.impl.factory.MQClientInstance.start() 12345678910111213141516171819202122232425262728293031323334public void start() throws MQClientException &#123; synchronized (this) &#123; switch (this.serviceState) &#123; case CREATE_JUST: this.serviceState = ServiceState.START_FAILED; // If not specified,looking address from name server if (null == this.clientConfig.getNamesrvAddr()) &#123; this.mQClientAPIImpl.fetchNameServerAddr(); &#125; // Start request-response channel this.mQClientAPIImpl.start(); //启动netty的客户端配置 // Start various schedule tasks this.startScheduledTask(); //启动定时任务，定时进行更新、验证、发送心跳等操作 // Start pull service this.pullMessageService.start(); //拉取消息消费 // Start rebalance service this.rebalanceService.start(); // 设置消费端重新负载，请求的初始化操作也在此方法内执行 // Start push service this.defaultMQProducer.getDefaultMQProducerImpl().start(false); // 推送消息消费，这里入参为false，是因为前面已经初始化过了，只需要初始化其它操作 log.info("the client factory [&#123;&#125;] start OK", this.clientId); this.serviceState = ServiceState.RUNNING; break; case RUNNING: break; case SHUTDOWN_ALREADY: break; case START_FAILED: throw new MQClientException("The Factory object[" + this.getClientId() + "] has been created before, and failed.", null); default: break; &#125; &#125; &#125; 以上，就是producer（消息发送端）的启动操作过程。接下来，就是重头——消息发送过程。 RocketMQ消息发送流程剖析Broker在启动后会周期性地向NameSrv注册自身及Topic路由信息，而生产者Producer同样会周期性地从NameSrv上拉取最新更新至本地的Topic路由信息。当Producer要开始发送某一Topic的消息时，便会从本地的路由表中找到Topic对应的路由，选择Topic下合适的Broker来发送消息。RocketMQ中，Topic底下包含若干个队列（Queue），也就是说，Topic对Queue是一对多的关系。每个Queue都记录了自己所属的Broker，对于同一个Topic而言，它的多个Queue可能指向同一个Broker。 如上文所述，Producer根据消息的Topic，选出对应的路由信息（TopicRouteData），再挑选出具体某个MessageQueue，将消息发送至MessageQueue对应的Broker。 假设TopicX上有4个Queue（queue1，queue2，queue3，queue4），那么Producer发送TopicX的消息时，会将消息平均发送到每个Queue，从而发送到每个Queue对应的Broker，至于Broker这边，仅Master节点才能接收Producer发来的消息并写入到本地存储，如果有Slave，则会再从Master同步至Slave。 接下来是发送消息的源码分析环节。 消息发送的流程解析消息发送的主要步骤包括：验证消息、查找路由、消息发送（包含异常处理机制）。 直接来看发送消息的默认实现，org.apache.rocketmq.client.impl.producer.DefaultMQProducerImpl.sendDefaultImpl(Message, CommunicationMode, SendCallback, long) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148private SendResult sendDefaultImpl( Message msg, final CommunicationMode communicationMode, final SendCallback sendCallback, final long timeout ) throws MQClientException, RemotingException, MQBrokerException, InterruptedException &#123; this.makeSureStateOK(); Validators.checkMessage(msg, this.defaultMQProducer); final long invokeID = random.nextLong(); long beginTimestampFirst = System.currentTimeMillis(); long beginTimestampPrev = beginTimestampFirst; long endTimestamp = beginTimestampFirst; TopicPublishInfo topicPublishInfo = this.tryToFindTopicPublishInfo(msg.getTopic()); if (topicPublishInfo != null &amp;&amp; topicPublishInfo.ok()) &#123; boolean callTimeout = false; MessageQueue mq = null; Exception exception = null; SendResult sendResult = null; int timesTotal = communicationMode == CommunicationMode.SYNC ? 1 + this.defaultMQProducer.getRetryTimesWhenSendFailed() : 1; int times = 0; String[] brokersSent = new String[timesTotal]; for (; times &lt; timesTotal; times++) &#123; String lastBrokerName = null == mq ? null : mq.getBrokerName(); MessageQueue mqSelected = this.selectOneMessageQueue(topicPublishInfo, lastBrokerName); if (mqSelected != null) &#123; mq = mqSelected; brokersSent[times] = mq.getBrokerName(); try &#123; beginTimestampPrev = System.currentTimeMillis(); if (times &gt; 0) &#123; //Reset topic with namespace during resend. msg.setTopic(this.defaultMQProducer.withNamespace(msg.getTopic())); &#125; long costTime = beginTimestampPrev - beginTimestampFirst; if (timeout &lt; costTime) &#123; callTimeout = true; break; &#125; sendResult = this.sendKernelImpl(msg, mq, communicationMode, sendCallback, topicPublishInfo, timeout - costTime); endTimestamp = System.currentTimeMillis(); this.updateFaultItem(mq.getBrokerName(), endTimestamp - beginTimestampPrev, false); switch (communicationMode) &#123; case ASYNC: return null; case ONEWAY: return null; case SYNC: if (sendResult.getSendStatus() != SendStatus.SEND_OK) &#123; if (this.defaultMQProducer.isRetryAnotherBrokerWhenNotStoreOK()) &#123; continue; &#125; &#125; return sendResult; default: break; &#125; &#125; catch (RemotingException e) &#123; endTimestamp = System.currentTimeMillis(); this.updateFaultItem(mq.getBrokerName(), endTimestamp - beginTimestampPrev, true); log.warn(String.format("sendKernelImpl exception, resend at once, InvokeID: %s, RT: %sms, Broker: %s", invokeID, endTimestamp - beginTimestampPrev, mq), e); log.warn(msg.toString()); exception = e; continue; &#125; catch (MQClientException e) &#123; endTimestamp = System.currentTimeMillis(); this.updateFaultItem(mq.getBrokerName(), endTimestamp - beginTimestampPrev, true); log.warn(String.format("sendKernelImpl exception, resend at once, InvokeID: %s, RT: %sms, Broker: %s", invokeID, endTimestamp - beginTimestampPrev, mq), e); log.warn(msg.toString()); exception = e; continue; &#125; catch (MQBrokerException e) &#123; endTimestamp = System.currentTimeMillis(); this.updateFaultItem(mq.getBrokerName(), endTimestamp - beginTimestampPrev, true); log.warn(String.format("sendKernelImpl exception, resend at once, InvokeID: %s, RT: %sms, Broker: %s", invokeID, endTimestamp - beginTimestampPrev, mq), e); log.warn(msg.toString()); exception = e; switch (e.getResponseCode()) &#123; case ResponseCode.TOPIC_NOT_EXIST: case ResponseCode.SERVICE_NOT_AVAILABLE: case ResponseCode.SYSTEM_ERROR: case ResponseCode.NO_PERMISSION: case ResponseCode.NO_BUYER_ID: case ResponseCode.NOT_IN_CURRENT_UNIT: continue; default: if (sendResult != null) &#123; return sendResult; &#125; throw e; &#125; &#125; catch (InterruptedException e) &#123; endTimestamp = System.currentTimeMillis(); this.updateFaultItem(mq.getBrokerName(), endTimestamp - beginTimestampPrev, false); log.warn(String.format("sendKernelImpl exception, throw exception, InvokeID: %s, RT: %sms, Broker: %s", invokeID, endTimestamp - beginTimestampPrev, mq), e); log.warn(msg.toString()); log.warn("sendKernelImpl exception", e); log.warn(msg.toString()); throw e; &#125; &#125; else &#123; break; &#125; &#125; if (sendResult != null) &#123; return sendResult; &#125; String info = String.format("Send [%d] times, still failed, cost [%d]ms, Topic: %s, BrokersSent: %s", times, System.currentTimeMillis() - beginTimestampFirst, msg.getTopic(), Arrays.toString(brokersSent)); info += FAQUrl.suggestTodo(FAQUrl.SEND_MSG_FAILED); MQClientException mqClientException = new MQClientException(info, exception); if (callTimeout) &#123; throw new RemotingTooMuchRequestException("sendDefaultImpl call timeout"); &#125; if (exception instanceof MQBrokerException) &#123; mqClientException.setResponseCode(((MQBrokerException) exception).getResponseCode()); &#125; else if (exception instanceof RemotingConnectException) &#123; mqClientException.setResponseCode(ClientErrorCode.CONNECT_BROKER_EXCEPTION); &#125; else if (exception instanceof RemotingTimeoutException) &#123; mqClientException.setResponseCode(ClientErrorCode.ACCESS_BROKER_TIMEOUT); &#125; else if (exception instanceof MQClientException) &#123; mqClientException.setResponseCode(ClientErrorCode.BROKER_NOT_EXIST_EXCEPTION); &#125; throw mqClientException; &#125; List&lt;String&gt; nsList = this.getmQClientFactory().getMQClientAPIImpl().getNameServerAddressList(); if (null == nsList || nsList.isEmpty()) &#123; throw new MQClientException( "No name server address, please set it." + FAQUrl.suggestTodo(FAQUrl.NAME_SERVER_ADDR_NOT_EXIST_URL), null).setResponseCode(ClientErrorCode.NO_NAME_SERVER_EXCEPTION); &#125; throw new MQClientException("No route info of this topic, " + msg.getTopic() + FAQUrl.suggestTodo(FAQUrl.NO_TOPIC_ROUTE_INFO), null).setResponseCode(ClientErrorCode.NOT_FOUND_TOPIC_EXCEPTION); &#125; 由于这个方法逻辑比较多，接下来我们分拆成几个部分来分析。 验证消息第一步，消息发送之前，首先确保生产者处于运行状态，这里调了 this.makeSureStateOK()，然后便是验证消息 Validators.checkMessage(msg, this.defaultMQProducer)，点进去org.apache.rocketmq.client.Validators.checkMessage(Message, DefaultMQProducer)会看到是验证消息是否符合相应的规范，包括具体的规范要求包括：Topic名称，消息体不能为空，消息长度不能等于0且默认不能超过允许发送消息的最大长度4M（maxMessageSize = 1024 * 1024 * 4）。 12345678910111213141516171819202122public static void checkMessage(Message msg, DefaultMQProducer defaultMQProducer) throws MQClientException &#123; if (null == msg) &#123; throw new MQClientException(ResponseCode.MESSAGE_ILLEGAL, "the message is null"); &#125; // topic Validators.checkTopic(msg.getTopic()); // body if (null == msg.getBody()) &#123; throw new MQClientException(ResponseCode.MESSAGE_ILLEGAL, "the message body is null"); &#125; if (0 == msg.getBody().length) &#123; throw new MQClientException(ResponseCode.MESSAGE_ILLEGAL, "the message body length is zero"); &#125; if (msg.getBody().length &gt; defaultMQProducer.getMaxMessageSize()) &#123; throw new MQClientException(ResponseCode.MESSAGE_ILLEGAL, "the message body size over max value, MAX: " + defaultMQProducer.getMaxMessageSize()); &#125; &#125; 查找路由第二步，查找Topic对应的路由信息（留意方法体的代码注释）。 org.apache.rocketmq.client.impl.producer.DefaultMQProducerImpl.tryToFindTopicPublishInfo(String) 1234567891011121314151617private TopicPublishInfo tryToFindTopicPublishInfo(final String topic) &#123; // 查本地缓存的表 TopicPublishInfo topicPublishInfo = this.topicPublishInfoTable.get(topic); if (null == topicPublishInfo || !topicPublishInfo.ok()) &#123; // 本地缓存中没有，则向NameSrv发起请求，并更新本地路由缓存 this.topicPublishInfoTable.putIfAbsent(topic, new TopicPublishInfo()); this.mQClientFactory.updateTopicRouteInfoFromNameServer(topic); topicPublishInfo = this.topicPublishInfoTable.get(topic); &#125; // 如果从NameSrv上查找到了，此处便直接返回找到的路由信息topicPublishInfo if (topicPublishInfo.isHaveTopicRouterInfo() || topicPublishInfo.ok()) &#123; return topicPublishInfo; &#125; else &#123; // 没有查找到，再次查询topic路由 this.mQClientFactory.updateTopicRouteInfoFromNameServer(topic, true, this.defaultMQProducer); topicPublishInfo = this.topicPublishInfoTable.get(topic); return topicPublishInfo; &#125; &#125; 这里有点吊诡的是，为什么在前面从NameSrv查不到路由信息，第二次就再来查一次，难道再试一次就能查到吗？带着疑问，跟进去org.apache.rocketmq.client.impl.factory.MQClientInstance.updateTopicRouteInfoFromNameServer(String, boolean, DefaultMQProducer)方法体里边一探究竟 123456789101112131415161718192021public boolean updateTopicRouteInfoFromNameServer(final String topic, boolean isDefault, DefaultMQProducer defaultMQProducer) &#123; try &#123; if (this.lockNamesrv.tryLock(LOCK_TIMEOUT_MILLIS, TimeUnit.MILLISECONDS)) &#123; try &#123; TopicRouteData topicRouteData; if (isDefault &amp;&amp; defaultMQProducer != null) &#123; topicRouteData = this.mQClientAPIImpl.getDefaultTopicRouteInfoFromNameServer(defaultMQProducer.getCreateTopicKey(), 1000 * 3); if (topicRouteData != null) &#123; for (QueueData data : topicRouteData.getQueueDatas()) &#123; int queueNums = Math.min(defaultMQProducer.getDefaultTopicQueueNums(), data.getReadQueueNums()); data.setReadQueueNums(queueNums); data.setWriteQueueNums(queueNums); &#125; &#125; &#125; else &#123; topicRouteData = this.mQClientAPIImpl.getTopicRouteInfoFromNameServer(topic, 1000 * 3); &#125; ... &#125; 这次调用 updateTopicRouteInfoFromNameServer(…)，传入的 isDefault 参数为 true，也就是说，会走 if 分支，这里是调 this.mQClientAPIImpl.getDefaultTopicRouteInfoFromNameServer(defaultMQProducer.getCreateTopicKey(), 1000 * 3) 从NameSrv查询Topic路由，不过这回不是查询消息所属的Topic路由信息，而是查询RocketMQ设置的一个默认Topic的路由，进去 defaultMQProducer.getCreateTopicKey() 看到 这个默认的 Topic 是 TBW102 （AUTO_CREATE_TOPIC_KEY_TOPIC = “TBW102”），这个Topic就是用来创建其他Topic所用的。如果某Broker配置了 autoCreateTopicEnable，允许自动创建Topic，那么在该Broker启动后，便会向自己的路由表中插入 TBW102 这个Topic，并注册到NameSrv，表明处理该Topic类型的消息。 如果默认Topic下查询到路由信息，则替换路由信息中读写队列个数为消息生产者默认的队列个数（defaultTopicQueueNums ）。如果isDefault 为false ，则使用参数topic 去查询；如果未查询到路由信息，则返回false ，表示路由信息未变化。 12345678910111213... // 如果路由信息找到，与本地缓存中的路由信息进行对比，判断路由信息是否发生了改变， 如果未发生变化，则直接返回chaged=falseif (topicRouteData != null) &#123; TopicRouteData old = this.topicRouteTable.get(topic); boolean changed = topicRouteDataIsChange(old, topicRouteData); if (!changed) &#123; changed = this.isNeedUpdateTopicRouteInfo(topic); &#125; else &#123; log.info("the topic[&#123;&#125;] route info changed, old[&#123;&#125;] ,new[&#123;&#125;]", topic, old, topicRouteData); &#125; ...&#125;... 然后，更新MQClientInstance Broker地址缓存（路由信息转化为PublishInfo）以及更新该MQClientInstance所管辖的所有消息发送关于该topic的路由信息（路由信息转化为MessageQueue列表，此具体实现在 topicRouteData2TopicSubscribeInfo(…) 方法，再根据MessageQueue列表进行更新）。 1234567891011121314151617181920212223242526// Update Pub info&#123; TopicPublishInfo publishInfo = topicRouteData2TopicPublishInfo(topic, topicRouteData); publishInfo.setHaveTopicRouterInfo(true); Iterator&lt;Entry&lt;String, MQProducerInner&gt;&gt; it = this.producerTable.entrySet().iterator(); while (it.hasNext()) &#123; Entry&lt;String, MQProducerInner&gt; entry = it.next(); MQProducerInner impl = entry.getValue(); if (impl != null) &#123; impl.updateTopicPublishInfo(topic, publishInfo); &#125; &#125;&#125;// Update sub info&#123; Set&lt;MessageQueue&gt; subscribeInfo = topicRouteData2TopicSubscribeInfo(topic, topicRouteData); Iterator&lt;Entry&lt;String, MQConsumerInner&gt;&gt; it = this.consumerTable.entrySet().iterator(); while (it.hasNext()) &#123; Entry&lt;String, MQConsumerInner&gt; entry = it.next(); MQConsumerInner impl = entry.getValue(); if (impl != null) &#123; impl.updateTopicSubscribeInfo(topic, subscribeInfo); &#125; &#125;&#125; org.apache.rocketmq.client.impl.factory.MQClientInstance.topicRouteData2TopicSubscribeInfo(String, TopicRouteData) 12345678910111213141516171819/** 循环遍历路由信息的QueueData 信息，如果队列没有写权限，则继续遍历下一个QueueData ；根据brokerName 找到brokerData 信息，* 找不到或没有找到Master节点（仅Master节点Broker才提供写消息服务），则遍历下一个QueueData ；* 根据写队列个数，根据topic＋序号创建MessageQueue ，填充topicPublishlnfo 的List&lt;QuueMessage＞ 。完成消息发送的路由查找。*/ public static Set&lt;MessageQueue&gt; topicRouteData2TopicSubscribeInfo(final String topic, final TopicRouteData route) &#123; Set&lt;MessageQueue&gt; mqList = new HashSet&lt;MessageQueue&gt;(); List&lt;QueueData&gt; qds = route.getQueueDatas(); for (QueueData qd : qds) &#123; if (PermName.isReadable(qd.getPerm())) &#123; for (int i = 0; i &lt; qd.getReadQueueNums(); i++) &#123; MessageQueue mq = new MessageQueue(topic, qd.getBrokerName(), i); mqList.add(mq); &#125; &#125; &#125; return mqList; &#125; 所以，当消息所属的Topic，假设叫Topic X吧，它本身没有在任何Broker上配置的时候，生产者就会查询默认Topic TBW102的路由信息，暂时作为Topic X的的路由，并插入到本地路由表中。当TopicX利用该路由发送到 Broker后，Broker发现自己并没有该Topic信息后，便会创建好该Topic，并更新到NameSrv中，表明后续接收TopicX的消息。 整理一下获取Topic路由的步骤： 先从本地缓存的路由表中查询； 没有找到的话，便向NameSrv发起请求，更新本地路由表，再次查询； 如果仍然没有查询到，表明Topic没有事先配置，则用Topic TBW102向NameSrv发起查询，返回TBW102的路由信息，暂时作为Topic的路由。 查找路由的过程解析到此，接下来是选择消息队列的过程。 发送消息队列我们此处所谓发送消息，其实是发送到Queue里的，RocketMQ里边的Queue是个抽象的概念，并不是我们所理解的数据结构里的队列Queue，上文已经提到，每个Topic的路由信息（topicRouteData）中可能包含若干Queue，而topicRouteData是由元数据管理中心NameSrv返回的。也就是说，Producer是从NameSrv拉取的路由信息为TopicRouteData，我们不妨先来看下它的属性： queueDatas 中包含了Topic对应的所有Queue信息，QueueData的结构如下： 选择队列接下来，回到 sendDefaultImpl() 方法，看一下拿到路由信息后的下一步，选择队列的过程实现，其中选择队列的逻辑加上了 超时机制 和 重试机制。当选择某个Queue发送消息失败后，只要还没超时，且没有超出最大重试次数，就是再次尝试选择某个Queue进行重试。 看回 org.apache.rocketmq.client.impl.producer.DefaultMQProducerImpl.sendDefaultImpl(Message, CommunicationMode, SendCallback, long) 方法 1234567891011121314151617181920212223242526272829303132333435363738394041// 重试次数内进行重试 for (; times &lt; timesTotal; times++) &#123; String lastBrokerName = null == mq ? null : mq.getBrokerName(); MessageQueue mqSelected = this.selectOneMessageQueue(topicPublishInfo, lastBrokerName); // 选择某个Queue 用来发送消息 if (mqSelected != null) &#123; mq = mqSelected; brokersSent[times] = mq.getBrokerName(); try &#123; beginTimestampPrev = System.currentTimeMillis(); if (times &gt; 0) &#123; //Reset topic with namespace during resend. msg.setTopic(this.defaultMQProducer.withNamespace(msg.getTopic())); &#125; long costTime = beginTimestampPrev - beginTimestampFirst; if (timeout &lt; costTime) &#123; // 在超时时间内进行重试 callTimeout = true; break; &#125; // 进行消息发送的核心实现 sendResult = this.sendKernelImpl(msg, mq, communicationMode, sendCallback, topicPublishInfo, timeout - costTime); endTimestamp = System.currentTimeMillis(); this.updateFaultItem(mq.getBrokerName(), endTimestamp - beginTimestampPrev, false); switch (communicationMode) &#123; // 三种不同的发送方式，相应的处理：除了同步需要重试另一个Broker以确保返回结果，其它直接返回null case ASYNC: return null; case ONEWAY: return null; case SYNC: if (sendResult.getSendStatus() != SendStatus.SEND_OK) &#123; if (this.defaultMQProducer.isRetryAnotherBrokerWhenNotStoreOK()) &#123; continue; &#125; &#125; return sendResult; default: break; &#125; &#125; // 以下是各种catch异常，此处省略这部分的代码 ... 由上面代码可知，选择Queue的具体逻辑在topicPublishInfo.selectOneMessageQueue（lastBrokerName）中。这里在调用时传入了lastBrokerName，目前我们还不知道是为了什么，所以带着疑惑进入方法内部看看吧。 123456789101112131415161718192021222324252627282930313233343536public MessageQueue selectOneMessageQueue(final TopicPublishInfo tpInfo, final String lastBrokerName) &#123; if (this.sendLatencyFaultEnable) &#123; try &#123; int index = tpInfo.getSendWhichQueue().getAndIncrement(); for (int i = 0; i &lt; tpInfo.getMessageQueueList().size(); i++) &#123; int pos = Math.abs(index++) % tpInfo.getMessageQueueList().size(); if (pos &lt; 0) pos = 0; MessageQueue mq = tpInfo.getMessageQueueList().get(pos); if (latencyFaultTolerance.isAvailable(mq.getBrokerName())) &#123; if (null == lastBrokerName || mq.getBrokerName().equals(lastBrokerName)) return mq; &#125; &#125; final String notBestBroker = latencyFaultTolerance.pickOneAtLeast(); int writeQueueNums = tpInfo.getQueueIdByBroker(notBestBroker); if (writeQueueNums &gt; 0) &#123; final MessageQueue mq = tpInfo.selectOneMessageQueue(); if (notBestBroker != null) &#123; mq.setBrokerName(notBestBroker); mq.setQueueId(tpInfo.getSendWhichQueue().getAndIncrement() % writeQueueNums); &#125; return mq; &#125; else &#123; latencyFaultTolerance.remove(notBestBroker); &#125; &#125; catch (Exception e) &#123; log.error("Error occurred when selecting message queue", e); &#125; return tpInfo.selectOneMessageQueue(); &#125; return tpInfo.selectOneMessageQueue(lastBrokerName); &#125; 我们来分析一下这段逻辑： 当lastBrokerName不为空时，将计数器进行自增，再遍历TopicPulishInfo中的MessageQueue列表，按照计数器数值对MessageQueue总个数进行取模，再根据取模结果，取出MessageQueue列表中的某个Queue，并判断Queue所属Broker的Name是否和lastBrokerName一致，一致则继续遍历。 当lastBrokerName为空时，同样将计数器进行自增，按照计数器数值对MessageQueue总个数进行取模，再根据取模结果，取出MessageQueue列表中的某个Queue，直接返回。概括一下，这段逻辑的主要部分就是利用计数器，来进行Queue的负载均衡。而lastBrokerName的作用，就是为了做负载均衡。 当某条消息第一次发送时，lastBrokerName 为空，此时就是直接取模进行负载均衡操作。但是如果消息发送失败，就会触发重试机制，发送失败有可能是因为Broker出现来某些故障，或者某些网络连通性问题，所以当消息第N次重试时，就要避开第N-1次时消息发往的Broker，也就是lastBrokerName。好了，我们已经了解了选择Queue 的来源及消息发送时Queue的负载均衡以及重试机制。下面让我们来看看消息的核心发送过程。 发送消息的核心实现好了，消息发送的核心，就在于最后一步，网络传输了，我们跟踪到 org.apache.rocketmq.client.impl.producer.DefaultMQProducerImpl.sendKernelImpl(Message, MessageQueue, CommunicationMode, SendCallback, TopicPublishInfo, long) 方法里边 12345String brokerAddr = this.mQClientFactory.findBrokerAddressInPublish(mq.getBrokerName());if (null == brokerAddr) &#123; tryToFindTopicPublishInfo(mq.getTopic()); brokerAddr = this.mQClientFactory.findBrokerAddressInPublish(mq.getBrokerName());&#125; 拿到Broker地址后，要将消息内容及其他信息封装进请求头：12345678910111213SendMessageRequestHeader requestHeader = new SendMessageRequestHeader();requestHeader.setProducerGroup(this.defaultMQProducer.getProducerGroup());requestHeader.setTopic(msg.getTopic());requestHeader.setDefaultTopic(this.defaultMQProducer.getCreateTopicKey());requestHeader.setDefaultTopicQueueNums(this.defaultMQProducer.getDefaultTopicQueueNums());requestHeader.setQueueId(mq.getQueueId());requestHeader.setSysFlag(sysFlag);requestHeader.setBornTimestamp(System.currentTimeMillis());requestHeader.setFlag(msg.getFlag());requestHeader.setProperties(MessageDecoder.messageProperties2String(msg.getProperties()));requestHeader.setReconsumeTimes(0);requestHeader.setUnitMode(this.isUnitMode());requestHeader.setBatch(msg instanceof MessageBatch); 请求头部封装好之后，接下来重点来看 org.apache.rocketmq.client.impl.MQClientAPIImpl.sendMessage(String, String, Message, SendMessageRequestHeader, long, CommunicationMode, SendCallback, TopicPublishInfo, MQClientInstance, int, SendMessageContext, DefaultMQProducerImpl)，这方法内部便是创建网络请求，调用封装的Netty接口进行网络传输了。 首先创建请求：123456789RemotingCommand request = null;if (sendSmartMsg || msg instanceof MessageBatch) &#123; SendMessageRequestHeaderV2 requestHeaderV2 = SendMessageRequestHeaderV2.createSendMessageRequestHeaderV2(requestHeader); request = RemotingCommand.createRequestCommand(msg instanceof MessageBatch ? RequestCode.SEND_BATCH_MESSAGE : RequestCode.SEND_MESSAGE_V2, requestHeaderV2);&#125; else &#123; request = RemotingCommand.createRequestCommand(RequestCode.SEND_MESSAGE, requestHeader);&#125;request.setBody(msg.getBody()); 这里按照是否发送 smartMsg ，创建了不同请求命令号的请求，接下来，根据消息发送方式（单向、同步、异步），调用不同的发送函数进行网络传输： 1234567891011121314151617181920212223switch (communicationMode) &#123; case ONEWAY: this.remotingClient.invokeOneway(addr, request, timeoutMillis); return null; case ASYNC: final AtomicInteger times = new AtomicInteger(); long costTimeAsync = System.currentTimeMillis() - beginStartTime; if (timeoutMillis &lt; costTimeAsync) &#123; throw new RemotingTooMuchRequestException("sendMessage call timeout"); &#125; this.sendMessageAsync(addr, brokerName, msg, timeoutMillis - costTimeAsync, request, sendCallback, topicPublishInfo, instance, retryTimesWhenSendFailed, times, context, producer); return null; case SYNC: long costTimeSync = System.currentTimeMillis() - beginStartTime; if (timeoutMillis &lt; costTimeSync) &#123; throw new RemotingTooMuchRequestException("sendMessage call timeout"); &#125; return this.sendMessageSync(addr, brokerName, msg, timeoutMillis - costTimeSync, request); default: assert false; break;&#125; 至此，消息的发送——从Producer把消息传输到Broker的过程分析就已经结束了。 批量消息发送批量消息发送是将 同一主题的多条消息一起打包发送到消息服务端，减少网络调用次数，提高网络传输效率。批量消息发送要解决的是如何将这些消息编码以便服务端能够正确解码出每条消息的消息内容 。 那 RocketMQ 如何编码多条消息呢 ？ 我们首先梳理一下 RocketMQ 网络请求命令设计。 其结构如下 单条消息发送时 ，消息体的内容将保存在 body 中 。 批量消息发送 ，需要将多条消息体的内容存储在 body 中，如何存储方便服务端正确解析出 每条消息呢？RocketMQ 采取的方式是，对单条消息 内容使用 固定格式进行存储，如下图 接下来梳理一下批量消息发送的核心流程1234public SendResult send( Collection&lt;Message&gt; msgs) throws MQClientException, RemotingException, MQBrokerException, InterruptedException &#123; return this.defaultMQProducerImpl.send(batch(msgs));&#125; 首先在消息发送端，调用 batch 方法 ，将一批消息封装成 MessageBatch 对象。 MessageBatch 继承自 Message 对象，Messag巳Batch 内部持有 List messages。这样的话，批量消息发送与单条消息发送的处理流程完全一样。MessageBatch 只需要将该集合中的每条消息的消息体 body 聚合成一个 byte[]，在消息服务端能够从该 byte[] 中正确解析出消息即可。 123public byte[] encode() &#123; return MessageDecoder.encodeMessages(messages); &#125; 在创建 RemotingCommand 对象时将调用 org.apache.rocketmq.common.message.MessageDecoder.encodeMessage(Message) 方法填充到 RemotingCommand 的 body 域中。多条消息编码方法如下 123456789101112131415161718192021222324252627282930313233343536373839public static byte[] encodeMessage(Message message) &#123; //only need flag, body, properties byte[] body = message.getBody(); int bodyLen = body.length; String properties = messageProperties2String(message.getProperties()); byte[] propertiesBytes = properties.getBytes(CHARSET_UTF8); //note properties length must not more than Short.MAX short propertiesLength = (short) propertiesBytes.length; int sysFlag = message.getFlag(); int storeSize = 4 // 1 TOTALSIZE + 4 // 2 MAGICCOD + 4 // 3 BODYCRC + 4 // 4 FLAG + 4 + bodyLen // 4 BODY + 2 + propertiesLength; ByteBuffer byteBuffer = ByteBuffer.allocate(storeSize); // 1 TOTALSIZE byteBuffer.putInt(storeSize); // 2 MAGICCODE byteBuffer.putInt(0); // 3 BODYCRC byteBuffer.putInt(0); // 4 FLAG int flag = message.getFlag(); byteBuffer.putInt(flag); // 5 BODY byteBuffer.putInt(bodyLen); byteBuffer.put(body); // 6 properties byteBuffer.putShort(propertiesLength); byteBuffer.put(propertiesBytes); return byteBuffer.array(); &#125; 在消息发送端将会按照上述结构进行解码，然后整个发送流程与单个消息发送没什么差异。 小结RocketMQ发送消息重点如下：1 ）消息生产者启动流程重点理解 MQClientlnstance、消息生产者之间的关系。2 ）消息队列负载机制消息生产者在发送消息时，如果本地路由表中未缓存 topic 的路由信息，向 NameServer 发送获取路由信息请求，更新本地路由信息表，并且消息生产者每隔 30s 从 NameServer 更新路由表。3 ）消息发送异常机制消息发送高可用主要通过两个手段：重试与 Broker 规避。Broker规避就是在一次消息发送过程中发现错误，在某一时间段内，消息生产者不会选择该 Broker（消息服务器）上的消息队列，提高发送消息的成功率。4 ）批量消息发送RocketMQ支持将同一主题下的多条消息一次性发送到消息服务端。]]></content>
  </entry>
  <entry>
    <title><![CDATA[RocketMQ NameServer]]></title>
    <url>%2F2019%2F05%2F25%2FRocketMQ-NameServer%2F</url>
    <content type="text"><![CDATA[路由中心Namesrv设计Namesrv就是RocketMQ的路由中心，也就是路由元信息的管理中心。主要作用是为消息生产者（Producer）、消费者（Consumer）提供关于主题（Topic）的路由信息，那么，Namesrv需要存储路由的基础信息，还要能管理Broker节点，包括路由注册、路由删除等功能。Namesrv被设计为RocketMQ统一的路由中心，这也就是为何，Producer与Consumer在启动时，统一需要在Namesrv上面注册，而非Broker。 路由元信息NameServer路由实现类：org.apache.rocketmq.namesrv.routeinfo.RouteInfoManager。 主要属性• topicQueueTable: Topic 下的消息队列路由信息，消息发送时根据路由表进行负载均衡。• brokerAddrTable : Broker 基础信息， 包含brokerName 、所属集群名称、主备Broker地址。• clusterAddrTable: Broker 集群信息，存储集群中所有Broker 名称。• brokerLiveTable: Broker 状态信息。NameServer 每次收到心跳包时会替换该信息，这是实现Broker服务更新的关键。• filterServerTable : Broker 上的FilterServer 列表，用于类模式消息过滤，这涉及到消息过滤的功能。 Namesrv实现类一览包结构不复杂，类数据量也不多，关键的几个类是：• NamesrvStartup：启动类• NamesrvController：核心控制器• RouteInfoManager：路由信息表 Namesrv启动从Namesrv的启动类NamesrvStartup撸起（文中提到代码均基于RocketMQ 4.5.1版本）：org.apache.rocketmq.namesrv.NamesrvStartup.main0(String[]) 1234567891011121314public static NamesrvController main0(String[] args) &#123; try &#123; NamesrvController controller = createNamesrvController(args); start(controller); String tip = "The Name Server boot success. serializeType=" + RemotingCommand.getSerializeTypeConfigInThisServer(); log.info(tip); System.out.printf("%s%n", tip); return controller; &#125; catch (Throwable e) &#123; e.printStackTrace(); System.exit(-1); &#125; return null;&#125; 整个NameServer的服务启动流程代码都在这个main0(String[] args)方法里了，咋一看，还是比较简单的，大体流程分三步： 创建NamesrvController对象， 启动NamesrvController， 记log并返回NamesrvController对象。步骤三就很简单了，一目了然，那么接下来分析步骤一及步骤二。 创建NamesrvController对象完整代码：org.apache.rocketmq.namesrv.NamesrvStartup.createNamesrvController(String[])1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162public static NamesrvController createNamesrvController(String[] args) throws IOException, JoranException &#123; System.setProperty(RemotingCommand.REMOTING_VERSION_KEY, Integer.toString(MQVersion.CURRENT_VERSION)); //PackageConflictDetect.detectFastjson(); Options options = ServerUtil.buildCommandlineOptions(new Options()); commandLine = ServerUtil.parseCmdLine("mqnamesrv", args, buildCommandlineOptions(options), new PosixParser()); if (null == commandLine) &#123; System.exit(-1); return null; &#125; final NamesrvConfig namesrvConfig = new NamesrvConfig(); final NettyServerConfig nettyServerConfig = new NettyServerConfig(); nettyServerConfig.setListenPort(9876); if (commandLine.hasOption('c')) &#123; String file = commandLine.getOptionValue('c'); if (file != null) &#123; InputStream in = new BufferedInputStream(new FileInputStream(file)); properties = new Properties(); properties.load(in); MixAll.properties2Object(properties, namesrvConfig); MixAll.properties2Object(properties, nettyServerConfig); namesrvConfig.setConfigStorePath(file); System.out.printf("load config properties file OK, %s%n", file); in.close(); &#125; &#125; if (commandLine.hasOption('p')) &#123; InternalLogger console = InternalLoggerFactory.getLogger(LoggerName.NAMESRV_CONSOLE_NAME); MixAll.printObjectProperties(console, namesrvConfig); MixAll.printObjectProperties(console, nettyServerConfig); System.exit(0); &#125; MixAll.properties2Object(ServerUtil.commandLine2Properties(commandLine), namesrvConfig); if (null == namesrvConfig.getRocketmqHome()) &#123; System.out.printf("Please set the %s variable in your environment to match the location of the RocketMQ installation%n", MixAll.ROCKETMQ_HOME_ENV); System.exit(-2); &#125; LoggerContext lc = (LoggerContext) LoggerFactory.getILoggerFactory(); JoranConfigurator configurator = new JoranConfigurator(); configurator.setContext(lc); lc.reset(); configurator.doConfigure(namesrvConfig.getRocketmqHome() + "/conf/logback_namesrv.xml"); log = InternalLoggerFactory.getLogger(LoggerName.NAMESRV_LOGGER_NAME); MixAll.printObjectProperties(log, namesrvConfig); MixAll.printObjectProperties(log, nettyServerConfig); final NamesrvController controller = new NamesrvController(namesrvConfig, nettyServerConfig); // remember all configs to prevent discard controller.getConfiguration().registerConfig(properties); return controller; &#125; 这里方法的代码就比较多了，我们拆成几段来撸就比较好分析了：org.apache.rocketmq.namesrv.NamesrvStartup.createNamesrvController(String[])123456Options options = ServerUtil.buildCommandlineOptions(new Options());commandLine = ServerUtil.parseCmdLine("mqnamesrv", args, buildCommandlineOptions(options), new PosixParser());if (null == commandLine) &#123; System.exit(-1); return null;&#125; 这一段是使用了Apache Commons CLI命令行（Command Line Interface）解析工具，它主要是根据运行时传递进来的参数生成commandLine命令行对象，用于解析运行时类似于 -c 指定文件路径，然后填充到 namesrvConfig 和 nettyServerConfig 对象中。那么，之后的两个小步骤，就是 这两个配置的填充过程了： org.apache.rocketmq.namesrv.NamesrvStartup.createNamesrvController(String[]) 123456789101112131415161718final NamesrvConfig namesrvConfig = new NamesrvConfig();final NettyServerConfig nettyServerConfig = new NettyServerConfig();nettyServerConfig.setListenPort(9876);if (commandLine.hasOption('c')) &#123; String file = commandLine.getOptionValue('c'); if (file != null) &#123; InputStream in = new BufferedInputStream(new FileInputStream(file)); properties = new Properties(); properties.load(in); MixAll.properties2Object(properties, namesrvConfig); MixAll.properties2Object(properties, nettyServerConfig); namesrvConfig.setConfigStorePath(file); System.out.printf("load config properties file OK, %s%n", file); in.close(); &#125;&#125; 这段是createNamesrvController(String[] args)方法最为核心的代码，从代码可以看到，先创建NamesrvConfig和NettyServerConfig对象，接着利用commandLine命令行工具读取-c指定的配置文件路径，然后将其读取到流中，生成properties对象，最后将namesrvConfig和nettyServerConfig对象进行初始化。 这里省略一些没有那么重要的细节代码，譬如支持配置查看模式（命令行添加 -p，则把当前的环境配置输出来）、日志上下文配置（包括日志文件路径设定，Logger获取），这里不细看，接着往下看： org.apache.rocketmq.namesrv.NamesrvStartup.createNamesrvController(String[])123456final NamesrvController controller = new NamesrvController(namesrvConfig, nettyServerConfig);// remember all configs to prevent discardcontroller.getConfiguration().registerConfig(properties);return controller; 到这里就是水到渠成，利用namesrvConfig和nettyServerConfig对象创建NamesrvController对象，然后在注册一遍properties，防止丢失。 启动NamesrvController回到 org.apache.rocketmq.namesrv.NamesrvStartup.main0(String[] args) 方法，看进去 start(NamesrvController) 方法org.apache.rocketmq.namesrv.NamesrvStartup.start(NamesrvController) 123456789101112131415161718192021222324public static NamesrvController start(final NamesrvController controller) throws Exception &#123; if (null == controller) &#123; throw new IllegalArgumentException("NamesrvController is null"); &#125; boolean initResult = controller.initialize(); if (!initResult) &#123; controller.shutdown(); System.exit(-3); &#125; Runtime.getRuntime().addShutdownHook(new ShutdownHookThread(log, new Callable&lt;Void&gt;() &#123; @Override public Void call() throws Exception &#123; controller.shutdown(); return null; &#125; &#125;)); controller.start(); return controller; &#125; 这个方法主要是对核心控制器进行初始化操作，其次是注册一个钩子函数，用于JVM进程关闭时，优雅地释放netty服务、线程池等资源，最后对核心控制器进行启动操作，接下来我们继续撸org.apache.rocketmq.namesrv.NamesrvController.initialize() 详细实现：org.apache.rocketmq.namesrv.NamesrvController.initialize()123456789101112131415161718192021222324252627282930313233public boolean initialize() &#123; // 加载kv配置 this.kvConfigManager.load(); // 加载netty网络服务对象，并注册 this.remotingServer = new NettyRemotingServer(this.nettyServerConfig, this.brokerHousekeepingService); this.remotingExecutor = Executors.newFixedThreadPool(nettyServerConfig.getServerWorkerThreads(), new ThreadFactoryImpl("RemotingExecutorThread_")); this.registerProcessor(); // 创建定时任务--每隔10秒扫描一次Broker，并定时删除不活跃的Broker this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() &#123; @Override public void run() &#123; NamesrvController.this.routeInfoManager.scanNotActiveBroker(); &#125; &#125;, 5, 10, TimeUnit.SECONDS); // 创建定时任务--每隔10分钟打印一遍kv配置 this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() &#123; @Override public void run() &#123; NamesrvController.this.kvConfigManager.printAllPeriodically(); &#125; &#125;, 1, 10, TimeUnit.MINUTES); // 之后是注册一个监听器，去监听Ssl上下文环境的变更，主要透过Tls证书文件监听进行 // 。。。 return true; &#125; 至此，就是最后一步了，就是创建Netty服务来进行通信，关于Netty的实现细节较多，这里不深入，要想了解可以看一下Netty相关文章。 路由启动时序图 路由注册路由注册即是Broker向Nameserver注册的过程，它们是通过Broker的心跳功能实现的，既然Nameserver是用来存储Broker的注册信息，那么我们就先来看看Nameserver到底存储了哪些信息，就是上文那张结构图，我们知道RouteInfoManager为路由信息表。 RocketMQ是基于订阅发布机制的，一个Topic拥有多个消息队列，如果不指定队列的数量，一个Broker默认会为每个Topic创建4个读队列和4个写队列，多个Broker组成集群，Broker会通过发送心跳包将自己的信息注册到路由中心，路由中心brokerLiveTable存储Broker的状态，它会根据Broker的心跳包更新Broker状态信息。 Broker发送心跳包org.apache.rocketmq.broker.BrokerController.start()12345678910111213this.registerBrokerAll(true, false, true); // 初次启动，这里会强制执行发送心跳包this.scheduledExecutorService.scheduleAtFixedRate(new Runnable() &#123; @Override public void run() &#123; try &#123; BrokerController.this.registerBrokerAll(true, false, brokerConfig.isForceRegister()); &#125; catch (Throwable e) &#123; log.error("registerBrokerAll Exception", e); &#125; &#125;&#125;, 1000 * 10, Math.max(10000, Math.min(brokerConfig.getRegisterNameServerPeriod(), 60000)), TimeUnit.MILLISECONDS); Broker在核心控制器启动时，会强制发送一次心跳包，接着创建一个定时任务，定时向路由中心发送心跳包。由 brokerConfig.getRegisterNameServerPeriod() 可以看到这个定时时间支持可配置，但是调了两个Math类的限制大小函数，把时间限定在[10，60]这个区间，默认是30秒。接下来，进去org.apache.rocketmq.broker.BrokerController.registerBrokerAll(boolean, boolean, boolean)方法看下：org.apache.rocketmq.broker.BrokerController.registerBrokerAll(boolean, boolean, boolean)1234567891011121314151617181920212223public synchronized void registerBrokerAll(final boolean checkOrderConfig, boolean oneway, boolean forceRegister) &#123; TopicConfigSerializeWrapper topicConfigWrapper = this.getTopicConfigManager().buildTopicConfigSerializeWrapper(); // 创建一个topic包装类 //这里的设计是，如果该broker没有读写权限，那么会新建一个临时的topicConfigTable，再set进包装类 if (!PermName.isWriteable(this.getBrokerConfig().getBrokerPermission()) || !PermName.isReadable(this.getBrokerConfig().getBrokerPermission())) &#123; ConcurrentHashMap&lt;String, TopicConfig&gt; topicConfigTable = new ConcurrentHashMap&lt;String, TopicConfig&gt;(); for (TopicConfig topicConfig : topicConfigWrapper.getTopicConfigTable().values()) &#123; TopicConfig tmp = new TopicConfig(topicConfig.getTopicName(), topicConfig.getReadQueueNums(), topicConfig.getWriteQueueNums(), this.brokerConfig.getBrokerPermission()); topicConfigTable.put(topicConfig.getTopicName(), tmp); &#125; topicConfigWrapper.setTopicConfigTable(topicConfigTable); &#125; // 判断该Broker是否需要发送心跳包 if (forceRegister || needRegister(this.brokerConfig.getBrokerClusterName(), this.getBrokerAddr(), this.brokerConfig.getBrokerName(), this.brokerConfig.getBrokerId(), this.brokerConfig.getRegisterBrokerTimeoutMills())) &#123; doRegisterBrokerAll(checkOrderConfig, oneway, topicConfigWrapper); // 执行发送心跳包 &#125; &#125; 该方法是Broker执行发送心跳包的核心控制方法，它主要做了topic的包装类封装操作，判断Broker此时是否需要执行发送心跳包，但查看org.apache.rocketmq.common.BrokerConfig#forceRegister字段的值，发现写死为true，也就是该判断永远为true，即默认配置下是每次都需要发送心跳包。进去看一下这个控制是否发心跳包的方法org.apache.rocketmq.broker.BrokerController.needRegister(String, String, String, long, int)org.apache.rocketmq.broker.BrokerController.needRegister(String, String, String, long, int)1234567891011121314151617private boolean needRegister(final String clusterName, final String brokerAddr, final String brokerName, final long brokerId, final int timeoutMills) &#123; TopicConfigSerializeWrapper topicConfigWrapper = this.getTopicConfigManager().buildTopicConfigSerializeWrapper(); List&lt;Boolean&gt; changeList = brokerOuterAPI.needRegister(clusterName, brokerAddr, brokerName, brokerId, topicConfigWrapper, timeoutMills); boolean needRegister = false; for (Boolean changed : changeList) &#123; if (changed) &#123; needRegister = true; break; &#125; &#125; return needRegister; &#125; 进去里边发现，是否需要发送心跳包的逻辑还得结合 org.apache.rocketmq.broker.out.BrokerOuterAPI.needRegister(String, String, String, long, TopicConfigSerializeWrapper, int) 方法来判断。org.apache.rocketmq.broker.out.BrokerOuterAPI.needRegister(String, String, String, long, TopicConfigSerializeWrapper, int)12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public List&lt;Boolean&gt; needRegister( final String clusterName, final String brokerAddr, final String brokerName, final long brokerId, final TopicConfigSerializeWrapper topicConfigWrapper, final int timeoutMills) &#123; final List&lt;Boolean&gt; changedList = new CopyOnWriteArrayList&lt;&gt;(); List&lt;String&gt; nameServerAddressList = this.remotingClient.getNameServerAddressList(); if (nameServerAddressList != null &amp;&amp; nameServerAddressList.size() &gt; 0) &#123; final CountDownLatch countDownLatch = new CountDownLatch(nameServerAddressList.size()); for (final String namesrvAddr : nameServerAddressList) &#123; brokerOuterExecutor.execute(new Runnable() &#123; @Override public void run() &#123; try &#123; QueryDataVersionRequestHeader requestHeader = new QueryDataVersionRequestHeader(); requestHeader.setBrokerAddr(brokerAddr); requestHeader.setBrokerId(brokerId); requestHeader.setBrokerName(brokerName); requestHeader.setClusterName(clusterName); RemotingCommand request = RemotingCommand.createRequestCommand(RequestCode.QUERY_DATA_VERSION, requestHeader); request.setBody(topicConfigWrapper.getDataVersion().encode()); RemotingCommand response = remotingClient.invokeSync(namesrvAddr, request, timeoutMills); DataVersion nameServerDataVersion = null; Boolean changed = false; switch (response.getCode()) &#123; case ResponseCode.SUCCESS: &#123; QueryDataVersionResponseHeader queryDataVersionResponseHeader = (QueryDataVersionResponseHeader) response.decodeCommandCustomHeader(QueryDataVersionResponseHeader.class); changed = queryDataVersionResponseHeader.getChanged(); byte[] body = response.getBody(); if (body != null) &#123; nameServerDataVersion = DataVersion.decode(body, DataVersion.class); if (!topicConfigWrapper.getDataVersion().equals(nameServerDataVersion)) &#123; //判断是否一致的DataVersion changed = true; &#125; &#125; if (changed == null || changed) &#123; changedList.add(Boolean.TRUE); &#125; &#125; default: break; &#125; log.warn("Query data version from name server &#123;&#125; OK,changed &#123;&#125;, broker &#123;&#125;,name server &#123;&#125;", namesrvAddr, changed, topicConfigWrapper.getDataVersion(), nameServerDataVersion == null ? "" : nameServerDataVersion); &#125; catch (Exception e) &#123; changedList.add(Boolean.TRUE); log.error("Query data version from name server &#123;&#125; Exception, &#123;&#125;", namesrvAddr, e); &#125; finally &#123; countDownLatch.countDown(); &#125; &#125; &#125;); &#125; try &#123; countDownLatch.await(timeoutMills, TimeUnit.MILLISECONDS); &#125; catch (InterruptedException e) &#123; log.error("query dataversion from nameserver countDownLatch await Exception", e); &#125; &#125; return changedList; &#125; 捋一捋这个方法的逻辑，发现，关键是这个判断 !topicConfigWrapper.getDataVersion().equals(nameServerDataVersion)，就是说，该Broker本地的topic配置的DataVersion与远程调用Namesrv获取回来的DataVersion是否一致，如果不是，则要发送心跳包。 我们再定位到 needRegister 远程调用到路由中心Namesrv的方法： org.apache.rocketmq.namesrv.routeinfo.RouteInfoManager.isBrokerTopicConfigChanged(String, DataVersion)，org.apache.rocketmq.namesrv.routeinfo.RouteInfoManager.queryBrokerTopicConfig(String) 123456789101112public boolean isBrokerTopicConfigChanged(final String brokerAddr, final DataVersion dataVersion) &#123; DataVersion prev = queryBrokerTopicConfig(brokerAddr); return null == prev || !prev.equals(dataVersion); &#125; public DataVersion queryBrokerTopicConfig(final String brokerAddr) &#123; BrokerLiveInfo prev = this.brokerLiveTable.get(brokerAddr); if (prev != null) &#123; return prev.getDataVersion(); &#125; return null; &#125; 发现，关键还是取决于Namesrv这边记录的存活Broker列表BrokerLiveInfo中的dataVersion，如果dataVersion为空或者当前dataVersion不等于brokerLiveTable存储的brokerLiveTable，就说明Broker信息版本对不上号，需要更新，也就是说Broker就需要发送心跳包。 NameSrv处理心跳包Nameserver的netty服务监听收到心跳包之后，会调用到路由中心以下方法进行处理：org.apache.rocketmq.namesrv.routeinfo.RouteInfoManager.registerBroker(String, String, String, long, String, TopicConfigSerializeWrapper, List, Channel)12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394public RegisterBrokerResult registerBroker( final String clusterName, final String brokerAddr, final String brokerName, final long brokerId, final String haServerAddr, final TopicConfigSerializeWrapper topicConfigWrapper, final List&lt;String&gt; filterServerList, final Channel channel) &#123; RegisterBrokerResult result = new RegisterBrokerResult(); try &#123; try &#123; this.lock.writeLock().lockInterruptibly(); // 获取集群下所有的Broker，并将当前Broker加入clusterAddrTable，由于brokerNames是Set结构，并不会重复 Set&lt;String&gt; brokerNames = this.clusterAddrTable.get(clusterName); if (null == brokerNames) &#123; brokerNames = new HashSet&lt;String&gt;(); this.clusterAddrTable.put(clusterName, brokerNames); &#125; brokerNames.add(brokerName); boolean registerFirst = false; BrokerData brokerData = this.brokerAddrTable.get(brokerName); // 获取Broker信息，如果是首次注册，那么新建一个BrokerData并加入brokerAddrTable if (null == brokerData) &#123; registerFirst = true; brokerData = new BrokerData(clusterName, brokerName, new HashMap&lt;Long, String&gt;()); this.brokerAddrTable.put(brokerName, brokerData); &#125; Map&lt;Long, String&gt; brokerAddrsMap = brokerData.getBrokerAddrs(); //Switch slave to master: first remove &lt;1, IP:PORT&gt; in namesrv, then add &lt;0, IP:PORT&gt; //The same IP:PORT must only have one record in brokerAddrTable Iterator&lt;Entry&lt;Long, String&gt;&gt; it = brokerAddrsMap.entrySet().iterator(); while (it.hasNext()) &#123; Entry&lt;Long, String&gt; item = it.next(); if (null != brokerAddr &amp;&amp; brokerAddr.equals(item.getValue()) &amp;&amp; brokerId != item.getKey()) &#123; it.remove(); &#125; &#125; // 这里判断Broker是否是已经注册过 String oldAddr = brokerData.getBrokerAddrs().put(brokerId, brokerAddr); registerFirst = registerFirst || (null == oldAddr); // 如果是Broker是Master节点吗，并且Topic信息更新或者是首次注册，那么创建更新topic队列信息 if (null != topicConfigWrapper &amp;&amp; MixAll.MASTER_ID == brokerId) &#123; if (this.isBrokerTopicConfigChanged(brokerAddr, topicConfigWrapper.getDataVersion()) || registerFirst) &#123; ConcurrentMap&lt;String, TopicConfig&gt; tcTable = topicConfigWrapper.getTopicConfigTable(); if (tcTable != null) &#123; for (Map.Entry&lt;String, TopicConfig&gt; entry : tcTable.entrySet()) &#123; this.createAndUpdateQueueData(brokerName, entry.getValue()); &#125; &#125; &#125; &#125; // 更新BrokerLiveInfo状态信息 BrokerLiveInfo prevBrokerLiveInfo = this.brokerLiveTable.put(brokerAddr, new BrokerLiveInfo( System.currentTimeMillis(), topicConfigWrapper.getDataVersion(), channel, haServerAddr)); if (null == prevBrokerLiveInfo) &#123; log.info("new broker registered, &#123;&#125; HAServer: &#123;&#125;", brokerAddr, haServerAddr); &#125; if (filterServerList != null) &#123; if (filterServerList.isEmpty()) &#123; this.filterServerTable.remove(brokerAddr); &#125; else &#123; this.filterServerTable.put(brokerAddr, filterServerList); &#125; &#125; if (MixAll.MASTER_ID != brokerId) &#123; String masterAddr = brokerData.getBrokerAddrs().get(MixAll.MASTER_ID); if (masterAddr != null) &#123; BrokerLiveInfo brokerLiveInfo = this.brokerLiveTable.get(masterAddr); if (brokerLiveInfo != null) &#123; result.setHaServerAddr(brokerLiveInfo.getHaServerAddr()); result.setMasterAddr(masterAddr); &#125; &#125; &#125; &#125; finally &#123; this.lock.writeLock().unlock(); &#125; &#125; catch (Exception e) &#123; log.error("registerBroker Exception", e); &#125; return result; &#125; 该方法是处理Broker心跳包的最核心方法，它主要做了对RouteInfoManager路由信息的一些更新操作，包括对clusterAddrTable、brokerAddrTable、topicQueueTable、brokerLiveTable等路由信息。 Broker路由注册时序图 路由删除前面部分我们分析了Nameserver启动时会创建一个定时任务，定时剔除不活跃的Broker。org.apache.rocketmq.namesrv.routeinfo.RouteInfoManager.scanNotActiveBroker()12345678910111213public void scanNotActiveBroker() &#123; Iterator&lt;Entry&lt;String, BrokerLiveInfo&gt;&gt; it = this.brokerLiveTable.entrySet().iterator(); while (it.hasNext()) &#123; Entry&lt;String, BrokerLiveInfo&gt; next = it.next(); long last = next.getValue().getLastUpdateTimestamp(); if ((last + BROKER_CHANNEL_EXPIRED_TIME) &lt; System.currentTimeMillis()) &#123; RemotingUtil.closeChannel(next.getValue().getChannel()); it.remove(); log.warn("The broker channel expired, &#123;&#125; &#123;&#125;ms", next.getKey(), BROKER_CHANNEL_EXPIRED_TIME); this.onChannelDestroy(next.getKey(), next.getValue().getChannel()); &#125; &#125; &#125; 剔除Broker信息的逻辑就比较简单了，就是遍历所有存活的Broker的信息，也就是BrokerLiveInfo，从BrokerLiveInfo获取状态信息，判断Broker的心跳时间是否已经超过限定值，如果超过，则执行剔除的逻辑。这个逻辑，会放在Namesrv初始化方法initialize()里以一个定时任务来执行，默认每隔10秒执行一次。 路由发现RocketMQ路由发现，是非实时的。当Topic路由出现变化后，Namesrv不主动推送给客户端，而是由客户端定时拉取主题最新的路由。根据主题名称拉取路由信息的命令编码为： GET ROUTEINTOBY_TOPIC 。RocketMQ 路由结果如下： • String orderTopicConf ：顺序消息配置内容，来自于kvConfig 。• List&lt; QueueData&gt; queueData ： topic 队列元数据。• List brokerDatas ： topic 分布的broker 元数据。• HashMap&lt; String/ brokerAdress/,List / filterServer /&gt; ： broker 上过滤服务器地址列表 NameServer 路由发现实现方法：org.apache.rocketmq.namesrv.processor.DefaultRequestProcessor.getRouteInfoByTopic(ChannelHandlerContext, RemotingCommand) 12345678910111213141516171819202122232425262728public RemotingCommand getRouteInfoByTopic(ChannelHandlerContext ctx, RemotingCommand request) throws RemotingCommandException &#123; final RemotingCommand response = RemotingCommand.createResponseCommand(null); final GetRouteInfoRequestHeader requestHeader = (GetRouteInfoRequestHeader) request.decodeCommandCustomHeader(GetRouteInfoRequestHeader.class); TopicRouteData topicRouteData = this.namesrvController.getRouteInfoManager().pickupTopicRouteData(requestHeader.getTopic()); if (topicRouteData != null) &#123; if (this.namesrvController.getNamesrvConfig().isOrderMessageEnable()) &#123; String orderTopicConf = this.namesrvController.getKvConfigManager().getKVConfig(NamesrvUtil.NAMESPACE_ORDER_TOPIC_CONFIG, requestHeader.getTopic()); topicRouteData.setOrderTopicConf(orderTopicConf); &#125; byte[] content = topicRouteData.encode(); response.setBody(content); response.setCode(ResponseCode.SUCCESS); response.setRemark(null); return response; &#125; response.setCode(ResponseCode.TOPIC_NOT_EXIST); response.setRemark("No topic route info in name server for the topic: " + requestHeader.getTopic() + FAQUrl.suggestTodo(FAQUrl.APPLY_TOPIC_URL)); return response; &#125; Step 1：调用RouterlnfoManager 的方法，从路由表topicQueueTable 、brokerAddrTable 、filterServerTable 中分别填充TopicRouteData 中的List&lt;QueueData＞、List&lt;BrokerData＞和 filterServer 地址表。Step 2 ： 如果找到主题对应的路由信息并且该主题为顺序消息，则从NameServerKVconfig 中获取关于顺序消息相关的配置填充路由信息。如果找不到路由信息CODE 则使用TOPIC NOT_EXISTS ，表示没有找到对应的路由。 小结本文主要介绍了NameServer 路由功能，包含路由元数据、路由注册与发现机制。下面以一张图来总结Namesrv的路由注册、删除机制。 Namesrv路由发现与删除机制的设计，主要是，消息生产者（Producer）在发送消息之前先从Namesrv获取Broker服务器地址列表，然后根据负载算法从列表中选择一台消息服务器进行发送 。Namesrv与每台Broker服务器保持长连接，Broker每隔30秒（可配置，默认30秒）发送一次心跳向Namesrv报告自己存活，而Namesrv则间隔10秒检测一次Broker是否存活（对比上一次报告时间是否超过120秒），如果检测到Broker宕机，则从路由注册表（BrokerLiveInfo）中将其移除。但是这样的设计存在这样一种情况： NameServer 需要等Broker 失效至少120秒才能将该Broker 从路由表中移除掉，那如果在Broker 故障期间，因为没有立马把路由变化通知到消息生产者，消息生产者Producer 根据主题获取到的路由信息包含已经看机的Broker ，会导致消息发送失败，那这种情况怎么办，岂不是消息发送不是高可用的？这个其实是在消息发送端提供容错机制来保证消息的高可用性的，这样的设计，主要是为了降低Namesrv实现的复杂性。而Namesrv本身的高可用，可以通过部署多台Namesrv服务器来实现，但彼此之间互不通信，这一点，跟我们熟悉的zookeeper，etcd这种不一样（namesrv不像zk那样分master、slave），这其实也是RocketMQ的namesrv设计的一个亮点，因为不需要维护master、slave的状态同步，所以实现更加简答。从以上亮点，可以看出，RocketMQ的Namesrv设计上追求简单高效。]]></content>
  </entry>
  <entry>
    <title><![CDATA[RocketMQ调试环境配置]]></title>
    <url>%2F2019%2F05%2F18%2FRocketMQ-Debug-Environment%2F</url>
    <content type="text"><![CDATA[调试环境及工具 JDK 1.8+ Maven IDE工具（Eclipse/IntelliJ IDEA） 拉取源码RocketMQ的官方仓库 https://github.com/apache/rocketmq ，为方便调试，可以 Fork 出属于自己的仓库。使用Eclipse从仓库里拉取代码。拉取并以Maven项目形式导入IDE工具，完成后，Maven会自动下载依赖包。 搭建之前先来一张RocketMQ物理部署架构图（集群）： 下文只介绍最小化的RocketMQ环境部署，暂时不考虑Namesrv集群、Broker集群、Comsumer集群。本文所涉及RocketMQ源码基于 4.5.1 版本（master分支）。搭建调试环境的过程： 启动 RocketMQ Namesrv （元数据管理，Broker、Producer、Consumer各组件的服务注册与发现） 启动 RocketMQ Broker （真正意义上的MQ服务端，接收消息、推送消费） 启动 RocketMQ Producer （消息生产者，发送消息至Broker） 启动 RocketMQ Consumer （消息消费者，消费Broker推送过来的消息） 消息服务端（Namesrv及Broker）的启动方式： 配置文件方式（配置文件多，路径多，配置项多而繁琐，不推荐×） 代码启动方式（代码简单可控，推荐√） 启动Namesrv配置文件方式以Eclipse为例简单说明一下，以配置文件方式来启动Namesrv模块 打开namesrv模块下的NamesrvStartup.java，选择Debug As，配置Debug Configuration，弹出De bug Configuration s 对话框。 选中Java Application 条目并单击右键，选择New 弹出Debug Configurations 对话框 设置RocketMQ 运行主目录。选择Environment 选项卡，添加环境变量ROCKET_HOME 在RocketMQ 运行主目录中创建conf 、logs 、store 三个文件夹 从RocketMQ distribution 部署目录中将broker.conf、logback_ broker.xml 文件复制到conf 目录中， logback_ namesrv.xml 文件则只需修改日志文件的目录， broker.conf 文件内容如下所示。定义一个自己的RocketMQ store目录路径，替换即可。 在Eclipse Debug 中运行NamesrvStartup ，并输出“ The Name Server boot success.Serializetype=JSON ” 。 代码启动方式（推荐√）打开 org.apache.rocketmq.namesrv.NameServerInstanceTest 单元测试类，参考 #startup() 方法，我们编写一个自己的启动器，NamesrvStarter， 包含 #main(String[] args) 静态方法，代码如下：12345678910111213public static void main(String[] args) throws Exception &#123; // NamesrvConfig 配置 final NamesrvConfig namesrvConfig = new NamesrvConfig(); // NettyServerConfig 配置 final NettyServerConfig nettyServerConfig = new NettyServerConfig(); nettyServerConfig.setListenPort(9876); // 设置端口，不设置的话默认9876端口 // 创建 NamesrvController 对象，并启动 NamesrvController namesrvController = new NamesrvController(namesrvConfig, nettyServerConfig); namesrvController.initialize(); namesrvController.start(); // 休眠1天，一直hold住线程不退出 Thread.sleep(DateUtils.MILLIS_PER_DAY);&#125; 核心配置均在代码里体现，右键运行，RocketMQ Namesrv 就启动完成。输出日志如下：1207:54:03.354 [NettyEventExecutor] INFO RocketmqRemoting - NettyEventExecutor service started07:54:03.355 [FileWatchService] INFO RocketmqCommon - FileWatchService service started 启动Broker配置文件方式以Eclipse为例简单说明一下，以配置文件方式来启动 Broker 模块 打开BrokerStartup.java，移动到Debug As ，选中Debug Configurations ，在弹出的对话框，选择arguments 选项卡，配置 -c 属性指定 broker 配置文件路径。 切换选项卡Environment ，配置RocketMQ 主目录。 以Debug 模式运行BrokerStartup.java ，查看${ROCKET_HOME} /logs/ broker.log 文件，没报错则表示启动成功。 代码启动方式（推荐√）打开 org.apache.rocketmq.broker.BrokerControllerTest 单元测试类，参考 #testBrokerRestart() 方法，我们编写一个BrokerStarter类，包含#main(String[] args) 方法，代码如下：123456789101112131415161718192021222324252627282930public static void main(String[] args) throws Exception &#123; // 设置版本号 System.setProperty(RemotingCommand.REMOTING_VERSION_KEY, Integer.toString(MQVersion.CURRENT_VERSION)); // NettyServerConfig 配置 final NettyServerConfig nettyServerConfig = new NettyServerConfig(); nettyServerConfig.setListenPort(10911); // BrokerConfig 配置 final BrokerConfig brokerConfig = new BrokerConfig(); brokerConfig.setBrokerName("broker-a"); brokerConfig.setNamesrvAddr("127.0.0.1:9876"); // MessageStoreConfig 配置 final MessageStoreConfig messageStoreConfig = new MessageStoreConfig(); messageStoreConfig.setDeleteWhen("04"); messageStoreConfig.setFileReservedTime(48); messageStoreConfig.setFlushDiskType(FlushDiskType.ASYNC_FLUSH); messageStoreConfig.setDuplicationEnable(false); BrokerPathConfigHelper.setBrokerConfigPath("D:/Users/xieruxu669/rockemq/conf/broker.conf"); // 指定broker配置文件的路径 // 创建 BrokerController 对象，并启动 BrokerController brokerController = new BrokerController(// brokerConfig, // nettyServerConfig, // new NettyClientConfig(), // messageStoreConfig); brokerController.initialize(); brokerController.start(); // 休眠1天，一直hold住线程不退出持续1天 System.out.println("Broker started...休眠1天"); Thread.sleep(DateUtils.MILLIS_PER_DAY);&#125; 然后，右键运行，RocketMQ Broker 就启动完成了。输出日志如下：1Broker started...休眠1天 莫惊讶，Broker启动过程其实是没有自己的INFO日志的，但是，Broker起来必然会到Namesrv上面注册，所以，打开Namesrv的控制台，会看到输出如下日志：12345678910111207:57:55.033 [NettyServerCodecThread_1] INFO RocketmqRemoting - NETTY SERVER PIPELINE: channelRegistered 127.0.0.1:259707:57:55.035 [NettyServerCodecThread_1] INFO RocketmqRemoting - NETTY SERVER PIPELINE: channelActive, the channel[127.0.0.1:2597]07:57:55.231 [RemotingExecutorThread_1] DEBUG RocketmqNamesrv - receive request, 103 127.0.0.1:2597 RemotingCommand [code=103, language=JAVA, version=315, opaque=0, flag(B)=0, remark=null, extFields=&#123;brokerId=0, bodyCrc32=1180178866, clusterName=DefaultCluster, brokerAddr=10.188.32.24:10911, haServerAddr=10.188.32.24:10912, compressed=false, brokerName=broker-a&#125;, serializeTypeCurrentRPC=JSON]07:57:55.263 [RemotingExecutorThread_1] INFO RocketmqNamesrv - new topic registered, %RETRY%please_rename_unique_group_name_4 QueueData [brokerName=broker-a, readQueueNums=1, writeQueueNums=1, perm=6, topicSynFlag=0]07:57:55.264 [RemotingExecutorThread_1] INFO RocketmqNamesrv - new topic registered, BenchmarkTest QueueData [brokerName=broker-a, readQueueNums=1024, writeQueueNums=1024, perm=6, topicSynFlag=0]07:57:55.264 [RemotingExecutorThread_1] INFO RocketmqNamesrv - new topic registered, OFFSET_MOVED_EVENT QueueData [brokerName=broker-a, readQueueNums=1, writeQueueNums=1, perm=6, topicSynFlag=0]07:57:55.264 [RemotingExecutorThread_1] INFO RocketmqNamesrv - new topic registered, TopicTest QueueData [brokerName=broker-a, readQueueNums=4, writeQueueNums=4, perm=6, topicSynFlag=0]07:57:55.264 [RemotingExecutorThread_1] INFO RocketmqNamesrv - new topic registered, broker-a QueueData [brokerName=broker-a, readQueueNums=1, writeQueueNums=1, perm=7, topicSynFlag=0]07:57:55.264 [RemotingExecutorThread_1] INFO RocketmqNamesrv - new topic registered, TBW102 QueueData [brokerName=broker-a, readQueueNums=8, writeQueueNums=8, perm=7, topicSynFlag=0]07:57:55.265 [RemotingExecutorThread_1] INFO RocketmqNamesrv - new topic registered, SELF_TEST_TOPIC QueueData [brokerName=broker-a, readQueueNums=1, writeQueueNums=1, perm=6, topicSynFlag=0]07:57:55.266 [RemotingExecutorThread_1] INFO RocketmqNamesrv - new topic registered, DefaultCluster QueueData [brokerName=broker-a, readQueueNums=16, writeQueueNums=16, perm=7, topicSynFlag=0]07:57:55.270 [RemotingExecutorThread_1] INFO RocketmqNamesrv - new broker registered, 10.188.32.24:10911 HAServer: 10.188.32.24:10912b new broker registered, 10.188.32.24:10911 HAServer: 10.188.32.24:10912b 这说明，Broker已经注册到了Namesrv上，也就是妥妥滴起来了。 可以用telnet命令测试一下连接端口1telnet 127.0.0.1 10911 启动 Producer123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/** * This class demonstrates how to send messages to brokers using provided &#123;@link DefaultMQProducer&#125;. */public class Producer &#123; public static void main(String[] args) throws MQClientException, InterruptedException &#123; /* * Instantiate with a producer group name. */ DefaultMQProducer producer = new DefaultMQProducer("please_rename_unique_group_name"); /* * Specify name server addresses. * &lt;p/&gt; * * Alternatively, you may specify name server addresses via exporting environmental variable: NAMESRV_ADDR * &lt;pre&gt; * &#123;@code * producer.setNamesrvAddr("name-server1-ip:9876;name-server2-ip:9876"); * &#125; * &lt;/pre&gt; */ /* * Launch the instance. */ producer.setNamesrvAddr("127.0.0.1:9876"); // 设定 Namesrv 的地址与端口 producer.start(); for (int i = 0; i &lt; 1000; i++) &#123; try &#123; /* * Create a message instance, specifying topic, tag and message body. */ Message msg = new Message("TopicTest" /* Topic */, "TagA" /* Tag */, ("Hello RocketMQ " + i).getBytes(RemotingHelper.DEFAULT_CHARSET) /* Message body */ ); /* * Call send message to deliver message to one of brokers. */ SendResult sendResult = producer.send(msg); System.out.printf("%s%n", sendResult); &#125; catch (Exception e) &#123; e.printStackTrace(); Thread.sleep(1000); &#125; &#125; /* * Shut down once the producer instance is not longer in use. */ producer.shutdown(); &#125;&#125; 这里第 28 行，增加了 producer.setNamesrvAddr(“127.0.0.1:9876”) 代码块，指明 Producer 使用的 Namesrv。送命题这里有道送命题：为什么 Producer 这里指明的是 Namesrv 的地址跟端口，而不是 Broker 呢？ 启动 Consumer打开 org.apache.rocketmq.example.quickstart.Consumer 示例类，代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class Consumer &#123; public static void main(String[] args) throws InterruptedException, MQClientException &#123; /* * Instantiate with specified consumer group name. */ DefaultMQPushConsumer consumer = new DefaultMQPushConsumer("please_rename_unique_group_name_4"); /* * Specify name server addresses. * &lt;p/&gt; * * Alternatively, you may specify name server addresses via exporting environmental variable: NAMESRV_ADDR * &lt;pre&gt; * &#123;@code * consumer.setNamesrvAddr("name-server1-ip:9876;name-server2-ip:9876"); * &#125; * &lt;/pre&gt; */ /* * Specify where to start in case the specified consumer group is a brand new one. */ consumer.setNamesrvAddr("127.0.0.1:9876"); // 设定 Namesrv 的地址与端口 consumer.setConsumeFromWhere(ConsumeFromWhere.CONSUME_FROM_FIRST_OFFSET); /* * Subscribe one more more topics to consume. */ consumer.subscribe("TopicTest", "*"); /* * Register callback to execute on arrival of messages fetched from brokers. */ consumer.registerMessageListener(new MessageListenerConcurrently() &#123; @Override public ConsumeConcurrentlyStatus consumeMessage(List&lt;MessageExt&gt; msgs, ConsumeConcurrentlyContext context) &#123; System.out.printf("%s Receive New Messages: %s %n", Thread.currentThread().getName(), msgs); return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; &#125; &#125;); /* * Launch the consumer instance. */ consumer.start(); System.out.printf("Consumer Started.%n"); &#125;&#125; 在 25 行，我们还增加了 consumer.setNamesrvAddr(“127.0.0.1:9876”) 代码块，指明 Consumer 使用的 RocketMQ Namesrv 。 然后，右键运行，RocketMQ Consumer 就启动完成。输入日志如下： 1208:17:20.362 [main] DEBUG i.n.u.i.l.InternalLoggerFactory - Using SLF4J as the default logging frameworkConsumer Started. 如有消息堆积，则会在启动时刷刷出现消费消息的日志再来一问，这里为何指定的不是Broker，而是Namesrv？]]></content>
  </entry>
  <entry>
    <title><![CDATA[MySQL-Transaction-MVCC]]></title>
    <url>%2F2019%2F05%2F07%2FMySQL-Transaction-MVCC%2F</url>
    <content type="text"><![CDATA[事务ACID事务的四大特性，Atomicity(原子性), Consistency(一致性), Isolation(隔离性), Durability(持久性)。 A（原子性）一个事务的操作，要么全部执行，要么全部不执行。 C（一致性）事务总是从一个一致的状态转换到另一个一致的状态。在事务开始之前和结束之后，数据库的完整性约束没有被破坏。 I（隔离性）隔离性决定了一个事务的修改结果在什么时候能够被其他事务看到。隔离性的概念，离不开并发控制（concurrency control）、可串行化（serializability）、锁（lock）等概念。 D（持久性）事务一旦提交，所有的变化都是永久的，即使发生宕机等故障，数据库也能将数据恢复。持久性保证的是事务系统的高可靠性，而非高可用。事务本身并不保证高可用性，需要一些系统共同配合来完成。 事务特性的实现MySQL事务的原子性和持久性，靠redo log实现，redo log称为重做日志，通常是物理日志，记录的是页的物理修改操作。一致性则用undo log来保证，undo log是回滚日志，是逻辑日志，根据每行纪录进行记录。两种日志的作用都可以视为一种恢复操作，redo恢复的是提交事务修改的页操作，而undo回滚行记录到某个特定的版本。 隔离级别SQL标准定义的四个隔离级别： Read Uncommitted（读未提交，RU）可以读到其它未提交事务导致的数据变更。最低的隔离级别，存在脏读的问题。 Read Committed（读已提交，RC）可以读到已提交的事务导致的数据变更，解决了RU的脏读问题，但存在不可重复读的问题。 Repeatable Read（可重复读，RR）InnoDB存储引擎默认支持的隔离级别便是Repeatable Read。可重复读，解决了RC的不可重复读问题，但也可能存在幻读的问题。InnoDB在RR级别下，使用Next-Key Lock算法，可以避免幻读的产生，所以说，在RR级别下已经能完全保证事务的最高隔离性要求，也就是以下的Serializable级别。 Serializable（串行化）最高的隔离级别，事务间串行化操作，可以理解为事务间无并发。 隔离级别由上至下依次为越来越高，而隔离级别越低则事务请求的锁就越少，或者锁保持的时间就越短。这就是为何大多数数据库系统默认的事务隔离级别是Read Committed（读已提交）。 并发控制并发的任务对同一个临界资源进行操作，如果不采取措施，可能导致不一致，故必须进行并发控制(Concurrency Control)。 锁如Java中的锁一样，利用普通锁的互斥性是保证一致性的常用手段，本质上一种串行化的执行过程。普通的锁，体现的是独占性： 操作数据前，锁住，实施互斥，不允许其他的并发任务操作; 操作完成后，释放锁，让其他任务执行; 这种方式简单粗暴，但性能上可以有优化的空间，于是，出现新的锁种类： 共享锁(Share Locks，记为S锁)，读取数据时加S锁 排他锁(eXclusive Locks，记为X锁)，修改数据时加X锁 共享锁之间不互斥，也就是：读读可并行排他锁与任何锁都互斥，也就是：写读、写写都不可并行 排他锁的劣在于，一旦有写，对读也存在串行化的影响，没法提高并发。而事实上可以优化，读可以不受写阻塞。数据多版本就是为了提高并发的一种优化手段。 数据多版本核心原理在于：读与写操作的数据，副本隔离不一致 写任务发生时，将数据克隆一份，以版本号区分; 写任务操作新克隆的数据，直至提交; 并发读任务可以继续读取旧版本的数据，不至于阻塞; 基于同一个初始版本v0，写任务将克隆一份数据副本，进行修改，修改后的数据视为新版本v1，但写任务未提交前，所有读任务都读取初始版本v0，不会受写任务阻塞。 提高并发的演进思路，就在于此： 普通锁，本质是串行执行读写锁，可以实现读读并发数据多版本，可以实现读写并发 Redo log的必要性Redo log用于实现事务的持久性（ACID里的D）。数据库事务提交后，必须将更新后的数据刷到磁盘上，以保证ACID特性。但这种数据落盘的方式，是随机写，性能较低，如果每次数据落盘都立马操作这种随机写，便会影响吞吐量。于是，默认通过“提交时强制写日志”（Force Log at Commit）的机制进行优化（当然，也可以设置为不强制写日志），也就是说，必须在将事务的数据变更行为记入redo log并进行日志持久化后，再等事务的commit操作完成后，这才算事务完成。 Redo log记录的两个阶段分别是： 先写入文件缓存 后必须进行一次fsync操作（确保持久化） 日志的写入采取文件末尾追加的方式，也就是顺序写，而后再定期（较短的时间间隔）fsync落盘，顺序写比之随机写，性能得到大大的提升。 假如某一时刻，数据库崩溃，还没来得及刷盘的数据，在数据库重启后，会重做redo日志文件里的内容，以保证已提交事务对数据产生的影响都刷到磁盘上。 Undo log的必要性Undo log用以保证原子性（A）与一致性（C），以及配合实现innoDB的MVCC机制。 数据库事务未提交时，会将事务修改数据的镜像(即修改前的旧版本)存放到undo日志里，当事务回滚时，或者数据库奔溃时，可以利用undo日志，即旧版本数据，撤销未提交事务对数据库产生的影响。 对于insert操作，undo日志记录新数据的PK(ROW_ID)，回滚时直接删除;对于delete/update操作，undo日志记录旧数据row，回滚时直接恢复 而两类操作存放数据的buffer是不同的。除了回滚操作，undo的另一作用便是为MVCC机制提供数据的历史版本回溯。 PS：为什么说undo log是逻辑日志？Undo log的回滚机制，只是保证所有数据变更行为被逻辑上取消了，也就是根据undo log找到数据变更行为，把对应的逆反行为执行上，即成功回滚。对于每个delete行为，则执行insert作回滚；每个update，则执行相反的update把数据恢复回去；而每个insert，则执行delete，但是这样并不会把物理存储结构的变化给变更回去，譬如，用户执行了insert 10W条记录的一个事务，会导致表空间增大，但在rollback完成后，仅仅是表数据记录回滚，表空间的大小并不会因此而收缩。而为了保证undo log的持久性，undo log的写入操作也会引发redo log的产生。 多版本并发控制（MVCC）回溯到数据历史版本首先InnoDB每一行数据还有一个DB_ROLL_PT的回滚指针，用于指向该行修改前的上一个历史版本。事实上，MySQL给每行数据添加了三个隐藏字段，分别是该行的隐式行ID（DB_ROW_ID），事务号（DB_TRX_ID）和上述的回滚指针。 当插入一条新数据时，记录上对应的回滚指针为NULL，表明没有历史版本。 更新记录时，原记录将被放入到undo表空间中，并通过DB_ROLL_PT指向该记录。session2查询返回的未修改数据就是从这个undo中返回的。MySQL就是根据记录上的回滚段指针及事务ID判断记录是否可见，如果不可见继续按照DB_ROLL_PT继续回溯查找。 Read View判断行记录是否可见Read View用来判断是否为当前执行事务所见，从而达到对事务之间的数据可见性控制。当开始一个事务或者每个查询语句执行时，把当前系统中活动的事务的ID都拷贝到一个列表，这个列表中最早的事务ID是tmin，最晚的事务ID为tmax，这个列表为Read View。当读到一行数据时，该行上面当前事务ID（DB_TRX_ID）记为tid0（也就是最后一次对数据进行修改的事务的ID），当前行数据是否可见的判断逻辑如图： READ COMMITTED事务内的每个查询语句执行时都会重新创建Read View，这样就会产生不可重复的现象。 REPEATABLE READ事务开始时创建Read View，在事务结束这段时间内，每一次查询都不会重新创建Read View，从而实现了可重复读。 InnoDB为何能够支持高并发回滚段里的数据，其实是历史数据的快照(snapshot)，这些数据是不会被修改，select可以肆无忌惮的并发读取他们。 快照读(Snapshot Read)，这种一致性不加锁的读(Consistent Nonlocking Read)，就是InnoDB并发如此之高的核心原因之一。 这里的一致性是指，事务读取到的数据，要么是事务开始前就已经存在的数据(当然，是其他已提交事务产生的)，要么是事务自身插入或者修改的数据。 普通的select语句都是快照读，显式加锁（select for update/select lock in share mode）的select语句不属于快照读。]]></content>
  </entry>
  <entry>
    <title><![CDATA[AOP]]></title>
    <url>%2F2019%2F04%2F22%2FAOP%2F</url>
    <content type="text"><![CDATA[整合总结，备忘… What’s AOPAspect Oriented Programming，面向切面编程。以 OOP 来对比理解： 纵向关系 OOP，横向角度 AOP 以 日志记录 为例，在没有AOP之前，如果需要在多个方法中进行日志记录，需要在每个方法中都重复编写同一段日志操作代码，哪怕日志操作记录的方法被封装到工具类(LogUtils)，仅需要一行调用即可，这样的操作还是对业务代码有侵入性，而类似日志统计、性能分析等这一类就被称为侵入性业务，使原本的业务逻辑代码跟日志操作这类代码有耦合，并且往往这类代码横跨并嵌入众多模块里边，在各个模块里分散得很厉害，到处都能见到，造成代码维护困难。 从对象组织角度来讲，我们一般采用的分类方法都是使用类似生物学分类的方法，以「继承」关系为主线，我们称之为纵向，也就是OOP。设计时只使用 OOP思想可能会带来两个问题： 对象设计的时候一般都是纵向思维，如果这个时候考虑这些不同类对象的共性，不仅会增加设计的难度和复杂性，还会造成类的接口过多而难以维护（共性越多，意味着接口契约越多）。 需要对现有的对象 动态增加 某种行为或责任时非常困难。而AOP就可以很好地解决以上的问题，怎么做到的？除了这种纵向分类之外，我们从横向的角度去观察这些对象，无需再去到处调用 LogUtils 了，声明哪些地方需要打印日志，这个地方就是一个切面，AOP 会在适当的时机为你把打印语句插进切面。 AOP用处参数校验和判空系统之间在进行接口调用时，往往是有入参传递的，入参是接口业务逻辑实现的先决条件，有时入参的缺失或错误会导致业务逻辑的异常，大量的异常捕获无疑增加了接口实现的复杂度，也让代码显得雍肿冗长，因此提前对入参进行验证是有必要的，可以提前处理入参数据的异常，并封装好异常转化成结果对象返回给调用方，也让业务逻辑解耦变得独立。 权限控制避免到处都是申请权限和处理权限的代码。 无痕埋点围绕方法调用前后进行接口调度次数统计的埋掉操作。 安全控制比如全局的登录状态流程控制。 日志记录常用于方法进入前、执行结果后的日志记录。 性能统计检测方法耗时其实已经有一些现成的工具。痛点是这些工具使用起来都比较麻烦，效率低下，而且无法针对某一个块代码或者某个指定的sdk进行查看方法耗时。可以采用 AOP 思想对每个方法做一个切点，在执行之后打印方法耗时。 事务处理声明方法，为特定方法加上事务，指定情况下（比如抛出异常）回滚事务。 异常处理替代防御性的 try-Catch。 缓存缓存某方法的返回值，下次执行该方法时，直接从缓存里获取。留意一些常用的缓存框架的使用方式，即可发现AOP的应用，譬如guava cache/caffeine，可以在真实的DAO方法前冠上@Cacheable便可以指定方法返回值来被缓存。 设计模式代理模式代理模式又分静态代理与动态代理。 静态代理 代理模式上，基本上有Subject角色，RealSubject角色，Proxy角色。其中：Subject角色负责定义RealSubject和Proxy角色应该实现的接口；RealSubject角色用来真正完成业务服务功能；Proxy角色负责将自身的Request请求，调用realsubject 对应的request功能来实现业务功能，自己不真正做业务。 动态代理通常，我们会使用代理模式来实现 AOP，这就意味着代理模式可以优雅的解决侵入性业务问题。之所以优雅，其中一个点就在于，「动态」二字。较之静态，动态就体现出更加灵活，在运行时动态地对某些东西代理，代理它去做了一些其他的事情。这种动态依赖的是反射机制。因为静态代理需要预先定义好代理类的代码实现，而当大量使用静态代理时，就可能产生大量的代理类，随着类的数量及规模增大，代码维护成本也随之增大，为了解决这个问题，就有了动态地创建代理类的想法。在运行期的代码中生成二进制字节码 由于JVM通过字节码的二进制信息加载类的，那么，如果我们在运行期系统中，遵循Java编译系统组织.class文件的格式和结构，生成相应的二进制数据，然后再把这个二进制数据加载转换成对应的类，这样，就完成了在代码中，动态创建一个类的能力了。 在运行时期可以按照Java虚拟机规范对class文件的组织规则生成对应的二进制字节码。当前有很多开源框架可以完成这些功能，如ASM，Javassist。 Proxy角色在执行代理业务的时候，无非是在调用真正业务之前或者之后做一些“额外”业务。 上图可以看出，代理类处理的逻辑很简单：在调用某个方法前及方法后做一些额外的业务。换一种思路就是：在触发（invoke）真实角色的方法之前或者之后做一些额外的业务。那么，为了构造出具有通用性和简单性的代理类，可以将所有的触发真实角色动作交给一个触发的管理器，让这个管理器统一地管理触发。这种管理器就是Invocation Handler。 动态代理模式的结构跟上面的静态代理模式有所不同的地方，就在于多引入了一个InvocationHandler角色。 在静态代理中，代理Proxy中的方法，都指定了调用了特定的realSubject中的对应的方法： 在上面的静态代理模式下，Proxy所做的事情，无非是调用在不同的request时，调用触发realSubject对应的方法；更抽象点看，Proxy所作的事情；在Java中 方法（Method）也是作为一个对象来看待了， 动态代理工作的基本模式就是将自己的方法功能的实现交给 InvocationHandler角色，外界对Proxy角色中的每一个方法的调用，Proxy角色都会交给InvocationHandler来处理，而InvocationHandler则调用具体对象角色的方法。如下图所示： 在这种模式之中：代理Proxy 和RealSubject 应该实现相同的功能，这一点相当重要。（这里说的功能，可以理解为某个类的public方法） 在面向对象的编程之中，如果想要约定Proxy 和RealSubject可以实现相同的功能，有两种方式：a. 一个比较直观的方式，就是定义一个功能接口，然后让Proxy 和RealSubject来实现这个接口。b. 还有比较隐晦的方式，就是通过继承。因为如果Proxy继承自RealSubject，这样Proxy则拥有了RealSubject的功能，Proxy还可以通过重写RealSubject中的方法，来实现多态。 其中JDK中提供的创建动态代理的机制，是以a这种思路设计的（基于接口），而cglib 则是以b思路设计的（基于类继承）。 资源出自动态代理的神总结文章：Java动态代理机制详解（JDK 和CGLIB，Javassist，ASM） 装饰者模式装饰者模式是在不必改变原类文件和使用继承的情况下，动态地扩展一个对象的功能。它是通过创建一个包装对象，也就是装饰者来包裹真实的对象。装饰者模式的实现上，与静态代理极为相似，都是透过增强被代理者的功能来做到扩展对象。关键的不同点在于：代理强调的是为其他对象提供一种代理以控制对这个对象的访问，而装饰者仅仅强调扩展，并不强调访问控制。而动态代理比装饰者模式更加灵活，被增强的对象和增强的内容都是可以更换的，动态化的。 AOP术语AOP具体的通知包含 @Before，前置通知，执行方法前执行 @AfterReturn，返回通知，正常返回方法后执行 @After，后置通知，方法最终结束后执行，相当于finaly @Around，环绕通知，围绕整个方法 @AfterThrowing，异常通知，抛出异常后执行开发者在命中连接点时，可以通过以上不同的通知，执行对应方法。这就是AOP中的Advisor。 一图胜千言： 辅助记忆各种通知类型： 具体术语包括 Aspect，切面，一个关注点的模块。例子中，LogAspect就是切面。 JoinPoint， 连接点，程序执行中的某个点，某个位置。例子中，testBean.getName()是连接点。 PointCut，切点，切面匹配连接点的点，一般与切点表达式相关，就是切面如何切点。例子中，@PointCut注解就是切点表达式，匹配对应的连接点 Advice，通知，指在切面的某个特定的连接点上执行的动作，也叫增强行为。例子中，before()与after()方法中的代码。 TargetObject，目标对象，指被切入的对象，也就是被代理的对象。例子中，从ctx中取出的testBean则是目标对象。 Weave，织入，将Advice作用在JoinPoint的过程。 一图胜千言（图片源自网络）： Spring AOP过程1、Spring加载自动代理器AnnotationAwareAspectJAutoProxyCreator，当作一个系统组件。 2、当一个bean加载到Spring中时，会触发自动代理器中的bean后置处理 3、bean后置处理，会先扫描bean中所有的Advisor 4、然后用这些Adviosr和其他参数构建ProxyFactory 5、ProxyFactory会根据配置和目标对象的类型寻找代理的方式（JDK动态代理或CGLIG代理） 6、然后代理出来的对象放回context中，完成Spring AOP代理配置，等待被代理类的被调用 7、响应被代理类被调度，设置拦截器回调（JDK Proxy透过InvocationHandler、CGLIB透过DynamicAdvisedInterceptor完成回调）。 源码分析以AspectJ实现为例： Maven POM依赖配置 1234567891011121314151617181920212223&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;version&gt;4.3.3.RELEASE&lt;/version&gt;&lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;4.3.3.RELEASE&lt;/version&gt;&lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aspects&lt;/artifactId&gt; &lt;version&gt;4.3.3.RELEASE&lt;/version&gt;&lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-aop&lt;/artifactId&gt; &lt;version&gt;4.3.3.RELEASE&lt;/version&gt;&lt;/dependency&gt; 定义一个待代理的业务Bean 12345678910111213public class TestBean &#123; private String name; public String getName() &#123; return name; &#125; public void setName( String name ) &#123; this.name = name; &#125;&#125; 定义一个切面Bean 123456789101112131415161718192021222324252627282930313233343536373839404142@Aspectpublic class LogAspect &#123; @Pointcut( "execution(* com.xu.test.aop.TestBean.getName())" ) public void getName() &#123; &#125; // 指向上面的getName()切点，透过getName()的PointCut注解指向真实被代理的方法，也就是execution里的表达式所指向的方法 @Before( "getName()" ) public void before( JoinPoint jp ) &#123; String clazzName = jp.getTarget().getClass().getName(); String methodName = jp.getSignature().getName(); System.out.println( "before " + clazzName + "." + methodName + " executing" ); &#125; @After( "getName()" ) public void after( JoinPoint jp ) &#123; String clazzName = jp.getTarget().getClass().getName(); String methodName = jp.getSignature().getName(); System.out.println( "after " + clazzName + "." + methodName + " executing" ); &#125; // org.springframework.aop.aspectj.AspectJAfterThrowingAdvice// @AfterThrowing( throwing = "ex", pointcut = "getName()" )// public void afterThrowing( JoinPoint jp, Throwable ex ) &#123;// String clazzName = jp.getTarget().getClass().getName();// String methodName = jp.getSignature().getName();// System.out.println( "throw exception when " + clazzName + "." + methodName + " executing" );// &#125; // org.springframework.aop.aspectj.AspectJAroundAdvice// @Around( value = "getName()" ) // // 除了around通知，其余类型的通知都不能用ProceedingJoinPoint，只能用普通的JoinPoint// public void around( ProceedingJoinPoint pjp ) throws Throwable &#123;// String clazzName = pjp.getTarget().getClass().getName();// String methodName = pjp.getSignature().getName();// System.out.println( "before " + clazzName + "." + methodName + " executing" );// Object result = pjp.proceed();// System.out.println( "after " + clazzName + "." + methodName + " executing, result = " + result );// &#125;&#125; 配置业务Bean与切面Bean（aop-test.xml） 1234567891011121314&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:aop="http://www.springframework.org/schema/aop" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd"&gt; &lt;aop:aspectj-autoproxy/&gt; &lt;bean id="testBean" class="com.xu.test.aop.TestBean"/&gt; &lt;bean class="com.xu.test.aop.LogAspect"/&gt;&lt;/beans&gt; 测试代理类执行 123456789101112131415public class TestAOP &#123; public static void main( String[] args ) &#123; ClassPathXmlApplicationContext ctx = new ClassPathXmlApplicationContext("aop-test.xml"); TestBean tb = ctx.getBean( "testBean", TestBean.class ); tb.setName( "xxx" ); String name = tb.getName(); System.out.println( name ); &#125;&#125; 运行结果 before com.xu.test.aop.TestBean.getName executingafter com.xu.test.aop.TestBean.getName executingxxx 上面的例子之所以能完成AOP的代理，只因为Spring的xml配置里面加了这一句 &lt; aop : aspectj-autoproxy / &gt; 加上了这一个配置，使得整个Spring项目拥有了AOP的功能。全局搜索下aspectj-autoproxy这个字段，可以发现，是这个类AspectJAutoProxyBeanDefinitionParser解析了这个元素。 其中的parse方法调用的是AopNamespaceUtils类中的registerAspectJAnnotationAutoProxyCreatorIfNecessary。这个方法作用是初始化一个AOP专用的Bean，并且注册到Spring容器中。 1234567891011class AspectJAutoProxyBeanDefinitionParser implements BeanDefinitionParser &#123; @Override public BeanDefinition parse(Element element, ParserContext parserContext) &#123; AopNamespaceUtils.registerAspectJAnnotationAutoProxyCreatorIfNecessary(parserContext, element); extendBeanDefinition(element, parserContext); return null; &#125; ...&#125; 12345678public static void registerAspectJAnnotationAutoProxyCreatorIfNecessary( ParserContext parserContext, Element sourceElement) &#123; BeanDefinition beanDefinition = AopConfigUtils.registerAspectJAnnotationAutoProxyCreatorIfNecessary( parserContext.getRegistry(), parserContext.extractSource(sourceElement)); useClassProxyingIfNecessary(parserContext.getRegistry(), sourceElement); registerComponentIfNecessary(beanDefinition, parserContext);&#125; 解析这三个操作， 1、第一句，注册一个AnnotationAwareAspectJAutoProxyCreator（称它为自动代理器），这个Creator是AOP的操作核心，也是扫描Bean，代理Bean的操作所在。 2、第二句，解析配置元素，决定代理的模式。其中有JDK动态代理，还有CGLIB代理。 3、第三句，作为系统组件，把Creator这个Bean，放到Spring容器中。让Spring实例化，启动这个Creator。 自动代理器自动代理器 AnnotationAwareAspectJAutoProxyCreator 继承自 AbstractAutoProxyCreator，AbstractAutoProxyCreator里边实现了BeanPostProceesor接口的postProcessAfterInitialization方法，这个方法是在一个Bean被加载并注册到Spring IOC容器后，由BeanFactory回调执行的，也就是说，切面是在目标对象被实例化的时候织入目标对象的，更准确的说，切面是在目标对象的bean在完成初始实例化之后，由bean工厂回调bean的后置处理器将切面织入到目标对象中的。 123456789101112131415/** * Create a proxy with the configured interceptors if the bean is * identified as one to proxy by the subclass. * @see #getAdvicesAndAdvisorsForBean */@Overridepublic Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123; if (bean != null) &#123; Object cacheKey = getCacheKey(bean.getClass(), beanName); if (!this.earlyProxyReferences.contains(cacheKey)) &#123; return wrapIfNecessary(bean, beanName, cacheKey); &#125; &#125; return bean;&#125; 里边的wrapIfNecessary方法会生成一个新的代理对象，返回context（容器上下文）中加载。 AOP最核心的逻辑就在这个 wrapIfNecessary方法里边，里边主要是获取通知(advice/advisor)放到一个名为specificInterceptors的数组里，然后作为参数去调用createProxy方法，创建对应的代理对象：1234567891011121314151617181920212223242526272829303132/** * Wrap the given bean if necessary, i.e. if it is eligible for being proxied. * @param bean the raw bean instance * @param beanName the name of the bean * @param cacheKey the cache key for metadata access * @return a proxy wrapping the bean, or the raw bean instance as-is */protected Object wrapIfNecessary(Object bean, String beanName, Object cacheKey) &#123; if (beanName != null &amp;&amp; this.targetSourcedBeans.contains(beanName)) &#123; return bean; &#125; if (Boolean.FALSE.equals(this.advisedBeans.get(cacheKey))) &#123; return bean; &#125; if (isInfrastructureClass(bean.getClass()) || shouldSkip(bean.getClass(), beanName)) &#123; this.advisedBeans.put(cacheKey, Boolean.FALSE); return bean; &#125; // Create proxy if we have advice. Object[] specificInterceptors = getAdvicesAndAdvisorsForBean(bean.getClass(), beanName, null); if (specificInterceptors != DO_NOT_PROXY) &#123; this.advisedBeans.put(cacheKey, Boolean.TRUE); Object proxy = createProxy( bean.getClass(), beanName, specificInterceptors, new SingletonTargetSource(bean)); this.proxyTypes.put(cacheKey, proxy.getClass()); return proxy; &#125; this.advisedBeans.put(cacheKey, Boolean.FALSE); return bean;&#125; 很显然，这里边有两个核心方法，就是getAdvicesAndAdvisorsForBean（获取通知），还有createProxy（创建代理） 获取通知getAdvicesAndAdvisorsForBean，顾名思义，就是获取被代理的Bean所关联的advice及advisor，自然地，这里有个疑问，advice与advisor是什么关系，为什么获取通知不是只需要获取advice即可？Spring使用org.springframework.aop.Advisor接口表示切面的概念，当完成对目标对象方法的增强行为操作(也就是通知，Advice)和切入点（Point）的设计开发之后，需要一个对象将目标对象、增强行为和切入点三者结合起来，而Advisor(通知器)就是一个实现这个功能的对象，即通过Advisor通知器，可以定义哪些目标对象的哪些方法在什么地方使用这些增强的行为。简单来讲，Advisor=Advice+Point。 1234567public interface Advisor &#123; //获取切面的通知Advice Advice getAdvice(); //判断这个通知是否和某个特定的实例对象相关 boolean isPerInstance(); &#125; 事实上，debug进去AbstractAdvisorAutoProxyCreator对getAdvicesAndAdvisorsForBean的实现代码，可以看到，其实这里确实只是获取Advisor通知器而已，如上文说的，每个Advisor对象持有一个Advice通知，一步步debug，进入到AnnotationAwareAspectJAutoProxyCreator.findCandidateAdvisors()，找到BeanFactoryAspectJAdvisorsBuilder中的buildAspectJAdvisors方法，这个方法里边就是寻找AspectBean，然后返回AspectBean中的所有Advisor的过程实现:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677/** * Look for AspectJ-annotated aspect beans in the current bean factory, * and return to a list of Spring AOP Advisors representing them. * &lt;p&gt;Creates a Spring Advisor for each AspectJ advice method. * @return the list of &#123;@link org.springframework.aop.Advisor&#125; beans * @see #isEligibleBean */public List&lt;Advisor&gt; buildAspectJAdvisors() &#123; List&lt;String&gt; aspectNames = null; synchronized (this) &#123; aspectNames = this.aspectBeanNames; if (aspectNames == null) &#123; List&lt;Advisor&gt; advisors = new LinkedList&lt;Advisor&gt;(); aspectNames = new LinkedList&lt;String&gt;(); String[] beanNames = BeanFactoryUtils.beanNamesForTypeIncludingAncestors(this.beanFactory, Object.class, true, false); for (String beanName : beanNames) &#123; if (!isEligibleBean(beanName)) &#123; continue; &#125; // We must be careful not to instantiate beans eagerly as in this // case they would be cached by the Spring container but would not // have been weaved Class&lt;?&gt; beanType = this.beanFactory.getType(beanName); if (beanType == null) &#123; continue; &#125; if (this.advisorFactory.isAspect(beanType)) &#123; aspectNames.add(beanName); AspectMetadata amd = new AspectMetadata(beanType, beanName); if (amd.getAjType().getPerClause().getKind() == PerClauseKind.SINGLETON) &#123; MetadataAwareAspectInstanceFactory factory = new BeanFactoryAspectInstanceFactory(this.beanFactory, beanName); List&lt;Advisor&gt; classAdvisors = this.advisorFactory.getAdvisors(factory); if (this.beanFactory.isSingleton(beanName)) &#123; this.advisorsCache.put(beanName, classAdvisors); &#125; else &#123; this.aspectFactoryCache.put(beanName, factory); &#125; advisors.addAll(classAdvisors); &#125; else &#123; // Per target or per this. if (this.beanFactory.isSingleton(beanName)) &#123; throw new IllegalArgumentException("Bean with name '" + beanName + "' is a singleton, but aspect instantiation model is not singleton"); &#125; MetadataAwareAspectInstanceFactory factory = new PrototypeAspectInstanceFactory(this.beanFactory, beanName); this.aspectFactoryCache.put(beanName, factory); advisors.addAll(this.advisorFactory.getAdvisors(factory)); &#125; &#125; &#125; this.aspectBeanNames = aspectNames; return advisors; &#125; &#125; if (aspectNames.isEmpty()) &#123; return Collections.emptyList(); &#125; List&lt;Advisor&gt; advisors = new LinkedList&lt;Advisor&gt;(); for (String aspectName : aspectNames) &#123; List&lt;Advisor&gt; cachedAdvisors = this.advisorsCache.get(aspectName); if (cachedAdvisors != null) &#123; advisors.addAll(cachedAdvisors); &#125; else &#123; MetadataAwareAspectInstanceFactory factory = this.aspectFactoryCache.get(aspectName); advisors.addAll(this.advisorFactory.getAdvisors(factory)); &#125; &#125; return advisors;&#125; 譬如，例子中的LogAspect就是AspectBean，它定义了Before与After两个通知（或者说增强行为），那么最终便返回LogAspect中的Before与After对应的Advisor通知器。 创建代理1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859/** * Create an AOP proxy for the given bean. * @param beanClass the class of the bean * @param beanName the name of the bean * @param specificInterceptors the set of interceptors that is * specific to this bean (may be empty, but not null) * @param targetSource the TargetSource for the proxy, * already pre-configured to access the bean * @return the AOP proxy for the bean * @see #buildAdvisors */ protected Object createProxy( Class&lt;?&gt; beanClass, String beanName, Object[] specificInterceptors, TargetSource targetSource) &#123; if (this.beanFactory instanceof ConfigurableListableBeanFactory) &#123; AutoProxyUtils.exposeTargetClass((ConfigurableListableBeanFactory) this.beanFactory, beanName, beanClass); &#125; /** *这里是与后面的代理拦截器链调用的逻辑有关 * ProxyFactory本质上就是一个ProxyConfig，它透过多层继承，最终继承到了ProxyConfig * 继承链路是：ProxyFactory -&gt; ProxyCreatorSupport -&gt; AdvisedSupport -&gt; ProxyConfig * 所以这里new一个ProxyFactory其实就是创建一个ProxyConfig对象， * 这个ProxyConfig对象将会存储与目标对象相匹配的advisor，这就是所谓的织入。 * 等到调用目标对象的时候再将advisor取出来包装（转换）成拦截器。最后组成拦截器链。 */ ProxyFactory proxyFactory = new ProxyFactory(); proxyFactory.copyFrom(this); if (!proxyFactory.isProxyTargetClass()) &#123; if (shouldProxyTargetClass(beanClass, beanName)) &#123; proxyFactory.setProxyTargetClass(true); &#125; else &#123; evaluateProxyInterfaces(beanClass, proxyFactory); &#125; &#125; /** 这里是指定拦截器（specificInterceptors）包装成advisor。 * 这个specificInterceptors是作为这个方法的参数传进来的。 * 也就是说在这之前就已经将与目标对象相匹配的拦截器构建好了。 */ Advisor[] advisors = buildAdvisors(beanName, specificInterceptors); for (Advisor advisor : advisors) &#123; //这里就是织入，将与目标对象相匹配的advisor存储到ProxyConfig对象中。 proxyFactory.addAdvisor(advisor); &#125; proxyFactory.setTargetSource(targetSource); customizeProxyFactory(proxyFactory); proxyFactory.setFrozen(this.freezeProxy); if (advisorsPreFiltered()) &#123; proxyFactory.setPreFiltered(true); &#125; return proxyFactory.getProxy(getProxyClassLoader()); &#125; 把Advisor丢到proxyFactory（ProxyConfig）之后，最后要从proxyFactory里获取一个代理对象。也就是，ProxyFactory的getProxy方法，一路debug进去方法，最终可以在DefaultAopProxyFactory中createAopProxy的实现中，看到代理对象是怎么被生成的：12345678910111213141516171819202122public class DefaultAopProxyFactory implements AopProxyFactory, Serializable &#123; @Override public AopProxy createAopProxy(AdvisedSupport config) throws AopConfigException &#123; if (config.isOptimize() || config.isProxyTargetClass() || hasNoUserSuppliedProxyInterfaces(config)) &#123; Class&lt;?&gt; targetClass = config.getTargetClass(); if (targetClass == null) &#123; throw new AopConfigException("TargetSource cannot determine target class: " + "Either an interface or a target is required for proxy creation."); &#125; if (targetClass.isInterface() || Proxy.isProxyClass(targetClass)) &#123; return new JdkDynamicAopProxy(config); &#125; return new ObjenesisCglibAopProxy(config); &#125; else &#123; return new JdkDynamicAopProxy(config); &#125; &#125; ...&#125; ProxyFactory会根据配置与目标对象的类型，选择用JDK动态代理，还是CGLIB的代理，代理后的对象会放回context中，然后等到程序执行时，会直接调用这个代理类。留意到这里的JDK动态代理 or CGLIB动态代理的选择逻辑，体现到了目标为接口时使用JDK动态代理，目标为类时使用CGLIB动态代理的意思。至此，整个代理的织入、连接过程就已完成。接下来的问题是，调用时怎么给目标类（被代理类）作访问拦截的。 AOP代理拦截上面的代码分析中，已经知道，在Spring AOP通过JDK的Proxy方式或者CGLIB方式生成代理对象的时候，相关的拦截器已经生成并配置到代理对象中去了。那么，拦截器的回调，是怎么设置的呢？有两种方式： JDK的Proxy方式生成代理对象：JdkDynamicAopProxy会通过连接点（ReflectiveMethodInvocation）来调用拦截器链中的拦截器（也就是调用通知方法） CGLIB方式生成代理对象：根据CGLIB使用要求，通过DynamicAdvisedInterceptor来完成回调。 在《Spring技术内幕（第2版）》中有截取到两种方式的拦截器在Spring代码中的实现。 JdkDynamicAopProxy的invoke拦截具体实现看org.springframework.aop.framework.JdkDynamicAopProxy.invoke(Object, Method, Object[])源码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081/** * Implementation of &#123;@code InvocationHandler.invoke&#125;. * &lt;p&gt;Callers will see exactly the exception thrown by the target, * unless a hook method throws an exception. */@Overridepublic Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; MethodInvocation invocation; Object oldProxy = null; boolean setProxyContext = false; TargetSource targetSource = this.advised.targetSource; Class&lt;?&gt; targetClass = null; Object target = null; try &#123; if (!this.equalsDefined &amp;&amp; AopUtils.isEqualsMethod(method)) &#123; return equals(args[0]); &#125; else if (!this.hashCodeDefined &amp;&amp; AopUtils.isHashCodeMethod(method)) &#123; return hashCode(); &#125; else if (method.getDeclaringClass() == DecoratingProxy.class) &#123; return AopProxyUtils.ultimateTargetClass(this.advised); &#125; else if (!this.advised.opaque &amp;&amp; method.getDeclaringClass().isInterface() &amp;&amp; method.getDeclaringClass().isAssignableFrom(Advised.class)) &#123; return AopUtils.invokeJoinpointUsingReflection(this.advised, method, args); &#125; Object retVal; if (this.advised.exposeProxy) &#123; oldProxy = AopContext.setCurrentProxy(proxy); setProxyContext = true; &#125; // 得到目标对象的地方 target = targetSource.getTarget(); if (target != null) &#123; targetClass = target.getClass(); &#125; // 这里获取到定义好的拦截器链 // 这里的this.advised是个AdvisedSupport类型（继承ProxyConfig），就是一个ProxyConfig // 进去getInterceptorsAndDynamicInterceptionAdvice方法，便会发现上文提到的Advisor转换成拦截器的过程 List&lt;Object&gt; chain = this.advised.getInterceptorsAndDynamicInterceptionAdvice(method, targetClass); // 如果没有设定拦截器（拦截器链为空），那么就直接调用target的对应方法 if (chain.isEmpty()) &#123; Object[] argsToUse = AopProxyUtils.adaptArgumentsIfNecessary(method, args); retVal = AopUtils.invokeJoinpointUsingReflection(target, method, argsToUse); &#125; else &#123; // 如果有拦截器的设定，那么需要调用拦截器之后才调用目标对象的相应方法 // 通过构造一个ReflectiveMethodInvocation对象来实现 invocation = new ReflectiveMethodInvocation(proxy, target, method, args, targetClass, chain); // 沿着拦截器链继续前进（执行） retVal = invocation.proceed(); &#125; Class&lt;?&gt; returnType = method.getReturnType(); if (retVal != null &amp;&amp; retVal == target &amp;&amp; returnType.isInstance(proxy) &amp;&amp; !RawTargetAccess.class.isAssignableFrom(method.getDeclaringClass())) &#123; retVal = proxy; &#125; else if (retVal == null &amp;&amp; returnType != Void.TYPE &amp;&amp; returnType.isPrimitive()) &#123; throw new AopInvocationException( "Null return value from advice does not match primitive return type for: " + method); &#125; return retVal; &#125; finally &#123; if (target != null &amp;&amp; !targetSource.isStatic()) &#123; targetSource.releaseTarget(target); &#125; if (setProxyContext) &#123; AopContext.setCurrentProxy(oldProxy); &#125; &#125;&#125; 12345678910111213141516171819/** * Determine a list of &#123;@link org.aopalliance.intercept.MethodInterceptor&#125; objects * for the given method, based on this configuration. * @param method the proxied method * @param targetClass the target class * @return List of MethodInterceptors (may also include InterceptorAndDynamicMethodMatchers) */public List&lt;Object&gt; getInterceptorsAndDynamicInterceptionAdvice(Method method, Class&lt;?&gt; targetClass) &#123; // 这里作了缓存，把已有的interceptor链存起来，除了首次要生成，后续获取都是走的缓存，节省时间 // 这个interceptor链的生成是由advisorChainFactory完成的，在这里使用的是DefaultAdvisorChainFactory MethodCacheKey cacheKey = new MethodCacheKey(method); List&lt;Object&gt; cached = this.methodCache.get(cacheKey); if (cached == null) &#123; cached = this.advisorChainFactory.getInterceptorsAndDynamicInterceptionAdvice( this, method, targetClass); this.methodCache.put(cacheKey, cached); &#125; return cached;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class DefaultAdvisorChainFactory implements AdvisorChainFactory, Serializable &#123; @Override public List&lt;Object&gt; getInterceptorsAndDynamicInterceptionAdvice( Advised config, Method method, Class&lt;?&gt; targetClass) &#123; // This is somewhat tricky... We have to process introductions first, // but we need to preserve order in the ultimate list. List&lt;Object&gt; interceptorList = new ArrayList&lt;Object&gt;(config.getAdvisors().length); Class&lt;?&gt; actualClass = (targetClass != null ? targetClass : method.getDeclaringClass()); boolean hasIntroductions = hasMatchingIntroductions(config, actualClass); AdvisorAdapterRegistry registry = GlobalAdvisorAdapterRegistry.getInstance(); for (Advisor advisor : config.getAdvisors()) &#123; if (advisor instanceof PointcutAdvisor) &#123; // Add it conditionally. PointcutAdvisor pointcutAdvisor = (PointcutAdvisor) advisor; if (config.isPreFiltered() || pointcutAdvisor.getPointcut().getClassFilter().matches(actualClass)) &#123; // Advisor入参，获取到已经注册好的方法拦截器 MethodInterceptor[] interceptors = registry.getInterceptors(advisor); MethodMatcher mm = pointcutAdvisor.getPointcut().getMethodMatcher(); if (MethodMatchers.matches(mm, method, actualClass, hasIntroductions)) &#123; if (mm.isRuntime()) &#123; // Creating a new object instance in the getInterceptors() method // isn't a problem as we normally cache created chains. for (MethodInterceptor interceptor : interceptors) &#123; interceptorList.add(new InterceptorAndDynamicMethodMatcher(interceptor, mm)); &#125; &#125; else &#123; interceptorList.addAll(Arrays.asList(interceptors)); &#125; &#125; &#125; &#125; else if (advisor instanceof IntroductionAdvisor) &#123; IntroductionAdvisor ia = (IntroductionAdvisor) advisor; if (config.isPreFiltered() || ia.getClassFilter().matches(actualClass)) &#123; Interceptor[] interceptors = registry.getInterceptors(advisor); interceptorList.addAll(Arrays.asList(interceptors)); &#125; &#125; else &#123; Interceptor[] interceptors = registry.getInterceptors(advisor); interceptorList.addAll(Arrays.asList(interceptors)); &#125; &#125; return interceptorList; &#125; ...&#125; Cglib2AopProxy的intercept拦截具体实现看org.springframework.aop.framework.Cglib2AopProxy.DynamicAdvisedInterceptor.intercept(Object, Method, Object[], MethodProxy)源码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * General purpose AOP callback. Used when the target is dynamic or when the * proxy is not frozen. */private static class DynamicAdvisedInterceptor implements MethodInterceptor, Serializable &#123; private AdvisedSupport advised; public DynamicAdvisedInterceptor(AdvisedSupport advised) &#123; this.advised = advised; &#125; public Object intercept(Object proxy, Method method, Object[] args, MethodProxy methodProxy) throws Throwable &#123; Object oldProxy = null; boolean setProxyContext = false; Class targetClass = null; Object target = null; try &#123; if (this.advised.exposeProxy) &#123; oldProxy = AopContext.setCurrentProxy(proxy); setProxyContext = true; &#125; target = getTarget(); if (target != null) &#123; targetClass = target.getClass(); &#125; // 从advised(AdvisedSupport类型，继承自ProxyConfig)中取得配置好的通知 List&lt;Object&gt; chain = this.advised.getInterceptorsAndDynamicInterceptionAdvice(method, targetClass); Object retVal; // 如果没有配置的AOP通知，那么就直接调用target的对应方法 if (chain.isEmpty() &amp;&amp; Modifier.isPublic(method.getModifiers())) &#123; retVal = methodProxy.invoke(target, args); &#125; else &#123; // 通过CglibMethodInvocation来启动advice通知 retVal = new CglibMethodInvocation(proxy, target, method, args, targetClass, chain, methodProxy).proceed(); &#125; retVal = massageReturnTypeIfNecessary(proxy, target, method, retVal); return retVal; &#125; finally &#123; if (target != null) &#123; releaseTarget(target); &#125; if (setProxyContext) &#123; AopContext.setCurrentProxy(oldProxy); &#125; &#125; &#125; ...&#125; AOP拦截器的调用两种方式对拦截器的调用都是在ReflectiveMethodInvocation中通过proceed方法实现。在proceed方法中逐个实现拦截器的拦截方法。每个拦截器在执行之前，需要对代理方法完成一个匹配判断（即Pointcut切点中需要进行matches匹配过程）。123456789101112131415161718192021222324252627@Overridepublic Object proceed() throws Throwable &#123; // 从索引为-1的拦截器开始调用，并按序递增 // 如果拦截器链中的拦截器迭代调用完毕，这里开始调用target的函数，这个函数时通过反射机制完成的 // 这个函数具体实现在AopUtils.invokeJoinpointUsingReflection方法中 if (this.currentInterceptorIndex == this.interceptorsAndDynamicMethodMatchers.size() - 1) &#123; return invokeJoinpoint(); &#125; Object interceptorOrInterceptionAdvice = this.interceptorsAndDynamicMethodMatchers.get(++this.currentInterceptorIndex); if (interceptorOrInterceptionAdvice instanceof InterceptorAndDynamicMethodMatcher) &#123; // 这里对拦截器进行动态匹配的判断 //这里是触发进行匹配的地方，如果和定义好的PointCut匹配，那么这个advice将会得到执行 InterceptorAndDynamicMethodMatcher dm = (InterceptorAndDynamicMethodMatcher) interceptorOrInterceptionAdvice; if (dm.methodMatcher.matches(this.method, this.targetClass, this.arguments)) &#123; return dm.interceptor.invoke(this); &#125; else &#123; return proceed(); &#125; &#125; else &#123; return ((MethodInterceptor) interceptorOrInterceptionAdvice).invoke(this); &#125;&#125; 至此，整个AOP实现的来龙去脉，便走完了个大致流程。 JVM级别的AOP基于Java Instrument的Agent实现 Reference Java动态代理机制详解（JDK 和CGLIB，Javassist，ASM） 一文读懂 AOP | 你想要的最全面 AOP 方法探讨 从源码入手，一文带你读懂Spring AOP面向切面编程 Spring设计内幕（第2版）》计文柯]]></content>
  </entry>
  <entry>
    <title><![CDATA[LRU Cache]]></title>
    <url>%2F2019%2F04%2F20%2FLRUCache%2F</url>
    <content type="text"><![CDATA[记录，备忘… LRULRU，即Least Recently Used（最近被使用得最少的），是一种常见的缓存淘汰策略，意思是把缓存中最久未被使用的值优先考虑淘汰。而基于这种淘汰策略的缓存就是所谓的LRU Cache。 LinkedHashMap–现成的LRUJava中的LinkedHashMap，可以简单理解为LinkedList+HashMap，Java中的LinkedList为双向链表，可以维护一个逻辑，就是按一个方向把元素按一个维度来线性排序：表头总是指向当前最近访问的节点，表尾总是指向当前最久未被访问的节点，也就是LRU。 LinkedHashMap数据结构示意图从HashMap角度基于HashMap角度看，就是加了双向指针的HashMap： 从LinkedList角度基于LinkedList角度看，可以简化为链表加上散列： Entry节点 以上仅仅是帮助理解的示意图，并非严谨数据结构图 双向链表简单实现基于HashMap，再实现双向链表的基本操作： 添加操作：由于是添加操作，就意味着可能出现容量不够，也就是链表已满，那么就要执行淘汰策略：删除最久未被访问的节点。 删除操作：把指定节点移除掉，移除前需调整涉及到的各个指针的指向，包括当前节点的前节点的后指针、后节点的前指针，链表的头指针、尾指针。 请求读取元素： 命中：更新表头指针，即把当前命中的节点作为头节点； 未命中：如果是单纯只读操作，而又未命中，则不会引起任何变化。 LRUCache.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148/** * 自定义节点结构简单实现双向链表，节点包含前后双向指针，基于HashMap存储节点达到时间复杂度O(1)的目的，最差为O(1+s)，s为Map.size * 额外维护两个指针：链表的头、尾指针 */public class LRUCache&lt;K, V&gt; &#123; private int capacity = 1 &lt;&lt; 30; private Map&lt;K, Node&gt; map = null; // 头指针 private Node head; // 尾指针 private Node tail; public LRUCache() &#123; map = new HashMap&lt;K, Node&gt;( capacity ); &#125; public LRUCache( int capacity ) &#123; this.capacity = capacity; map = new HashMap&lt;K, Node&gt;( capacity ); &#125; /** * 定义 节点的结构 */ public class Node &#123; private K key; private V value; // 前指针 private Node prev; // 后指针 private Node next; &#125; public void put( K key, V value ) &#123; Node node = map.get( key ); if( null == node ) &#123; node = new Node(); node.key = key; &#125; node.value = value; if( removeEldest( node ) ) &#123; removeTail(); &#125; map.put( key, node ); moveToHead( node ); &#125; /** * 定义成方法，让它可以开放出去，让使用者重写来自定义逻辑 */ protected boolean removeEldest( Node eldest ) &#123; return map.size() &gt;= capacity; &#125; private void removeTail() &#123; if( tail != null ) &#123; remove( tail ); &#125; &#125; /** * 管好4个指针即可： 链表的头指针、尾指针，当前节点的前节点的后指针、后节点的前指针， * 因为是当前节点是被删除的节点，所以自己的前后指针不需要管 * * 1. 如果当前节点是尾节点，把尾指针往前挪一个节点 * 2. 如果当前节点是头节点，把头指针往后挪一个节点 * 3. 如果当前节点的前指针不为空，前节点的后指针指向后节点 * 4. 如果当前节点的后指针不为空，后节点的前指针指向前节点 */ public void remove( Node node ) &#123; if( tail == node ) &#123; tail = tail.prev; &#125; if( head == node ) &#123; head = head.next; &#125; if( node.prev != null ) &#123; node.prev.next = node.next; &#125; if( node.next != null ) &#123; node.next.prev = node.prev; &#125; map.remove( node.key ); &#125; public V get( K key ) &#123; Node node = map.get( key ); if( node != null ) &#123; moveToHead( node ); return node.value; &#125; return null; &#125; /** * 管好最多6个指针即可： * 链表的头指针(head)、尾指针(tail)， * 被移位的节点的前节点的后指针(node.prev.next)、后节点的前指针(node.next.prev)， * 被移位的节点自己的前后指针(node.prev，node.next) */ private void moveToHead( Node node ) &#123; // 头尾指针为空，说明在挪头操作之前，一个节点都没有，而当前节点为新增节点，则只需要把头尾指针都指向当前节点即可 if( tail == null || head == null ) &#123; head = tail = node; return; &#125; // 如果当前节点已经为头节点，则啥也不需要做，直接返回 if( head == node ) &#123; return; &#125; // 如果当前节点是尾节点，则挪头操作后必然要把尾指针需要改为尾节点的上一个节点 if( tail == node ) &#123; tail = tail.prev; &#125; // 如果当前节点的前节点不为空，则把前节点的后指针指向当前节点的前节点 if( node.prev != null ) &#123; node.prev.next = node.next; &#125; // 如果当前节点的后节点不为空，则把后节点的前指针指向当前节点的前节点 if( node.next != null ) &#123; node.next.prev = node.prev; &#125; // 接下来是挪头操作： // 1. 前边已经判断过了头节点是存在的，那么把头节点的前指针由null指向为当前节点 // 2. 当前节点的后指针指向头节点 // 3. 当前节点的前指针指向null // 4. 头指针指向当前节点 head.prev = node; node.next = head; node.prev = null; head = node; &#125; 为什么使用双向链表队列行不行？不行，队列只能做到先进先出，但是如果是访问排在中间的数据，此时无法把中间的数据移动到顶端。 单链表行不行？如果用单链表，能保证实现最新或最热节点放到一侧，但是最久未被使用要放到另一侧，如果只有表头一个指针，那么获取尾节点，则需要从头指针一直到尾，遍历整个单链表，而双向链表同时还有尾指针，尾指针可以帮助直接定位到最后一个节点。更关键的是，在做移动/删除节点的操作的时候，当需要把当前节点的前节点的后指针指向为后节点时（node.prev.next=node.next），获取前节点的操作就只需要透过前指针获取即可，但如果是在单链表中，则需要进行更费时的遍历查找操作来获取到前节点。 为什么使用HashMapHashMap是用于降低寻找节点的时间复杂度的。主要是因为可以快速定位（散列查找的最优时间复杂度是O(1)）。配合LinkedList，定位+移位节点的操作的效率变得更高。 线程安全为了保证Cache的读写在多线程下正确，也就是线程安全，最简单粗暴的就是给引起指针变化的操作及遍历操作全加锁。]]></content>
  </entry>
  <entry>
    <title><![CDATA[关于Netty]]></title>
    <url>%2F2019%2F04%2F14%2FAbout-Netty%2F</url>
    <content type="text"><![CDATA[总结，备忘… What’s NettyNetty是一个 Java 开源框架，一个提供异步的、事件驱动的网络应用程序框架工具，作用是封装了JAVA NIO所支持的多路复用的I/O模型，还封装了Java BIO支持的步网络通信模型，对应用程序层面屏蔽网络底层的实现细节，让应用开发者快速开发高性能，高可靠性的网络服务器和客户端程序。 Why Netty本质上Netty是一个框架，要成为一个主流的框架，首要条件，必须是好用。尽管Java NIO、Java AIO框架己经实现了各主流操作系统的底层支持，但比之Netty还是不够，Netty能提供的好处有更多： 对信息格式的良好封装基于责任链模式的编码和解码功能，提供Java原生NIO没有提供的诸如针对Protocol Buffer、JSON等信息格式的封装。 处理很多上层特有服务框架除了本身要兼容各类操作系统的I/O底层实现，还要提供例如客户端权限，还有上面提到的信息格式封装、简单的数据读取等上层服务。事实上，不仅NIO，netty也有对BIO框架的再次封装，Netty框架是一个面向上层业务实现进行封装的“业务层”框架。 解决Java原生NIO的bug 空轮询问题Linux内核上出现的“不能阻塞导致CPU的使用率100%”问题Bug出现在 Linux 系统环境，大致是说 Java NIO 框架在实现 Linux 内核 kernel 2.6+ 中的 epoll 模型时，Selector.select(timeout)方法不能阻塞指定的 timeout 时间，导致 CPU 100% 的情况。Java官方称在JDK 7版本中问题被解决，Netty 框架在JDK 轩的环境下在 JavaNIO 框架封装之上解决了这 Bug。 空指针问题发生于Selector.open()的NPEJDK-6427854 : (se) NullPointerException in Selector.open() https://bugs.java.com/bugdatabase/view_bug.do?bug_id=6427854这个问题在 Netty 框架中，在负责进行 Java NIO Selector 的 NIOEventLoop 类中得到了解决。 解决半包/粘包问题半包/粘包问题 多路复用I/O模型Netty的核心在于封装了多路复用的I/O模型—-I/O模型 白话Reactor模型经典的现实举例:一个餐厅同时有100位客人到店，他们到店后要做的第一件事情就是点菜。但是问题来了，餐厅老板为了节约人力成本目前只有一位大堂服务员拿着唯一的一本菜单等待客人进行服务。 那么最粗暴（但是最简单）的方法是（记为方法A）：无论有多少客人等待点餐，服务员都把仅有的一份菜单递给其中一位客人，然后站在客人身旁等待这个客人完成点菜过程。在记录客人点菜内容后，把点菜记录交给后堂厨师。然后再来到第二位客人面前将以上工作方式重复一次……然后是第X位客人。很明显，这样设置服务流程是不行的。因为随后的80位客人，在等待超时后就会离店，还会给差评。于是，餐厅老板通过一种办法（记为方法B）进行了改进。老板立刻雇用99名服务员，同时印制99本新的菜单。每一名服务员手持一份菜单负责1位客人（关键不只在于服务员，还在于菜单。因为没有菜单客人也无法点菜）。在客人点完菜后，记录点菜内容交给后堂厨师（当然为了更高效，后堂厨师最好也有100名）。这样每一位客人享受的都是 VIP 服务，客人当然不会走还会给好评。但是高昂的人力成本就让人头疼了。另外一种办法（记为方法C），就是改进点菜的方式：当客人到店后，自己领取一份菜单想好自己要点的菜后，再呼叫服务员。服务员站在客人身边记录点菜内容。将菜单递给厨师的过程也要进行改进，并不是每一份菜单记录好以后 都要交给后堂厨师。服务员可以记录好多份菜单后，同时交给厨师。那么这种方式，对于老板来说人力成本是最低的；对于客人来说，虽然不再享受VIP服务井且要等待一定的时间，但是这些都是可接受的，对于服务员来说，基本上他的时间都没有浪费，被老板榨干了每一滴血汗。如果你是老板，会采用哪种方式呢？到店情况：并发量。到店情况不理想时，一名服务员一份菜单，当然是足够了。所以不同的老板在不同的场合下，将会灵活选择服务员和菜单的配置。客人：客户端请求。点餐内容：客户端发送的实际数据。老板：操作系统。人力成本：系统资源。菜单：文件状态描述符。操作系统对于一个进程能够同时持有的文件状态描述符的个数是有限制的，在Linux系统中可用$ulimit -n 命令查看这个限制值，当然也可以（并且应该）进行内核参数调整。服务员：操作系统内核用于网络 I/O 操作的线程（内核线程）。厨师：应用程序线程（厨房就是应用程序进程）。餐单传递方式：包括了阻塞式和非阻塞式两种。方法A: 阻塞式／非阻塞式，同步I/O。方法B: 使用线程（池）进行处理的阻塞式／非阻塞式同步I/O。方法C: 多路复用网络 I/O 模型。 Reactor线程模型基于Reactor处理模式中，定义以下三种角色: Reactor将I/O事件分派给对应的Handler Acceptor处理客户端新连接，并分派请求到处理器链中 Handlers执行非阻塞读/写任务 单Reactor单线程模型 这是最基本的单Reactor单线程模型。其中Reactor线程，负责多路分离套接字，有新连接到来触发connect事件之后，交由Acceptor进行处理，有IO读写事件之后交给hanlder处理。 Acceptor主要任务就是构建handler，在获取到和client相关的SocketChannel之后，绑定到相应的hanlder上，对应的SocketChannel有读写事件之后，基于racotor分发,hanlder就可以处理了（所有的IO事件都绑定到selector上，由Reactor分发）。 该模型适用于处理器链中业务处理组件能快速完成的场景。不过，这种单线程模型不能充分利用多核资源，所以实际使用的不多。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104 /** * 等待事件到来，分发事件处理 */ class Reactor implements Runnable &#123;​ private Reactor() throws Exception &#123;​ SelectionKey sk = serverSocket.register(selector, SelectionKey.OP_ACCEPT); // attach Acceptor 处理新连接 sk.attach(new Acceptor()); &#125;​ public void run() &#123; try &#123; while (!Thread.interrupted()) &#123; selector.select(); Set selected = selector.selectedKeys(); Iterator it = selected.iterator(); while (it.hasNext()) &#123; it.remove(); //分发事件处理 dispatch((SelectionKey) (it.next())); &#125; &#125; &#125; catch (IOException ex) &#123; //do something &#125; &#125;​ void dispatch(SelectionKey k) &#123; // 若是连接事件获取是acceptor // 若是IO读写事件获取是handler Runnable runnable = (Runnable) (k.attachment()); if (runnable != null) &#123; runnable.run(); &#125; &#125;​ &#125; /** * 连接事件就绪,处理连接事件 */ class Acceptor implements Runnable &#123; @Override public void run() &#123; try &#123; SocketChannel c = serverSocket.accept(); if (c != null) &#123;// 注册读写 new Handler(c, selector); &#125; &#125; catch (Exception e) &#123;​ &#125; &#125; &#125; /** * 处理读写业务逻辑 */ class Handler implements Runnable &#123; public static final int READING = 0, WRITING = 1; int state; final SocketChannel socket; final SelectionKey sk;​ public Handler(SocketChannel socket, Selector sl) throws Exception &#123; this.state = READING; this.socket = socket; sk = socket.register(selector, SelectionKey.OP_READ); sk.attach(this); socket.configureBlocking(false); &#125;​ @Override public void run() &#123; if (state == READING) &#123; read(); &#125; else if (state == WRITING) &#123; write(); &#125; &#125;​ private void read() &#123; process(); //下一步处理写事件 sk.interestOps(SelectionKey.OP_WRITE); this.state = WRITING; &#125;​ private void write() &#123; process(); //下一步处理读事件 sk.interestOps(SelectionKey.OP_READ); this.state = READING; &#125;​ /** * task 业务处理 */ public void process() &#123; //do something &#125; &#125; 单Reactor多线程模型相对于第一种单线程的模式来说，在处理业务逻辑，也就是获取到IO的读写事件之后，交由线程池来处理，这样可以减小主reactor的性能开销，从而更专注的做事件分发工作了，从而提升整个应用的吞吐。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/** * 多线程处理读写业务逻辑 */ class MultiThreadHandler implements Runnable &#123; public static final int READING = 0, WRITING = 1; int state; final SocketChannel socket; final SelectionKey sk;​ //多线程处理业务逻辑 ExecutorService executorService = Executors. newFixedThreadPool(Runtime.getRuntime().availableProcessors());​​ public MultiThreadHandler(SocketChannel socket, Selector sl) throws Exception &#123; this.state = READING; this.socket = socket; sk = socket.register(selector, SelectionKey.OP_READ); sk.attach(this); socket.configureBlocking(false); &#125;​ @Override public void run() &#123; if (state == READING) &#123; read(); &#125; else if (state == WRITING) &#123; write(); &#125; &#125;​ private void read() &#123; //任务异步处理 executorService.submit(() -&gt; process());​ //下一步处理写事件 sk.interestOps(SelectionKey.OP_WRITE); this.state = WRITING; &#125;​ private void write() &#123; //任务异步处理 executorService.submit(() -&gt; process());​ //下一步处理读事件 sk.interestOps(SelectionKey.OP_READ); this.state = READING; &#125;​ /** * task 业务处理 */ public void process() &#123; //do IO ,task,queue something &#125; &#125; 多Reactor多线程模型第三种模型比起第二种模型，是将Reactor分成两部分: mainReactor负责监听server socket，用来处理新连接的建立，将建立的socketChannel指定注册给subReactor。 subReactor维护自己的selector, 基于mainReactor 注册的socketChannel多路分离IO读写事件，读写网 络数据，对业务处理的功能，另外扔给worker线程池来完成。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112/** * 多work 连接事件Acceptor,处理连接事件 */ class MultiWorkThreadAcceptor implements Runnable &#123;​ // cpu线程数相同多work线程 int workCount =Runtime.getRuntime().availableProcessors(); SubReactor[] workThreadHandlers = new SubReactor[workCount]; volatile int nextHandler = 0;​ public MultiWorkThreadAcceptor() &#123; this.init(); &#125;​ public void init() &#123; nextHandler = 0; for (int i = 0; i &lt; workThreadHandlers.length; i++) &#123; try &#123; workThreadHandlers[i] = new SubReactor(); &#125; catch (Exception e) &#123; &#125;​ &#125; &#125;​ @Override public void run() &#123; try &#123; SocketChannel c = serverSocket.accept(); if (c != null) &#123;// 注册读写 synchronized (c) &#123; // 顺序获取SubReactor，然后注册channel SubReactor work = workThreadHandlers[nextHandler]; work.registerChannel(c); nextHandler++; if (nextHandler &gt;= workThreadHandlers.length) &#123; nextHandler = 0; &#125; &#125; &#125; &#125; catch (Exception e) &#123; &#125; &#125; &#125; /** * 多work线程处理读写业务逻辑 */ class SubReactor implements Runnable &#123; final Selector mySelector;​ //多线程处理业务逻辑 int workCount =Runtime.getRuntime().availableProcessors(); ExecutorService executorService = Executors.newFixedThreadPool(workCount);​​ public SubReactor() throws Exception &#123; // 每个SubReactor 一个selector this.mySelector = SelectorProvider.provider().openSelector(); &#125;​ /** * 注册chanel * * @param sc * @throws Exception */ public void registerChannel(SocketChannel sc) throws Exception &#123; sc.register(mySelector, SelectionKey.OP_READ | SelectionKey.OP_CONNECT); &#125;​ @Override public void run() &#123; while (true) &#123; try &#123; //每个SubReactor 自己做事件分派处理读写事件 selector.select(); Set&lt;SelectionKey&gt; keys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; iterator = keys.iterator(); while (iterator.hasNext()) &#123; SelectionKey key = iterator.next(); iterator.remove(); if (key.isReadable()) &#123; read(); &#125; else if (key.isWritable()) &#123; write(); &#125; &#125;​ &#125; catch (Exception e) &#123;​ &#125; &#125; &#125;​ private void read() &#123; //任务异步处理 executorService.submit(() -&gt; process()); &#125;​ private void write() &#123; //任务异步处理 executorService.submit(() -&gt; process()); &#125;​ /** * task 业务处理 */ public void process() &#123; //do IO ,task,queue something &#125; &#125;​ 典型的多路复用 I/O 实现多路复用 I/O 模型在应用层工作效率比我们俗称的 I/O 模型快的本质原因是，前者不再使用操作系统级别的“同步 ”模型，OS操作进程/线程的挂起与恢复涉及的用户态与核心态的切换会引起较大的开销。Linux 操作系统环境下，多路复用 I/O 型就是技术人员通常简称的 NIO 技术。多路复用目前具体的实现主要包括四种： select、poll、epoll、kqueue。 多路复用I/O技术的优缺点优多路复用 I/O 技术由操作系统提供支持，并提供给各种高级语言进行使用。它针对阻塞式同步 I/O 和非阻塞式同步 I/O 而言有很多优势，最直接的效果就是它绕过了 I/O 在操作系统层面的 accept() 方法的阻塞问题。 使用 多路复用I/O 技术 ，应用程序就可以不用再单纯使用多线程技术来解决并发 I/O 处理的性能问题了（针对操作系统内核 I/O 管理模块和应用程序进程而言都是这样的）。在实际业务的处理中，应用程序进程还是需要引入（一般由线程池支持）多线程技术的* 同一个端口可以处理多种网络协议。例如，使用 ServerSocketChannel 类的服务器端口监昕，既可以接收到TCP协议又可以接收 UDP协议内容。也就是说端口的数据接收规则只和 Selector 注册的需要关心的事件有关。 操作系统级别的优化： 多路复用I/O技术可以使操作系统级别在一个端口上能够同时接受多个客户端的I/O时间，同时具有之前我们讲到的阻塞式同步I/O的所有特点。Selector的一部分作用更相当于“轮询代理器”。 依然是同步I/O模型： 多路复用I/O，是基于操作系统级别对“同步I/O”的实现。这里所说的“同步I/O”，简单一句话解释就是：只有上层（包括上层的某种代理机制）系统询问“我”是否有某个事件发生了，否则“我”不会主动告诉上层系统事件发生了。 缺 多路复用 I/O 技术最适用的是“高并发”场景，所谓高并发是指1毫秒内至少同时有成百上千个连接请求准备就绪，其他情况下多路复用 I/O 技术发挥不出它的明显优势。 使用Java NIO 进行功能实现，相对于传统的 Socket 套接宇实现要复杂一些，所以实际应用中，需要根据自己的业务需求进行技术选择。 Netty的几个重要概念 Netty线程机制采用的多Reactor模型：Boss线程+Work线程Boss线程负责发现连接到服务器的新的 Channel (SocketServerChannel 的 ACCEPT事件），并且将这个 Channel经过检查后注册到 Work 连接池的某个 EventLoop 线程中.而当 Work 线程发现操作系统有它感兴趣的 I/O 事件时（例如SocketChannel的READ事件），则调用相应的ChannelHandler事件。当某个channel失效后（例如显示调用ctx.close()），这个channel将从绑定的EventLoop中剔除。在Netty中，如果我们使用的是Java NIO框架实现的对多路复用I/O模型的支持，那么进行这个循环的是NIOEventLoop类（processSelectedKeyPlain方法、processSelectedKey方法）。另外这个类中Netty解决了空轮询bug及Selector.open()的NPE。一个Work线程池的线程将按照底层封装Java NIO框架中Selector的事件状态，决定要执行ChannelHandler中的哪一个事件方法（Netty中包括了channelRegistered、channelUnregistered、channelActive、channelInactive等事件方法）。执行完成后，Work线程将一直轮询直到操作系统回复下一个它所管理的channel发生了新的I/O事件。 ByteBufNetty 重写了 Java NIO 框架中的缓存结构，井将这个结构应用在更上层的封装中。 io.netty.buffer.EmptyByteBuf：这是一个初始容量和最大容量都为0的缓存区。一般我们用这种缓存区描述“没有任何处理结果”，并将其向下 Handler 传递。 io.netty.buffer.ReadOnlyByteBuf：这是一个不允许任何“写请求”的只读缓存区。一般通过Unpooled.unmodifiableBuffer(ByteBuf)方法将某一个可正常读写缓存区转变而成。如果我们需要在下一个 Handler 理的过程中禁止写入任何数据到缓存区，那么就可以在这个 Ha ndler 中进行“只读缓存区”的转换。 io.netty.buffer.UnpooledDirectByteBuf：基本的 Java NIO 框架的 ByteBuffer 封装。直接使用这个缓存区实现来处理 Handler 事件。 io.netty.buffer.PooledByteBuf: Netty 4.x 版本的缓存新特性。主要是为了减少之前unpoolByteBuf 在创建和销毁时的 GC 时间。 ChannelChannel可译为通道。你可以使用 Java NIO 中的 Channel 去初步理解它，但实际上它的意义和 Java NIO 中的通道意义还不 样。我们可以解释成 “更抽象、更丰富”。 Netty中的channel专门指网络通信，不同于Java NIO中的Channel，后者还有指类似FileChannel本地文件的I/O通道。 Netty更加抽象，它不仅封装了多路复用I/O模型，还封装了Java BIO模型。 ChannelPipeline和ChannelHandlerNetty 中的每一个 Channel ，都有一个独立的 ChannelPipeline 中文名称为“通道水管”。只不过这个水管是双向的，里面流淌着数据，数据可以通过这个“水管”流入到服务器，也可以通过这个“水管”从服务器流出。(1) 责任链和适配器的应用(2) ChannellnboundHandler 类举例 HttpRequestDecoder ByteArrayDecoder DelimiterBasedFrameDecoder ProtobuIDecoder和ProtobufVarint32FrameDecoder等标准数据格式解析处理器 (3) ChannelOutboundHandler 类举例 HttpResponseEncoder ByteArrayEncoder ProtobutEncoder、ProtobufVarint32LengthFieldPrepender、MarshallingEncoder、JZlibEncoder等。 半包/粘包问题在TCP连接中，指令和指令之间没有间隔，接收方可能为了接收两条连贯的指令，一共做了三次的接收，而第二次接收会收到一部分的包1的部分内容和包2的部分内容。半包是指，接收方应用程序在接收信息时，没有收到一个完整的信息格式块。粘包是指，接收方应用程序在接收信息时，除了接收到发送方应用程序发送的某一个完整数据信息描述，还接收到了发送方应用程序发送的下一个数据信息的一部分。半包和粘包问题产生的根本原因是TCP本质上没有“数据块”的概念，而是一连串的数据流。在应用程序层面上、业务层面上，我们自行定义的“数据块”在TCP层面上并不被协议认可。这个问题只会发生在TCP协议进行连续发送数据时（TCP长连接），由于UDP都是有边界的数据报，所以UDP不会出现这个问题。而TCP短连接也不会出现这问题，因为发送完一个指令信息后连接就断开了，不会再发送第二个指令数据。半包/粘包是一个应用层问题。要解决半包/粘包问题，就是在应用程序层面建立协商一致的信息还原依据。常见的有两种方式： 消息定长即保证每一个完整的信息描述的长度都是一定的，这样无论 TCP/IP 协议如何进行分片，数据接收方都可以按照固定长度进行消息的还原。 增加分隔符在完整的一块数据结束后增加协商一致的分隔符（例如增加一个回车符）Netty提供了多种解码器的封装帮助解决半包/粘包问题。 FixedLengthFrameDecoder DelimiterBasedFrameDecoder LineBasedFrameDecoder 甚至针对不同的数据格式， Netty都提供了半包和粘包问题的现成解决方式。例如 ProtobuN arint32FrameDecoder 解码器，就是专门解决 Protobuf 数据格式在 TCP 长连接传输时的半包问题的。 Reference《高性能服务系统构建与实战》–银文杰/编著【NIO系列】——之Reactor模型]]></content>
  </entry>
  <entry>
    <title><![CDATA[图解Spring事务传播类型]]></title>
    <url>%2F2019%2F03%2F19%2FSpring-Transaction-Propagation%2F</url>
    <content type="text"><![CDATA[Spring事务传播机制Spring管理的事务是逻辑事务，而且物理事务和逻辑事务最大差别就在于事务传播行为，事务传播行为用于指定在多个事务方法间调用时，事务是如何在这些方法间传播的。 下面以a.save()里调用了b.save()举例，事务方法之间调用时事务如何传播。12345678910111213@Servicepublic class A &#123; @Autowired private B b; @Transactional(propagation=Propagation.XXX) public void save() &#123; // ... b.save(); // ... &#125;&#125; 12345678@Servicepublic class B &#123; @Transactional(propagation=Propagation.XXX) public void save() &#123; // ... &#125;&#125; 1. REQUIRED默认的spring事务传播级别，使用该级别的特点是，如果上下文中已经存在事务，那么就加入到事务中执行，如果当前上下文中不存在事务，则新建事务执行。如下图：由于两个方法属于同一个物理事务，如果发生回滚，则两者都回滚。 2. SUPPORTS顾名思义就是可以支持事务，如果b.save()在事务环境中运行，则以事务形式运行，否则以非事务运行。注：本文中所有的所谓“非事务”、“无事务”概念都是指方法整体不以一个事务单元来执行的意思，譬如，假设b.save()里有多条SQL操作，如果以“非事务”方式执行b.save()，就意味着，这里边每条SQL操作都是单独一个事务执行，而不是整个方法只起一个事务来执行多条SQL的这种方式。 3. MANDATORY必须在一个事务中运行，也就是说，b.save()只能在已有事务的方法中被调用，否则会抛异常。 4. REQUIRES_NEW总是会创建一个新事务（包括物理事务），该传播级别的特点是，每次都会新建一个事务，并且同时将上下文中的事务挂起，执行当前新建事务完成以后，上下文事务恢复再执行。如下图：两个方法之间既不属于同一个逻辑事务也不属于同一个物理事务。 5. NOT_SUPPORTED顾名思义不支持事务，当处于存在事务的上下文环境中运行时，b.save()会暂停当前已开启的事务，意味着a.save()的事务被挂起直至b.save()以非事务方法运行完毕后，a.save()的事务继续执行。 6. NEVER绝不能在事务环境中运行，如果a.save()里声明了使用事务，而b.save()的事务类型声明为never，那么只能以抛异常告终。 7. NESTED嵌套事务支持。该传播级别特征是，如果上下文中存在事务，则嵌套事务执行，如果不存在事务，则新建事务。Nested和RequiresNew的区别： RequiresNew每次都创建新的独立的物理事务，而Nested只有一个物理事务；Nested嵌套事务回滚或提交不会导致外部事务回滚或提交，但外部事务回滚将导致嵌套事务回滚，而RequiresNew由于都是全新的事务，所以之间是无关联的 Nested使用JDBC 3的保存点实现，即如果使用低版本驱动将导致不支持嵌套事务使用嵌套事务，必须确保具体事务管理器实现的nestedTransactionAllowed属性为true，否则不支持嵌套事务，如DataSourceTransactionManager默认支持，而HibernateTransactionManager默认不支持，需要我们来开启。]]></content>
      <tags>
        <tag>Spring</tag>
        <tag>Transaction</tag>
        <tag>Propagation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于页缓存技术提高中间件读写性能的应用]]></title>
    <url>%2F2019%2F03%2F17%2FAbout-OS-Page-Cache%2F</url>
    <content type="text"><![CDATA[页缓存在计算机系统中，页面缓存（有时也称为磁盘缓存）是用于源自诸如硬盘驱动器（HDD）或固态驱动器（SSD）的辅助存储设备的页面的透明缓存。操作系统将页面缓存保留在主存储器（RAM，内存）的其他未使用部分中，从而更快地访问缓存页面的内容和达到整体性能改进。页面缓存在具有分页内存管理的内核中实现，并且对应用程序大多是透明的。通常是由操作系统将所有未直接分配给应用程序的物理内存用于页面缓存。 以linux系统为例，linux中页缓存的本质就是对于磁盘中的部分数据在内存中保留一定的副本，使得应用程序能够快速的读取到磁盘中相应的数据，并实现不同进程之间的数据共享。 因此，linux中页缓存的引入主要是为了解决两类重要的问题： 1.磁盘读写速度较慢（ms 级别); 2.实现不同进程之间或者同一进程的前后不同部分之间对于数据的共享； 在虚拟内存机制出现以前，操作系统使用块缓存机制，但是在虚拟内存出现以后操作系统管理IO的粒度更大，因此采用了页缓存机制。此后，和后备存储的数据交互普遍以页为单位。页缓存是基于页的、面向文件的一种缓存机制。 简单来讲，页缓存就是一种副本技术，为加速磁盘读写而在内存中保留的一份磁盘数据的副本，又称为磁盘缓存、文件系统缓存、操作系统缓存等。 —- Page Cache(From Wikipedia) 页缓存在RocketMQ的应用刷盘策略作为一款纯 Java 语言开发的消息引擎，RocketMQ 自主研发的存储组件，依赖 Page Cache 进行加速和堆积，意味着它的性能会受到 JVM、 GC、内核、Linux 内存管理机制、文件 IO 等因素的影响。Rocketmq中的所有消息都是持久化到硬盘的，但会使用系统PageCache加速访问，消息的落地方式是先写PageCache后刷盘，可以保证内存与磁盘都有一份数据，访问时，可以直接从内存读取。如图所示，一条消息从客户端发送出，到最终落盘持久化。 —- RocketMQ相关总结 Kafka的高性能写入页缓存技术 + 磁盘顺序写同RocketMQ的持久化操作的设计类似，Kafka每次接收到数据都会往磁盘上去写，而为了保证数据写入性能，Kafka也是基于操作系统的页缓存来实现文件写入的。 在消息生产端写入消息时，其实是直接写入到OS Cache（Page Cache）中，也就是说仅仅是写入到内存中，而接下来由操作系统自己决定什么时候把os cache里的数据真的刷入磁盘文件中。 另外，还有关键的一点，就是kafka写数据的方式是顺序写入，也就是说，仅仅将数据追加到文件的末尾，而不是在文件的随机位置来修改数据。 —- Kafka如何实现每秒上百万的超高并发写入？ Elastic Search 准实时搜索Elasticsearch和磁盘之间有一层称为FileSystem Cache的系统缓存（OS Cache），正是由于这层cache的存在才使得es能够拥有更快搜索响应能力。 我们都知道一个index是由若干个segment组成，随着每个segment的不断增长，我们索引一条数据后可能要经过分钟级别的延迟才能被搜索，为什么有种这么大的延迟，这里面的瓶颈点主要在磁盘。 持久化一个segment需要fsync操作用来确保segment能够物理的被写入磁盘以真正的避免数据丢失，但是fsync操作比较耗时，所以它不能在每索引一条数据后就执行一次，如果那样索引和搜索的延迟都会非常之大。 所以这里需要一个更轻量级的处理方式，从而保证搜索的延迟更小。这就需要用到上面提到的FileSystem Cache，所以在es中新增的document会被收集到indexing buffer区后被重写成一个segment然后直接写入filesystem cache中，这个操作是非常轻量级的，相对耗时较少，之后经过一定的间隔或外部触发后才会被flush到磁盘上，这个操作非常耗时。但只要sengment文件被写入cache后，这个sengment就可以打开和查询，从而确保在短时间内就可以搜到，而不用执行一个full commit也就是fsync操作，这是一个非常轻量级的处理方式而且是可以高频次的被执行，而不会破坏es的性能。 —- Elastic Search的近实时搜索 框架的设计思想，本质大同由一个页缓存技术的利用可以看出，多个中间件在处理数据高性能读写问题方面的思想基本大同。]]></content>
      <tags>
        <tag>MiddleWare</tag>
        <tag>Cache</tag>
        <tag>Kafka</tag>
        <tag>Elastic Search</tag>
        <tag>RocketMQ</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis分布式锁实现去服务单点问题实践]]></title>
    <url>%2F2019%2F02%2F17%2FDistributed-Lock-HA%2F</url>
    <content type="text"><![CDATA[问题背景常见的单个应用服务需要完成某个功能模块，由于业务设计上的原因，该服务只允许一个而不能多个服务实例同时运行。但在高可用架构设计上，这存在单点问题，即一旦主机发生故障，如宕机或网络中断等，而导致服务终止，这种场景下我们希望能采用一种优雅的方式保证服务不中断。于是我们可以采用分布式锁来实现一主多备的高可用方案。 一主多备我们可以实现服务的一主多备模式，有且仅有一个master，至少一个standby，当master节点失败后则由多个standby中选取一个作为主节点继续提供服务。 分布式锁考虑到需要有多个服务节点，但同时只允许一个服务节点运行，我们可以实现分布式锁来解决问题。 分布式锁的几个要求 最基本要求：互斥性（唯一性），同一时间只能被一个机器节点上的一个线程获得锁。 避免死锁：可重入性。 高可用的锁获取及锁释放。 高性能的锁获取及锁释放。 阻塞性：最好是一把阻塞锁。 分布式锁的常见三种实现方式分布式锁的常见3种实现方式有基于数据库、缓存及zookeeper的实现，网上一抓有一大把的实现过程可以搜索，此处不详细说明，只简单说明一下各个实现的核心思路及优劣。 基于数据库的实现方式核心思想在于：利用数据库表的唯一索引，以方法名字段作为唯一键，想要执行方法时，使用方法名向表中插入数据，成功则表示获得锁，执行完成后再删除对应的行数据以释放锁。 优点思路简单，容易理解。 问题及优化 高可用及性能问题：基于数据库实现，数据库的可用性及性能直接影响分布式锁的可用性及性能。数据库要避免单点，需要双机部署、数据同步、主备切换等。 不可重入：因为同一个线程在释放锁之前，行数据一直存在，无法再次成功插入数据。所以，需要在表中新增一列，用于记录当前获取到锁的机器和线程信息，在再次获取锁的时候，先查询表中机器和线程信息是否和当前机器和线程相同，若相同则直接获取锁。 没有锁失效机制：因为有可能出现成功插入数据后，服务器宕机了，对应的数据没有被删除，导致锁一直被占用，不但其它节点无法获得锁，当服务恢复后也一直获取不到锁，所以，需要在表中新增一列，用于记录失效时间，并且需要有定时任务清除这些失效的数据。 非阻塞锁：获取不到锁直接返回失败，所以需要优化获取逻辑，循环多次去获取。 注意：这只是使用基于数据库的一种方法，使用数据库实现分布式锁还有很多其他的玩法！ 基于Zookeeper的实现方式ZooKeeper是一个为分布式应用提供一致性服务的开源组件，它内部是一个分层的文件系统目录树结构，规定同一个目录下只能有一个唯一文件名。基于ZooKeeper实现分布式锁的步骤如下： （1）创建一个目录作为锁目录；（2）线程A想获取锁就在锁目录下创建临时顺序节点；（3）获取锁目录下所有的子节点，然后获取比自己小的兄弟节点，如果不存在，则说明当前线程顺序号最小，获得锁；（4）线程B获取所有节点，判断自己不是最小节点，设置监听比自己次序小的节点；（5）线程A处理完，删除自己的节点，线程B监听到变更事件，判断自己是不是最小的节点，如果是则获得锁。 优点Zookeeper是一个分布式协同服务，使用Zookeeper实现分布式锁具有天然优势，最大的优点是API使用简单。 建议直接使用zookeeper第三方库Curator客户端，这个客户端中封装了一个可重入的锁服务。12345678910111213141516171819public boolean tryLock(long timeout, TimeUnit unit) throws InterruptedException &#123; try &#123; return interProcessMutex.acquire(timeout, unit); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return true;&#125;public boolean unlock() &#123; try &#123; interProcessMutex.release(); &#125; catch (Throwable e) &#123; log.error(e.getMessage(), e); &#125; finally &#123; executorService.schedule(new Cleaner(client, path), delayTimeForClean, TimeUnit.MILLISECONDS); &#125; return true;&#125; 其它优点： 解决单点问题：ZK是集群部署的，只要集群中有半数以上的机器存活，就可以对外提供服务。 解决不可重入问题：客户端在创建节点的时候，把当前客户端的主机信息和线程信息直接写入到节点中，下次想要获取锁的时候和当前最小的节点中的数据比对一下就可以了。如果和自己的信息一样，那么自己直接获取到锁，如果不一样就再创建一个临时的顺序节点，参与排队。 解决锁失效问题：使用ZK可以有效让锁自动失效而释放。在创建锁的时候，客户端会在ZK中创建一个临时节点，一旦客户端获取到锁之后突然挂掉（Session连接断开），那么这个临时节点就会自动删除掉。其他客户端就可以再次获得锁。 解决非阻塞性问题：Watch机制，客户端可以通过在ZK中创建顺序节点，并且在节点上绑定监听器，一旦节点有变化，Zookeeper会通知客户端，客户端可以检查自己创建的节点是不是当前所有节点中序号最小的，如果是，那么自己就获取到锁。 缺点 性能问题：性能上不如使用缓存实现分布式锁。因为每次在创建锁和释放锁的过程中，都要动态创建、销毁瞬时节点来实现锁功能。ZK中创建和删除节点只能通过Leader服务器来执行，然后将数据同不到所有的Follower机器上。 并发问题：在网络抖动情况下，客户端与ZK集群的session连接断了，那么zk以为客户端宕掉了，就会删除临时节点，这时候其他客户端就可以获取到分布式锁了。就可能产生并发问题。这个问题不常见是因为zk有重试机制，一旦zk集群检测不到客户端的心跳，就会重试，Curator客户端支持多种重试策略。多次重试之后还不行的话才会删除临时节点。（所以，选择一个合适的重试策略也比较重要，要在锁的粒度和并发之间找一个平衡。） 建议尽可能的使用Zookeeper来实现分布式协同服务，但如果，业务强依赖于一个Redis集群且服务并没有使用Zookeeper的意愿，不妨可以试试使用Redis。 基于缓存的实现方式各种成熟的缓存产品，包括Redis，memcached以及Tair，分布式锁实现思路基本类似，以Redis举例说明核心思路：（1）获取锁的时候，使用setnx加锁，并使用expire命令为锁添加一个超时时间，超过该时间则自动释放锁，锁的标识值为一个随机生成的UUID，通过此标识在释放锁的时候进行判断。（2）获取锁的时候还设置一个获取的超时时间，若超过这个时间则放弃获取锁。（3）释放锁的时候，通过UUID判断是不是该锁，若是该锁，则执行delete进行锁释放。 优点性能高是最大优点，基于内存实现的缓存系统。 解决单点问题：Redis既可单点部署，也支持集群部署。 解决不可重入问题：在一个线程获取到锁之后，把当前主机信息和线程信息保存起来，下次再获取之前先检查自己是不是当前锁的持有者。 解决锁失效问题：设定锁key失效时间。 非阻塞锁：可以循环多次去执行获取操作，直至获得锁。 至于失效时间设多久才好？Redisson给出了解决方案–Watchdog看门狗：先获得锁，默认过期时间30秒，如果处理完了，走正常逻辑。 对一个值加锁之后，会在自身维护一个Watchdog后台线程，维护一个内部队列，每过10秒去重新设置一下锁Key的过期时间，这样，一个锁即使对应的进程挂掉，也就维持30秒的时间，如果没有挂，并且30秒不够用了，内部队列会不断的更新这个过期时间为30秒，保证不会出现锁饥饿的问题。以下为重设Key过期时间的核心方法12345678910111213141516171819202122232425262728293031private void scheduleExpirationRenewal() &#123; if (expirationRenewalMap.containsKey(getEntryName())) &#123; return; &#125; Timeout task = commandExecutor.getConnectionManager().newTimeout(new TimerTask() &#123; @Override public void run(Timeout timeout) throws Exception &#123; Future&lt;Boolean&gt; future = expireAsync(internalLockLeaseTime, TimeUnit.MILLISECONDS); future.addListener(new FutureListener&lt;Boolean&gt;() &#123; @Override public void operationComplete(Future&lt;Boolean&gt; future) throws Exception &#123; expirationRenewalMap.remove(getEntryName()); if (!future.isSuccess()) &#123; log.error("Can't update lock " + getName() + " expiration", future.cause()); return; &#125; if (future.getNow()) &#123; // reschedule itself scheduleExpirationRenewal(); &#125; &#125; &#125;); &#125; &#125;, internalLockLeaseTime / 3, TimeUnit.MILLISECONDS); if (expirationRenewalMap.putIfAbsent(getEntryName(), task) != null) &#123; task.cancel(); &#125; &#125; 事实上，使用Redisson，还有一个好处，就是Redisson调Redis命令的底层实现，是使用Lua脚本，这样做，是因为假设有一大坨复杂的业务逻辑，可以通过封装在Lua脚本中发送给Redis，保证这段复杂的业务逻辑执行的原子性。 123456789101112131415161718192021Future&lt;Long&gt; tryLockInnerAsync(final long leaseTime, final TimeUnit unit, long threadId) &#123; internalLockLeaseTime = unit.toMillis(leaseTime); return commandExecutor.evalWriteAsync(getName(), LongCodec.INSTANCE, RedisCommands.EVAL_LONG, "local mode = redis.call('hget', KEYS[1], 'mode'); " + "if (mode == false) then " + "redis.call('hset', KEYS[1], 'mode', 'write'); " + "redis.call('hset', KEYS[1], ARGV[2], 1); " + "redis.call('pexpire', KEYS[1], ARGV[1]); " + "return nil; " + "end; " + "if (mode == 'write') then " + "if (redis.call('hexists', KEYS[1], ARGV[2]) == 1) then " + "redis.call('hincrby', KEYS[1], ARGV[2], 1); " + "redis.call('pexpire', KEYS[1], ARGV[1]); " + "return nil; " + "end; " + "end;" + "return redis.call('pttl', KEYS[1]);", Arrays.&lt;Object&gt;asList(getName()), internalLockLeaseTime, getLockName(threadId)); &#125; 缺点 通过超时时间来控制锁的失效时间并非十分的靠谱。譬如在指定的锁Key更新时间内网络抖动导致无法正常失效时间，被认为锁被主节点主动释放了，而实际上并没有，但备节点此时可能已抢得锁，出现多客户端获得锁的问题。 最大的问题，还是在于Redis的主从集群的复制问题。客户端1对Redis Master写入锁Key的Value，此时会异步复制给Redis Slave。一旦发生Master宕机，Redis主从切换，Slave变成了新的Master，而此时若有客户端2来尝试加锁，在新的Master上完成了加锁，而客户端1也以为自己成功加了锁，此时也出现了多客户端获得锁的问题。 使用Redis分布式锁去服务单点实践1234567891011121314151617181920212223242526272829303132333435363738private static final String REDIS_ADDRESS = "192.168.4.104:6379";private static final String REDIS_PASSWORD = "redis";private static final String LOCK_KEY = "redis_lock";public static void runWithRedisLock(String[] args) &#123; RedissonClient redissonClient = RedisUtils.getRedissonClientInstance( REDIS_ADDRESS, REDIS_PASSWORD); RedissonLock lock = (RedissonLock) redissonClient.getLock(LOCK_KEY); // 异步方式尝试Redis加锁操作 Future&lt;Boolean&gt; tryLockAsync = lock.tryLockAsync(1, TimeUnit.SECONDS); try &#123; // 一定要通过调get方法拿到执行加锁的结果，因为是异步方式加锁，调此方法会阻塞直至拿到执行结果 if (tryLockAsync.get()) &#123; // 尝试执行业务逻辑，如果发生异常，则释放锁 try &#123; Application.runApplication(args); &#125; catch (Exception e) &#123; ILOG.error("run application exception, ", e); if( lock.isHeldByCurrentThread() ) &#123; lock.unlock(); &#125; &#125; &#125; else &#123; ILOG.info("lock[&#123;&#125;] is being held", lock.getName()); Application.stopApplication(args); &#125; &#125; catch (InterruptedException e) &#123; ILOG.error("lock interrupted exception, ", e); &#125; catch (ExecutionException e) &#123; ILOG.error("lock executed exception, ", e); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738import org.redisson.Config;import org.redisson.Redisson;import org.redisson.RedissonClient;import org.redisson.SingleServerConfig;public class RedisUtils &#123; private static String address; private static String password; public static RedissonClient getRedissonClientInstance(String redisAddress, String redisPassword) &#123; address = redisAddress; password = redisPassword; return RedissonClientHolder.instance; &#125; private static class RedissonClientHolder &#123; private static RedissonClient instance = createRedissonClient(address, password); &#125; private static RedissonClient createRedissonClient(String address, String password) &#123; try &#123; Config config = new Config(); SingleServerConfig singleSerververConfig = config.useSingleServer(); singleSerververConfig.setAddress(address).setPassword(password) .setConnectionMinimumIdleSize(1).setConnectionPoolSize(64); RedissonClient redissonClient = Redisson.create(config); return redissonClient; &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; &#125;&#125; 降级处理的实践如上所述，如果Redis（或Redis集群）服务宕掉了，或者当网络抖动或Redis集群主从切换导致的各种异常导致Redis方式不可行，于是，需要做降级处理，所谓降级，就是异常情况下的备胎处理方式，这里我们先假设最容易处理的一种方式，如果系统中只有一主一从两个节点，那么我们可以简单处理，让双方错开一个定时间隔，分别使用netCat命令向对方进行端口检测，如果检测到对方节点中有正在运行的服务，则不启动甚至退出自己当前的服务。如果对方节点不在运行服务，则说明自己可以“获得锁”，便可运行服务，取代对方成为主节点。 123456789101112131415161718public static void main(final String[] args) throws InterruptedException &#123; // 设置定时器，定时检测运行 Timer timer = new Timer(); timer.schedule(new TimerTask() &#123; @Override public void run() &#123; try &#123; runWithRedisLock(args); &#125; catch (Exception e) &#123; runWithNetCat(args); &#125; &#125; &#125;, 0, 10000);&#125; 12345678910111213141516171819202122232425262728293031323334private static volatile boolean isRunning = false;// 假设两台机器服务地址+端口分别为[对方："192.168.11.20:8080", 本机："192.168.11.21:8080"]// 对方的服务地址private static final String ADDRESS = "192.168.11.20";// 对方的服务端口private static final String PORT = "8080";public static void runWithNetCat(String[] args) &#123; if (isRunning) &#123; ILOG.info("application is already running"); return; &#125; boolean hasRunner = false; if (netCat(ADDRESS, PORT)) &#123; hasRunner = true; Application.stopApplication(args); &#125; if (!hasRunner) &#123; Application.runApplication(args); &#125;&#125;private static boolean netCat(String host, int port) &#123; try &#123; NetUtil.netCat(host, port, new byte[] &#123;&#125;); return true; &#125; catch (Exception e) &#123; ILOG.warn("net check error. ", e); return false; &#125;&#125; 使用服务端口检测方法的优缺点这样的做法非常容易理解且操作简单，但也存在问题，譬如当两者不能互相ping通对方时，则两者可能都以为自己可以升级为主，造成脑裂现象。 降级处理的更优实践由上分析，我们得知依赖网络去简单做服务端口检测会存在不可靠，并且条件也限定在系统中只有一主一备两节点，如果存在多台节点服务，即一主多备时，我们还是要保证顺序一致性。基于这一点考虑，我们若使用数据库的实现方式，便可以满足实现上更简单、更可靠的降级要求。 基于Spring Boot的启动优化方案Spring Boot提供了 CommandLineRunner 接口，实现了 CommandLineRunner 接口的 Component 会在所有 Spring Beans 都初始化之后，SpringApplication.run() 之前执行，我们可以在这个方法里hold住服务的启动，在这里边作加锁成功的判断，从而实现一种只加载容器但不启动服务的“预加载”方案，这么做是因为，我们知道，spring应用启动的过程中，容器加载是较为耗时的，这种“待机式”的方案，可以让应用先准备好，当需要启动服务时便可更快速地进入服务状态。而除 CommandLineRunner 之外，使用ApplicationRunner也可以达到相同的目的，两者差别不大。 ReferenceZookeeper分布式锁原理图：七张图彻底讲清楚ZooKeeper分布式锁的实现原理【石杉的架构笔记】]]></content>
      <tags>
        <tag>Distributed</tag>
        <tag>HA</tag>
        <tag>Redis</tag>
        <tag>Redisson</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[几张图总结几种I/O类型]]></title>
    <url>%2F2018%2F11%2F20%2FLinux-IO-Types%2F</url>
    <content type="text"><![CDATA[Unix/Linux支持的5种I/O类型： 同步模型（synchronous IO） 阻塞IO（bloking IO） 非阻塞IO（non-blocking IO） 多路复用IO（multiplexing IO） 信号驱动式IO（signal-driven IO） 异步IO（asynchronous IO） 过程图解阻塞 非阻塞 多路复用 信号驱动 异步 怎么理解I/O过程分两个阶段： 数据从硬件（网卡、硬盘）拷贝到内核的内存空间。 数据从内核的内存空间拷贝到用户态的内存空间为什么要分两个阶段？（撇开零拷贝不说）前提是用户态程序（应用程序）不能直接跟硬件打交道，而能与硬件打交道的就只能是内核，用户态程序要想从硬件获得数据，必须通知内核我要获得硬件中的数据，此过程称为系统调用，为第一阶段。于是，自然就有了第二阶段，数据从内核内存空间到用户态内存空间。（对用户态来讲，此过程才是I/O发生的地方） 概括每种I/O类型的过程OK，有了两阶段的概念后，可以简单来理解并总结每种I/O类型的过程: 一图胜千言： 浅谈零拷贝既然提到零拷贝，那就粗浅地说明一下：零拷贝是指，减免了数据在内核空间和用户空间来回拷贝（无cpu copy过程）。用户态程序调用mmap()，磁盘上的数据会通过DMA（直接内存存取，direct memory access，一种透过DMA控制器让内存与硬盘/网卡直接对接的IO技术，过程中无需依赖CPU的大量中断负载）被拷贝到内核缓冲区，接着操作系统会把这段内核缓冲区与应用程序共享，这种所谓的共享，方式就是mmap（内存映射，memory map），这样就不需要把内核缓冲区的内容往用户空间拷贝。用户态程序再调用write()，操作系统直接将内核缓冲区的内容拷贝到socket缓冲区中，这一切都发生在内核态，最后，socket缓冲区再把数据发到网卡去。好吧，一图胜千言： OK，上面所说的DMA技术直接让内存对接硬盘IO，以及mmap让用户程序共享内核缓冲区，都可以省掉内核空间与用户空间之间的cpu copy，但是还有个问题，就是内核与socket缓冲区之间是否也可以免掉一次CPU copy呢？借助于硬件上的帮助，我们是可以办到的。之前我们是把页缓存的数据拷贝到socket缓存中，实际上，我们仅仅需要把缓冲区描述符传到socket缓冲区，再把数据长度传过去，这样DMA控制器直接将页缓存中的数据打包发送到网络（网卡）中去就可以了。这样，就是可以实现真正的全程零CPU拷贝了。 Reference《UNIX网络编程》 浅析Linux中的零拷贝技术]]></content>
      <tags>
        <tag>I/O</tag>
        <tag>UNIX</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RocketMQ相关总结]]></title>
    <url>%2F2018%2F11%2F18%2FRocketMQ-Summary%2F</url>
    <content type="text"><![CDATA[总结RocketMQ相关知识点，便于回顾记忆… RocketMQ相关网址官网：https://rocketmq.incubator.apache.org/ 源码：https://github.com/apache/rocketmq RocketMQ总体特点 能够保证严格的消息顺序 提供丰富的消息拉取模式 高效的订阅者水平扩展能力 实时的消息订阅机制 亿级消息堆积能力 核心原理数据结构主要以commitLog为消息存储的数据结构。 （1）所有数据单独储存到commit Log ，完全顺序写，随机读 （2）对最终用户展现的队列实际只储存消息在Commit Log 的位置信息，并且串行方式刷盘 （3）按照MessageId查询消息 （4）根据查询的key的hashcode%slotNum得到具体的槽位置 （5）根据slotValue（slot对应位置的值）查找到索引项列表的最后一项 （6）遍历索引项列表返回查询时间范围内的结果集 刷盘策略作为一款纯 Java 语言开发的消息引擎，RocketMQ 自主研发的存储组件，依赖 Page Cache 进行加速和堆积，意味着它的性能会受到 JVM、 GC、内核、Linux 内存管理机制、文件 IO 等因素的影响。Rocketmq中的所有消息都是持久化到硬盘的，但会使用系统PageCache加速访问，消息的落地方式是先写PageCache后刷盘，可以保证内存与磁盘都有一份数据，访问时，可以直接从内存读取。如图所示，一条消息从客户端发送出，到最终落盘持久化，每个环节都有产生延迟的风险。 《不一样的技术创新-阿里巴巴2016双十一背后的技术》一书中提到，有线上数据显示，RocketMQ 写消息链路存在偶发的高达数秒的延迟 同步刷盘同步刷盘是指，broker在收到每个消息后，都是先要保存到硬盘上，然后再给producer确认。 异步刷盘异步刷盘就是先回复确认，然后批量保存到硬盘上。异步刷盘有更好的性能，当然也有更大的丢失消息的风险。 角色关系图 架构图 架构特点所有的集群都具有水平扩展能力，无单点障碍。 NameServer以轻量级的方式提供服务发现和路由功能，每个NameServer存有全量的路由信息，提供对等的读写服务，是一个几乎无状态节点，可集群部署，节点之间无任何信息同步，支持快速扩缩容。 Broker为实际的消息队列服务器(MQ Server)，在整体架构中，可以看作是Producer与Comsumer之间的驳脚者，消息通过它从Producer接收，并存储，后转发给Consumer。以Topic为纬度支持轻量级的队列，单机可以支撑上万队列规模，支持消息推拉模型，具备多副本容错机制（2副本或3副本）、强大的削峰填谷以及上亿级消息堆积能力，同时可严格保证消息的有序性。 除此之外，Broker还提供了同城异地容灾能力，丰富的Metrics统计以及告警机制。这些都是传统消息系统无法比拟的。 Producer由用户进行分布式部署，消息由Producer通过多种负载均衡模式发送到Broker集群，发送低延时，支持快速失败。 Consumer也由用户部署，支持PUSH和PULL两种消费模式（推模式的实现也是用的拉方式），支持集群消费和广播消息，提供实时的消息订阅机制，满足大多数消费场景。 RocketMQ亮点-支持多种消费模式RocketMQ最初还未正式称为RocketMQ，一开始v1.0还是叫metaQ，经历了3代的重要演进，v3.0开始改名RocketMQ，其重要改进包括消息获取模式。 第一代，推模式，数据存储采用关系型数据库。在这种模式下，消息具有很低的延迟特性，并且很容易支持分布式事务。尤其在阿里淘宝这种高频交易场景中，具有非常广泛地应用。典型代表包括Notify、Napoli。 第二代，拉模式，自研的专有消息存储。在日志处理方面能够媲美Kafka的吞吐性能，但考虑到淘宝的应用场景，尤其是其交易链路的高可靠需求，消息引擎并没有一味的追求吞吐，而是将稳定可靠放在首位。因为采用了长连接拉模式，在消息的实时方面丝毫不逊推模式。典型代表MetaQ。 第三代，以拉模式为主，兼有推模式的高性能、低延迟消息引擎RocketMQ，在二代功能特性的基础上，为电商金融领域添加了可靠重试、基于文件存储的分布式事务等特性，并做了大量优化。从2012年开始，经历了历次双11核心交易链路检验。目前已经捐赠给Apache基金会。 不难看出，RocketMQ其实是伴随着阿里巴巴整个生态的成长，逐渐衍生出来的高性能，高可用，兼具高吞吐量和低延迟、能够同时满足电商领域和金融领域的极尽苛刻场景的消息中间件。 Broker部署方式单Master 这种方式风险较大，一旦Broker 重启或者宕机时，会导致整个服务不可用，不建议线上环境使用。 多Master模式多台Broker，全是Master 优点：配置简单，单个Master 宕机或重启维护对应用无影响，在磁盘配置为 RAID10 时，即使机器宕机不可恢复情况下，由于RAID10 磁盘非常可靠，消息也不会丢（异步刷盘丢失少量消息，同步刷盘一条不丢）。性能最高。缺点：单台机器宕机期间，这台机器上未被消费的消息在机器恢复之前不可订阅，消息实时性会受到受到影响。 HA方案：多Master/Slave对模式每个 Master 配对一个 Slave，有多对Master-Slave。 Master/Slave复制方式 同步双写写入消息时，master先写入，之后复制到slave，确认slave也存储了消息后才向producer答复返回成功。 优点：数据与服务都无单点，Master宕机情况下，消息无延迟，服务可用性与数据可用性都非常高缺点：性能比异步复制模式略低，大约低10%左右，发送单个消息的 RT 会略高。目前主宕机后，备机不能自动切换为主机，后续会支持自动切换功能。 异步复制先答复producer，再去向salve复制。 优点：即使磁盘损坏，消息丢失的非常少，且消息实时性不会受影响，因为 Master 宕机后，消费者仍然可以从 Slave 消费，此过程对应用透明。不需要人工干预。性能同多 Master 模式几乎一样。缺点：Master宕机，磁盘损坏情况，会丢失少量消息。 通过同步复制技术可以完全避免单点，同步复制势必会影响性能，适合应用于消息可靠性要求极高的场合。RocketMQ从3.0版本开始支持同步双写。 两种消息消费的交互方式的区别留意源码可以得知：consumer被分为2类：MQPullConsumer和MQPushConsumer，本质都是拉模式（pull），即consumer轮询从broker拉取消息。区别在于： push方式里，consumer把轮询过程封装了，并注册MessageListener监听器，取到消息后，唤醒MessageListener的consumeMessage()来消费，对客户端而言，感觉消息是被推送（push）过来的。 pull方式里，取消息的过程，RocketMQ交给了用户自己实现，首先通过待消费的Topic拿到MessageQueue的集合，遍历MessageQueue集合，然后针对每个MessageQueue批量取消息，一次取完后，记录该队列下一次要取的开始offset，直到取完了，再换另一个MessageQueue。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879public class Consumer &#123; // Java缓存 private static final Map&lt;MessageQueue, Long&gt; offseTable = new HashMap&lt;MessageQueue, Long&gt;(); private static final String nameServAndAddr = "172.16.235.77:9876;172.16.235.78:9876"; private static final String consumerGroupName ="ConsumerGroupName"; private static final String consumber ="Consumber"; /** * 主动拉取方式消费 * * @throws MQClientException */ public static void main(String[] args) throws MQClientException &#123; /** * 一个应用创建一个Consumer，由应用来维护此对象，可以设置为全局对象或者单例&lt;br&gt; * 注意：ConsumerGroupName需要由应用来保证唯一 ,最好使用服务的包名区分同一服务,一类Consumer集合的名称， * 这类Consumer通常消费一类消息，且消费逻辑一致 * PullConsumer：Consumer的一种，应用通常主动调用Consumer的拉取消息方法从Broker拉消息，主动权由应用控制 */ DefaultMQPullConsumer consumer = new DefaultMQPullConsumer(consumerGroupName); // //nameserver服务 consumer.setNamesrvAddr(nameServAndAddr); consumer.setInstanceName(consumber); consumer.start(); // 拉取订阅主题的队列，默认队列大小是4 Set&lt;MessageQueue&gt; mqs = consumer.fetchSubscribeMessageQueues("TopicTest1"); for (MessageQueue mq : mqs) &#123; System.out.println("Consume from the queue: " + mq); SINGLE_MQ: while (true) &#123; try &#123; PullResult pullResult = consumer.pullBlockIfNotFound( mq, null, getMessageQueueOffset(mq), 32); List&lt;MessageExt&gt; list = pullResult.getMsgFoundList(); if (list != null &amp;&amp; list.size() &lt; 100) &#123; for (MessageExt msg : list) &#123; System.out.println(new String(msg.getBody())); &#125; &#125; System.out.println(pullResult.getNextBeginOffset()); putMessageQueueOffset(mq, pullResult.getNextBeginOffset()); switch (pullResult.getPullStatus()) &#123; case FOUND: break; case NO_MATCHED_MSG: break; case NO_NEW_MSG: break SINGLE_MQ; case OFFSET_ILLEGAL: break; default: break; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; consumer.shutdown(); &#125; private static void putMessageQueueOffset(MessageQueue mq, long offset) &#123; offseTable.put(mq, offset); &#125; private static long getMessageQueueOffset(MessageQueue mq) &#123; Long offset = offseTable.get(mq); if (offset != null) &#123; System.out.println(offset); return offset; &#125; return 0; &#125; &#125; RocketMQ使用长轮询Pull方式，可保证消息非常实时，消息实时性不低于Push 长轮询Pull：建立长连接，每隔一定时间，客户端向服务端发起请求询问数据，如有则返回数据，如无则返回空，然后关闭请求。长轮询与普通轮询的不同之处在于，哪怕服务端此时没有数据，连接还是保持的，等到有数据时可以立即返回（也就模拟push），或者超时返回。长轮询好处在于可以减少无效请求，保证消息实时性获取，又不会造成积压。 推拉模式的具体选取视乎实际情况而定，在一些离线大批量数据处理系统中，消息获取的需求强调的更多是吞吐量，而非低延迟，此时拉模式可能更优。 RocketMQ 高可用保障通过可用性计算公式可以看出，要提升系统的可用性，需要在保障系统健壮性以延长平均无故障时间的基础上，进一步加强系统的故障自动恢复能力以缩短平均故障修复时间。 RocketMQ 高可用架构设计并实现了 Controller 组件，按照单主状态、异步复制状态、半同步状态以及最终的同步复制状态的有限状态机进行转换。在最终的同步复制状态下，Master 和 Slave 任一节点故障时，其它节点能够在秒级时间内切换到单主状态继续提供服务。相比于之前人工介入重启来恢复服务，RokcetMQ 高可用架构赋予了系统故障自动恢复的能力，能极大缩短平均故障恢复时间，提升系统的可用性。 下图描述了 RocketMQ 高可用架构中有限状态机的转换： 1） 第一个节点启动后，Controller 控制状态机切换为单主状态，通知启动节点以 Master 角色提供服务。2） 第二个节点启动后， Controller 控制状态机切换成异步复制状态。Master 通过异步方式向 Slave 复制数据。3） 当 Slave 的数据即将赶上 Master，Controller 控制状态机切换成半同步状态，此时命中 Master 的写请求会被 Hold 住，直到 Master以异步方式向 Slave 复制了所有差异的数据。4） 当半同步状态下 Slave 的数据完全赶上 Master 时，Controller控制状态机切换成同步复制模式，Mater 开始以同步方式向 Slave 复制数据。该状态下任一节点出现故障，其它节点能够在秒级内切换到单主状态继续提供服务。Controller 组件控制 RocketMQ 按照单主状态，异步复制状态，半同步状态，同步复制状态的顺序进行状态机切换。中间状态的停留时间与主备之间的数据差异以及网络带宽有关，但最终都会稳定在同步复制状态下。 如何保证消息有序消费？消息有序指的是一类消息消费时，能按照发送的顺序来消费。例如：一个订单产生了 3 条消息，分别是订单创建、订单付款、订单完成。消费时，要按照这个顺序消费才有意义。但同时订单之间又是可以并行消费的。 假如生产者产生了2条消息：M1、M2，要保证这两条消息的顺序，应该怎样做？你脑中想到的可能是这样： M1发送到S1后，M2发送到S2，如果要保证M1先于M2被消费，那么需要M1到达消费端后，通知S2，然后S2再将M2发送到消费端。 这个模型存在的问题是，如果M1和M2分别发送到两台Server上，就不能保证M1先达到，也就不能保证M1被先消费，那么就需要在MQ Server集群维护消息的顺序。那么如何解决？一种简单的方式就是将M1、M2发送到同一个Server上： 这样可以保证M1先于M2到达MQServer（客户端等待M1成功后再发送M2），根据先达到先被消费的原则，M1会先于M2被消费，这样就保证了消息的顺序。 这个模型，理论上可以保证消息的顺序，但在实际运用中你应该会遇到下面的问题： 只要将消息从一台服务器发往另一台服务器，就会存在网络延迟问题。如上图所示，如果发送M1耗时大于发送M2的耗时，那么M2就先被消费，仍然不能保证消息的顺序。即使M1和M2同时到达消费端，由于不清楚消费端1和消费端2的负载情况，仍然有可能出现M2先于M1被消费。如何解决这个问题？将M1和M2发往同一个消费者即可，且发送M1后，需要消费端响应成功后才能发送M2。 但又会引入另外一个问题，如果发送M1后，消费端1没有响应，那是继续发送M2呢，还是重新发送M1？一般为了保证消息一定被消费，肯定会选择重发M1到另外一个消费端2，就如下图所示。 这样的模型就严格保证消息的顺序，细心的你仍然会发现问题，消费端1没有响应Server时有两种情况，一种是M1确实没有到达，另外一种情况是消费端1已经响应，但是Server端没有收到。如果是第二种情况，重发M1，就会造成M1被重复消费。也就是我们后面要说的第二个问题，消息重复问题。 回过头来看消息顺序问题，严格的顺序消息非常容易理解，而且处理问题也比较容易，要实现严格的顺序消息，简单且可行的办法就是： 保证生产者 - MQServer - 消费者是一对一对一的关系 但是这样设计，并行度就成为了消息系统的瓶颈（吞吐量不够），也会导致更多的异常处理，比如：只要消费端出现问题，就会导致整个处理流程阻塞，我们不得不花费更多的精力来解决阻塞的问题。 但我们的最终目标是要集群的高容错性和高吞吐量。这似乎是一对不可调和的矛盾，那么阿里是如何解决的？ 有些问题，看起来很重要，但实际上我们可以通过合理的设计或者将问题分解来规避。如果硬要把时间花在解决它们身上，实际上是浪费的，效率低下的。从这个角度来看消息的顺序问题，我们可以得出两个结论： 不关注乱序的应用实际大量存在 队列无序并不意味着消息无序 最后我们从源码角度分析RocketMQ怎么实现发送顺序消息。 一般消息是通过轮询所有队列来发送的（负载均衡策略），顺序消息可以根据业务，比如说订单号相同的消息发送到同一个队列。下面的示例中，OrderId相同的消息，会发送到同一个队列： 123456789// RocketMQ默认提供了两种MessageQueueSelector实现：随机/HashSendResult sendResult = producer.send(msg, new MessageQueueSelector() &#123; @Override public MessageQueue select(List&lt;MessageQueue&gt; mqs, Message msg, Object arg) &#123; Integer id = (Integer) arg; int index = id % mqs.size(); return mqs.get(index); &#125;&#125;, orderId); 在获取到路由信息以后，会根据MessageQueueSelector实现的算法来选择一个队列，同一个OrderId获取到的队列是同一个队列。 12345678910111213private SendResult send() &#123; // 获取topic路由信息 TopicPublishInfo topicPublishInfo = this.tryToFindTopicPublishInfo(msg.getTopic()); if (topicPublishInfo != null &amp;&amp; topicPublishInfo.ok()) &#123; MessageQueue mq = null; // 根据我们的算法，选择一个发送队列 // 这里的arg = orderId mq = selector.select(topicPublishInfo.getMessageQueueList(), msg, arg); if (mq != null) &#123; return this.sendKernelImpl(msg, mq, communicationMode, sendCallback, timeout); &#125; &#125;&#125; 消息重复–如何保证幂等性上面在解决消息顺序问题时，引入了一个新的问题，就是消息重复。那么RocketMQ是怎样解决消息重复的问题呢？还是“恰好”不解决。 造成消息的重复的根本原因是：网络不可达。只要通过网络交换数据，就无法避免这个问题。所以解决这个问题的办法就是不解决，转而绕过这个问题。那么问题就变成了：如果消费端收到两条一样的消息，应该怎样处理？ 消费端处理消息的业务逻辑保持幂等性 保证每条消息都有唯一编号且保证消息处理成功与去重表的日志同时出现 第1条很好理解，只要保持幂等性，不管来多少条重复消息，最后处理的结果都一样。第2条原理就是利用一张日志表来记录已经处理成功的消息的ID，如果新到的消息ID已经在日志表中，那么就不再处理这条消息。 我们可以看到第1条的解决方式，很明显应该在消费端实现，不属于消息系统要实现的功能。第2条可以消息系统实现，也可以业务端实现。正常情况下出现重复消息的概率不一定大，且由消息系统实现的话，肯定会对消息系统的吞吐量和高可用有影响，所以最好还是由业务端自己处理消息重复的问题，这也是RocketMQ不解决消息重复的问题的原因。 RocketMQ不保证消息不重复，如果你的业务需要保证严格的不重复消息，需要你自己在业务端去重。 事务消息RocketMQ除了支持普通消息，顺序消息，另外还支持事务消息。 旧版事务消息：参考 RocketMQ总结整理-事务消息 RocketMQ 4.3 新版事务消息： 这张图说明了事务消息的大致方案，分为两个逻辑：正常事务消息的发送及提交、事务消息的补偿流程。 事务消息发送及提交： 发送消息（half消息） 服务端响应消息写入结果 根据发送结果执行本地事务（如果写入失败，此时half消息对业务不可见，本地逻辑不执行） 根据本地事务状态执行Commit或者Rollback（Commit操作生成消息索引，消息对消费者可见） 补偿流程： 对没有Commit/Rollback的事务消息（pending状态的消息），从服务端发起一次“回查” Producer收到回查消息，检查回查消息对应的本地事务的状态 根据本地事务状态，重新Commit或者Rollback 补偿阶段用于解决消息Commit或者Rollback发生超时或者失败的情况。 新版事务消息的设计原理RocketMQ事务消息的提交方式是2PC，一阶段消息可以理解为Prepared Message或者Pending Message，实际上就是说，消息要先提交并落地到Broker，但不能是对用户可见的。如何做到写入了消息但是对用户不可见?——写入消息数据，但是不创建对应的消息的索引信息。RocketMQ消息在服务端的存储结构如上，每条消息都会有对应的索引信息，Consumer通过索引读取消息。那么实现一阶段写入的消息不被用户消费（需要在Commit后才能消费），只需要写入Storage Queue，但是不构建Index Queue即可。 RocketMQ中具体实现策略是：写入的如果是事务消息，则对消息的Topic和Queue等属性进行替换，同时将原来的Topic和Queue信息存储到消息的属性中。代码实现如下：12345678910111213141516public PutMessageResult putHalfMessage(MessageExtBrokerInner messageInner) &#123; return store.putMessage(parseHalfMessageInner(messageInner)); &#125; private MessageExtBrokerInner parseHalfMessageInner(MessageExtBrokerInner msgInner) &#123; MessageAccessor.putProperty(msgInner, MessageConst.PROPERTY_REAL_TOPIC, msgInner.getTopic()); MessageAccessor.putProperty(msgInner, MessageConst.PROPERTY_REAL_QUEUE_ID, String.valueOf(msgInner.getQueueId())); msgInner.setSysFlag( MessageSysFlag.resetTransactionValue(msgInner.getSysFlag(), MessageSysFlag.TRANSACTION_NOT_TYPE)); msgInner.setTopic(TransactionalMessageUtil.buildHalfTopic()); msgInner.setQueueId(0); msgInner.setPropertiesString(MessageDecoder.messageProperties2String(msgInner.getProperties())); return msgInner; &#125; 替换属性后这条消息被写入到TransactionalMessageUtil.buildHalfTopic()的Queue 0中：12345678910111213public class TransactionalMessageUtil &#123; public static final String REMOVETAG = "d"; public static Charset charset = Charset.forName("utf-8"); ... public static String buildHalfTopic() &#123; return MixAll.RMQ_SYS_TRANS_HALF_TOPIC; &#125; ...&#125; 在完成Storage Queue的写入后，在appendCallback中，普通消息会去构建消息索引，而如果发现是事务消息，则跳过了创建索引的逻辑。 RocketMQ将事务消息一阶段发送的消息称为Half消息，我们可以理解为，这条消息相对普通消息的操作只做了一半（只落地而未索引），不算是一条完整的普通消息 在完成一阶段写入一条对用户不可见的消息后，二阶段如果是Commit操作，则需要让消息对用户可见；如果是Rollback则需要撤销一阶段的消息。 先说Rollback的情况。对于Rollback，本身一阶段的消息对用户是不可见的，其实不需要真正撤销消息（实际上RocketMQ也无法去真正的删除一条消息，因为是顺序写文件的）。 但是区别于这条消息没有确定状态（Pending状态，事务悬而未决），需要一个操作来标识这条消息的最终状态。 RocketMQ事务消息方案中引入了Op消息的概念，用Op消息标识事务消息是否状态已经确定（Commit或者Rollback）。如果一条事务消息没有对应的Op消息，说明这个事务的状态还无法确定（可能是二阶段失败了）。 引入Op消息后，事务消息无论是Commit或者Rollback都会记录一个Op操作。 Commit相对于Rollback只是在写入Op消息前创建Half消息的索引。 Half消息的索引构建 在执行二阶段的Commit操作时，需要构建出Half消息的索引。 一阶段的Half消息由于是写到一个特殊的Topic，所以二阶段构建索引时需要读取出Half消息，并将Topic和Queue替换成真正的目标的Topic和Queue，之后通过一次普通消息的写入操作来生成一条对用户可见的消息。 所以RocketMQ事务消息二阶段其实是利用了一阶段存储的消息的内容，在二阶段时恢复出一条完整的普通消息，然后走一遍消息写入流程。 如何处理二阶段失败的消息 如果二阶段失败了，比如在Commit操作时出现网络问题导致Commit失败，那么需要通过一定的策略使这条消息最终被Commit。 RocketMQ采用了一种补偿机制，称为“回查”。 Broker端对未确定状态的消息发起回查，将消息发送到对应的Producer端（同一个Group的Producer），由Producer根据消息来检查本地事务的状态，进而执行Commit或者Rollback。 Broker端通过对比Half消息和Op消息进行事务消息的回查并且推进CheckPoint（记录那些事务消息的状态是确定的）。 值得注意的一点是具体实现中，在回查前，系统会执行putBackHalfMsgQueue操作，即将Half消息重新写一遍到Half消息的Queue中。这么做其实是为了能有效的推进上面的CheckPoint。 新版事务消息设计总结 通过写Half消息的方式来实现一阶段消息对用户不可见 通过Op消息来标记事务消息的状态 通过读取Half消息来生成一条新的Normal消息来完成二阶段Commit之后消息对Consumer可见 通过Op消息来执行回查 Reference 分布式消息引擎 《不一样的技术创新-阿里巴巴2016双十一背后的技术》 RocketMq知识点理解 RocketMQ总结整理 RocketMQ事务消息实现分析]]></content>
      <tags>
        <tag>消息队列</tag>
        <tag>分布式</tag>
        <tag>异步</tag>
        <tag>削峰</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[总结ThreadLocal]]></title>
    <url>%2F2018%2F11%2F05%2FThreadLocal-Summary%2F</url>
    <content type="text"><![CDATA[ThreadLoacal简介ThreadLocal类是修饰变量的，重点是在控制变量的作用域，初衷可不是为了解决线程并发和线程冲突的，而是为了让变量的种类变的更多更丰富，方便人们使用罢了。根据变量的作用域，可以将变量分为全局变量，局部变量。简单的说，类里面定义的变量是全局变量，函数里面定义的变量是局部变量。还有一种作用域是线程作用域，线程一般是跨越几个函数的。为了在几个函数之间共用一个变量，所以才出现：线程变量，这种变量在Java中就是ThreadLocal变量。ThreadLocal变量，不同于它们的普通对应物，因为访问某个变量（通过其get 或 set 方法）的每个线程都有自己的局部变量，它独立于变量的初始化副本。ThreadLocal为每个使用该变量的线程分配一个独立的变量副本。所以每一个线程都可以独立地改变自己的副本，而不会影响其他线程所对应的副本。所以我们说，ThreadLocal为解决多线程程序的并发问题提供了一种新的思路。 ThreadLocal的用处Web开发中常见到的一个问题：多用户session问题。假设有多个用户需要获取用户信息，一个线程对应一个用户。在mybatis中，session用于操作数据库，那么设置、获取操作分别是session.set()、session.get()，如何保证每个线程都能正确操作达到想要的结果呢？ 假如我们要设置一个变量，作为各个线程共享的变量，来存储session信息，那么当我们需要让每个线程独立地设置session信息而不被其它线程打扰，要怎么做呢？很容易想到了加锁，譬如synchronized，互斥同步锁synchronized自JDK1.5经过优化后，不会很消耗资源了，但当成千上万个操作来临之时，扛高并发能力不说，数据返回延迟带来的用户体验变差又如何解决？ 那么，就上文提出的问题，引申出来，像mybatis，hibernate一类的框架是如何解决这个session问题的呢？ 来看一下，mybatis的SqlSessionManager类：123456789101112131415161718192021222324252627282930313233343536373839404142public class SqlSessionManager implements SqlSessionFactory, SqlSession &#123; private final SqlSessionFactory sqlSessionFactory; private final SqlSession sqlSessionProxy; private ThreadLocal&lt;SqlSession&gt; localSqlSession = new ThreadLocal&lt;SqlSession&gt;(); private SqlSessionManager(SqlSessionFactory sqlSessionFactory) &#123; this.sqlSessionFactory = sqlSessionFactory; this.sqlSessionProxy = (SqlSession) Proxy.newProxyInstance( SqlSessionFactory.class.getClassLoader(), new Class[]&#123;SqlSession.class&#125;, new SqlSessionInterceptor()); &#125; ... public void startManagedSession() &#123; this.localSqlSession.set(openSession()); &#125; public void startManagedSession(boolean autoCommit) &#123; this.localSqlSession.set(openSession(autoCommit)); &#125; public void startManagedSession(Connection connection) &#123; this.localSqlSession.set(openSession(connection)); &#125; ... @Override public Connection getConnection() &#123; final SqlSession sqlSession = localSqlSession.get(); if (sqlSession == null) &#123; throw new SqlSessionException("Error: Cannot get connection. No managed session is started."); &#125; return sqlSession.getConnection(); &#125; ...&#125; 留意到，mybatis里的localSqlSession就是用的ThreadLocal变量来实现。 从内存模型出发看ThreadLocal： 我们知道，在虚拟机中，堆内存就是用于存储共享数据，也就是这里所说的主内存。 每个线程将会在堆内存中开辟一块空间叫做线程的工作内存，附带一块缓存区用于存储共享数据副本。那么，共享数据在堆内存当中，线程通信就是通过主内存为中介，线程在本地内存读并且操作完共享变量操作完毕以后，把值写入主内存。 ThreadLocal被称为线程局部变量，说白了就是线程工作内存的一小块内存，用于存储数据。 那么，ThreadLocal.set()、ThreadLocal.get()方法，就相当于把数据存储于线程本地，取也是在本地内存读取。就不会像synchronized需要频繁的修改主内存的数据，再把数据复制到工作内存，也大大提高访问效率。 那么，我们再来回答上面引出的问题，mybatis为什么要用ThreadLocal来存储session？ 首先，因为线程间的数据交互是通过工作内存与主存的频繁读写完成通信，然而存储于线程本地内存，提高访问效率，避免线程阻塞造成cpu吞吐率下降。再者，在多线程中，每一个线程都各自维护session，轻易完成对线程独享资源的操作。 理解ThreadLocal的关键源码首先，要理解ThreadLocal的数据结构，我们可以看它的set/get方法：ThreadLocal.java1234567891011121314151617181920public void set(T value) &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); &#125;...void createMap(Thread t, T firstValue) &#123; t.threadLocals = new ThreadLocalMap(this, firstValue);&#125;...ThreadLocalMap getMap(Thread t) &#123; return t.threadLocals;&#125; Thread.java1ThreadLocal.ThreadLocalMap threadLocals = null; ThreadLocalMap作为ThreadLocal的静态内部类，用于存储多个ThreadLocal对象 ThreadLocal对象作为ThreadLocalMap的key来存储，我们set进去的独享数据作为value存储 留意到它里边调到的getMap(Thread)方法，得知ThreadLocalMap的获取跟当前Thread有关，仔细看threadLocals其实就是当前线程的一个ThreadLocalMap变量。也就是说，一个线程对应一个ThreadLocalMap，get()就是当前程获取自己的ThreadLocalMap。 12345678910111213public T get() &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) &#123; ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; @SuppressWarnings("unchecked") T result = (T)e.value; return result; &#125; &#125; return setInitialValue(); &#125; 线程根据使用那一小块的线程本地内存，以ThreadLocal对象作为key，去获取存储于ThreadLocalMap中的值。 ThreadLocal内存泄露引用关系图 先引用一张经典的引用关系图来说明当前线程(currentThread)以及threadLocalMap、key、threadLocal实例几个之间的引用关系： 利用这图来回顾总结一下ThreadLocal的实现：每个Thread 维护一个 ThreadLocalMap 映射表，这个映射表的 key 是 ThreadLocal实例本身，value 是真正需要存储的 Object。 也就是说 ThreadLocal 本身并不存储值，它只是作为一个 key 来让线程从 ThreadLocalMap 获取 value。值得注意的是图中的虚线，表示 ThreadLocalMap 是使用 ThreadLocal 的弱引用作为 Key 的，弱引用的对象在 GC 时会被回收。 ThreadLocal为什么会内存泄漏我们可以理解到，每个线程都会创建一块工作内存，每个线程都有一个ThreadLocalMap，而ThreadLocalMap可以有多个key，也就是说可以存储多个ThreadLocal。那么假设，开启1万个线程，每个线程创建1万个ThreadLocal，也就是每个线程维护1万个ThreadLocal小内存空间！那么，当线程执行结束以后，如果一个ThreadLocal没有外部强引用来引用它而是用弱引用来引用，那么系统 GC 的时候，这个ThreadLocal势必会被回收，这样一来，ThreadLocalMap中就会出现key为null的Entry，就没有办法访问这些key为null的Entry的value，如果当前线程不结束的话，这些key为null的Entry的value就会一直存在一条强引用链：Thread Ref -&gt; Thread -&gt; ThreaLocalMap -&gt; Entry -&gt; value永远无法回收，造成内存泄漏。 Key使用什么引用才好？如上，key对ThreadLocal使用弱引用会发生内存泄露。那么，如果使用强使用，问题是否就得以解决？若 key 使用强引用：引用的ThreadLocal的对象被回收了，但是ThreadLocalMap还持有ThreadLocal的强引用，如果没有手动删除，ThreadLocal不会被回收，导致Entry内存泄漏。那么如果 key 使用弱引用：引用的ThreadLocal的对象被回收了，由于ThreadLocalMap持有ThreadLocal的引用是弱引用，即使没有手动删除，ThreadLocal也会被回收。至于value，则在下一次ThreadLocalMap调用set,get，remove的时候会被清除。所以比较两种情况，我们可以发现：由于ThreadLocalMap的生命周期跟Thread一样长，如果都没有手动删除对应key，都会导致内存泄漏，但是使用弱引用可以多一层保障：弱引用ThreadLocal不会内存泄漏，而对应的value在下一次ThreadLocalMap调用set,get,remove的时候会被清除。因此，ThreadLocal内存泄漏的根源是：由于ThreadLocalMap的生命周期跟Thread一样长，如果没有手动删除对应key就会导致内存泄漏，而不是因为弱引用。 ThreadLocal防内存泄露最佳实践综上，我们可以理解ThreadLocal为避免内存泄露的设计大致上是： JVM利用ThreadLocalMap的Key为弱引用，来避免ThreadLocal内存泄露。 由于Key设置为弱引用，那么，当ThreadLocal存储很多Key为null的Entry的时候，而不再去调用remove、get、set方法，那么将导致内存泄漏。所以，每次使用完ThreadLocal，都调用它的remove()方法，清除数据，则可以达到回收弱引用的结果，这是最佳的使用实践。否则，在使用线程池的情况下，没有及时清理ThreadLocal，不仅是内存泄漏的问题，更严重的是可能导致业务逻辑出现问题。所以，使用ThreadLocal就跟加锁完要解锁一样，用完就清理。 Reference《并发编程（四）：ThreadLocal从源码分析总结到内存泄漏》《深入分析 ThreadLocal 内存泄漏问题》]]></content>
      <tags>
        <tag>线程</tag>
        <tag>并发</tag>
        <tag>同步</tag>
        <tag>锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[整理Volatile]]></title>
    <url>%2F2018%2F11%2F03%2FVolatile-Summary%2F</url>
    <content type="text"><![CDATA[整理volatile相关，便于回顾记忆… Volatile简介volatile是轻量级的synchronized，它在多处理器开发中保证了共享变量的“可见性”，但不像synchronized一样保证原子性。可见性的意思是当一个线程修改一个共享变量时，另外一个线程能读到这个修改的值。如果volatile变量修饰符使用恰当的话，它比synchronized的使用和执行成本更低，因为它不会引起线程上下文的切换和调度。 Volatile的特性 volatile可见性；对一个volatile的读，总可以看到对这个变量最近一次的写； volatile原子性；volatile对单个读/写具有原子性（32位Long、Double），但是复合操作除外，例如i++; JVM底层采用“内存屏障”来实现volatile语义，禁止重排序以保证有序性 理解volatile特性的一个好方法是把对volatile变量的单个读/写，看成是使用同一个锁对这些单个读/写操作做了同步。下面通过具体的示例来说明，示例代码如下。 123456789101112131415class VolatileFeaturesExample &#123; volatile long vl = 0L; // 使用volatile声明64位的long型变量 public void set(long l) &#123; vl = l; // 单个volatile变量的写 &#125; public void getAndIncrement () &#123; vl++; // 复合（多个）volatile变量的读/写 &#125; public long get() &#123; return vl; // 单个volatile变量的读 &#125;&#125; 假设有多个线程分别调用上面程序的3个方法，这个程序在语义上和下面程序等价。 12345678910111213141516class VolatileFeaturesExample &#123; long vl = 0L; // 64位的long型普通变量 public synchronized void set(long l) &#123; // 对单个的普通变量的写用同一个锁同步 vl = l; &#125; public void getAndIncrement () &#123; // 普通方法调用 long temp = get(); // 调用已同步的读方法 temp += 1L; // 普通写操作 set(temp); // 调用已同步的写方法 &#125; public synchronized long get() &#123; // 对单个的普通变量的读用同一个锁同步 return vl; &#125;&#125; 如上面示例程序所示，一个volatile变量的单个读/写操作，与一个普通变量的读/写操作都是使用同一个锁来同步，它们之间的执行效果相同。锁的happens-before规则保证释放锁和获取锁的两个线程之间的内存可见性，这意味着对一个volatile变量的读，总是能看到（任意线程）对这个volatile变量最后的写入。锁的语义决定了临界区代码的执行具有原子性。这意味着，即使是64位的long型和double型变量，只要它是volatile变量，对该变量的读/写就具有原子性。如果是多个volatile操作或类似于volatile++这种复合操作，这些操作整体上不具有原子性。 Volataile的内存语义及其实现Java语言规范对volatile的定义如下：1Java编程语言允许线程访问共享变量，为了确保共享变量能被准确和一致地更新，线程应该确保通过排他锁单独获得这个变量。 通俗点讲就是说一个变量如果用volatile修饰了，则Java可以确保所有线程看到这个变量的值是一致的，如果某个线程对volatile修饰的共享变量进行更新，那么其他线程可以立马看到这个更新，这就是所谓的线程可见性。 在了解volatile实现原理之前，我们先来看下与其实现原理相关的CPU术语与说明。下表是CPU术语的定义 Volatile是如何来保证可见性的呢？让我们在X86处理器下通过工具获取JIT编译器生成的汇编指令来查看对volatile进行写操作时，CPU会做什么事情。 Java代码如下：1instance = new Singleton(); // instance是volatile变量 转变成汇编代码，如下。10x01a3de1d: movb $0×0,0×1104800(%esi);0x01a3de24: lock addl $0×0,(%esp); 有volatile变量修饰的共享变量进行写操作的时候会多出第二行汇编代码，通过查IA-32架构软件开发者手册可知，Lock前缀的指令在多核处理器下会引发了两件事情。 1） 将当前处理器缓存行的数据写回到系统内存。2） 这个写回内存的操作会使在其他CPU里缓存了该内存地址的数据无效。为了提高处理速度，处理器不直接和内存进行通信，而是先将系统内存的数据读到内部缓存（L1，L2或其他）后再进行操作，但操作完不知道何时会写到内存。如果对声明了volatile的变量进行写操作，JVM就会向处理器发送一条Lock前缀的指令，将这个变量所在缓存行的数据写回到系统内存。但是，就算写回到内存，如果其他处理器缓存的值还是旧的，再执行计算操作就会有问题。所以，在多处理器下，为了保证各个处理器的缓存是一致的，就会实现缓存一致性协议，每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期了，当处理器发现自己缓存行对应的内存地址被修改，就会将当前处理器的缓存行设置成无效状态，当处理器对这个数据进行修改操作的时候，会重新从系统内存中把数据读到处理器缓存里。下面来具体讲解volatile的两条实现原则。 Lock前缀指令会引起处理器缓存回写到内存。Lock前缀指令导致在执行指令期间，声言处理器的LOCK#信号。在多处理器环境中，LOCK#信号确保在声言该信号期间，处理器可以独占任何共享内存[2]。但是，在最近的处理器里，LOCK＃信号一般不锁总线，而是锁缓存，毕竟锁总线开销的比较大。在8.1.4节有详细说明锁定操作对处理器缓存的影响，对于Intel486和Pentium处理器，在锁操作时，总是在总线上声言LOCK#信号。但在P6和目前的处理器中，如果访问的内存区域已经缓存在处理器内部，则不会声言LOCK#信号。相反，它会锁定这块内存区域的缓存并回写到内存，并使用缓存一致性机制来确保修改的原子性，此操作被称为“缓存锁定”，缓存一致性机制会阻止同时修改由两个以上处理器缓存的内存区域数据。 一个处理器的缓存回写到内存会导致其他处理器的缓存无效。IA-32处理器和Intel 64处理器使用MESI（修改、独占、共享、无效）控制协议去维护内部缓存和其他处理器缓存的一致性。在多核处理器系统中进行操作的时候，IA-32和Intel 64处理器能嗅探其他处理器访问系统内存和它们的内部缓存。处理器使用嗅探技术保证它的内部缓存、系统内存和其他处理器的缓存的数据在总线上保持一致。例如，在Pentium和P6 family处理器中，如果通过嗅探一个处理器来检测其他处理器打算写内存地址，而这个地址当前处于共享状态，那么正在嗅探的处理器将使它的缓存行无效，在下次访问相同内存地址时，强制执行缓存行填充。 Volatile写-读建立的happens-before关系从JSR-133开始（即从JDK5开始），volatile变量的写-读可以实现线程之间的通信。从内存语义的角度来说，volatile的写-读与锁的释放-获取有相同的内存效果：volatile写和锁的释放有相同的内存语义；volatile读与锁的获取有相同的内存语义。12345678910111213141516class VolatileExample &#123; int a = 0; volatile boolean flag = false; public void writer() &#123; a = 1; // 1 flag = true; // 2 &#125; public void reader() &#123; if (flag) &#123; // 3 int i = a; // 4 …… &#125; &#125; &#125; 假设线程A执行writer()方法之后，线程B执行reader()方法。根据happens-before规则，这个过程建立的happens-before关系可以分为3类：1) 根据程序次序规则，1 happens-before 2;3 happens-before 4。2) 根据volatile规则，2 happens-before 3。3) 根据happens-before的传递性规则，1 happens-before 4。在上图中，每一个箭头链接的两个节点，代表了一个happens-before关系。黑色箭头表示程序顺序规则；橙色箭头表示volatile规则；蓝色箭头表示组合这些规则后提供的happens-before保证。这里A线程写一个volatile变量后，B线程读同一个volatile变量。A线程在写volatile变量之前所有可见的共享变量，在B线程读同一个volatile变量后，将立即变得对B线程可见。 限制重排序重排序分为编译器重排序和处理器重排序。为了实现volatile内存语义，JMM会分别限制这两种类型的重排序类型.以下是JMM针对编译器制定的volatile重排序规则表：举例来说，第三行最后一个单元格的意思是：在程序中，当第一个操作为普通变量的读或写时，如果第二个操作为volatile写，则编译器不能重排序这两个操作。从表中可以看出： 当第二个操作是volatile写时，不管第一个操作是什么，都不能重排序。这个规则确保volatile写之前的操作不会被编译器重排序到volatile写之后。 当第一个操作是volatile读时，不管第二个操作是什么，都不能重排序。这个规则确保volatile读之后的操作不会被编译器重排序到volatile读之前。 当第一个操作是volatile写，第二个操作是volatile读时，不能重排序。 为了实现volatile的内存语义，编译器在生成字节码时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序。对于编译器来说，发现一个最优布置来最小化插入屏障的总数几乎不可能。为此，JMM采取保守策略。下面是基于保守策略的JMM内存屏障插入策略。 在每个volatile写操作的前面插入一个StoreStore屏障。 在每个volatile写操作的后面插入一个StoreLoad屏障。 在每个volatile读操作的后面插入一个LoadLoad屏障。 在每个volatile读操作的后面插入一个LoadStore屏障。 上述内存屏障插入策略非常保守，但它可以保证在任意处理器平台，任意的程序中都能得到正确的volatile内存语义。下面是保守策略下，volatile写插入内存屏障后生成的指令序列示意图:图中的StoreStore屏障可以保证在volatile写之前，其前面的所有普通写操作已经对任意处理器可见了。这是因为StoreStore屏障将保障上面所有的普通写在volatile写之前刷新到主内存。这里比较有意思的是，volatile写后面的StoreLoad屏障。此屏障的作用是避免volatile写与后面可能有的volatile读/写操作重排序。因为编译器常常无法准确判断在一个volatile写的后面是否需要插入一个StoreLoad屏障（比如，一个volatile写之后方法立即return）。为了保证能正确实现volatile的内存语义，JMM在采取了保守策略：在每个volatile写的后面，或者在每个volatile读的前面插入一个StoreLoad屏障。从整体执行效率的角度考虑，JMM最终选择了在每个volatile写的后面插入一个StoreLoad屏障。因为volatile写-读内存语义的常见使用模式是：一个写线程写volatile变量，多个读线程读同一个volatile变量。当读线程的数量大大超过写线程时，选择在volatile写之后插入StoreLoad屏障将带来可观的执行效率的提升。从这里可以看到JMM在实现上的一个特点：首先确保正确性，然后再去追求执行效率。下面是在保守策略下，volatile读插入内存屏障后生成的指令序列示意图:图中的LoadLoad屏障用来禁止处理器把上面的volatile读与下面的普通读重排序。LoadStore屏障用来禁止处理器把上面的volatile读与下面的普通写重排序。上述volatile写和volatile读的内存屏障插入策略非常保守。在实际执行时，只要不改变volatile写-读的内存语义，编译器可以根据具体情况省略不必要的屏障。下面通过具体的示例代码进行说明。1234567891011121314class VolatileBarrierExample &#123; int a; volatile int v1 = 1; volatile int v2 = 2; void readAndWrite() &#123; int i = v1; // 第一个volatile读 int j = v2; // 第二个volatile读 a = i + j; // 普通写 v1 = i + 1; // 第一个volatile写 v2 = j * 2; // 第二个 volatile写 &#125; …// 其他方法&#125; 针对readAndWrite()方法，编译器在生成字节码时可以做如下的优化。注意，最后的StoreLoad屏障不能省略。因为第二个volatile写之后，方法立即return。此时编译器可能无法准确断定后面是否会有volatile读或写，为了安全起见，编译器通常会在这里插入一个StoreLoad屏障。上面的优化针对任意处理器平台，由于不同的处理器有不同“松紧度”的处理器内存模型，内存屏障的插入还可以根据具体的处理器内存模型继续优化。以X86处理器为例，图3-21中除最后的StoreLoad屏障外，其他的屏障都会被省略。前面保守策略下的volatile读和写，在X86处理器平台可以优化成如下图所示。前文提到过，X86处理器仅会对写-读操作做重排序。X86不会对读-读、读-写和写-写操作做重排序，因此在X86处理器中会省略掉这3种操作类型对应的内存屏障。在X86中，JMM仅需在volatile写后面插入一个StoreLoad屏障即可正确实现volatile写-读的内存语义。这意味着在X86处理器中，volatile写的开销比volatile读的开销会大很多（因为执行StoreLoad屏障开销会比较大）。 Reference《Java并发编程的艺术》–方腾飞]]></content>
      <tags>
        <tag>线程</tag>
        <tag>并发</tag>
        <tag>同步</tag>
        <tag>锁</tag>
        <tag>内存模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[整理Synchronized]]></title>
    <url>%2F2018%2F11%2F03%2FSynchronized-Summary%2F</url>
    <content type="text"><![CDATA[整理volatile相关，便于回顾记忆… Synchronized简介Synchronized一直是Java多线程并发编程中用作同步的元老级角色，很多人对它的概念都是一个重量级锁，但在JDK 1.6，对synchronized进行了各种优化，为了减少获得锁和释放锁带来的性能消耗而引入了偏向锁和轻量级锁，和锁的存储结构和升级过程。Synchronized实现同步的基础在于Java中的每一个对象都可以作为，所以本质上synchronized就是一把对象锁。Synchronized可作用于类，静态方法，普通方法，及代码块，具体表现为以下3种形式： 对于普通同步方法，锁是当前实例对象 对于静态同步方法，锁是当前类的Class对象。 对于同步方法块，锁是Synchonized括号里配置的对象 当一个线程试图访问同步代码块时，它首先必须得到锁，退出或抛出异常时必须释放锁。那么锁到底存在哪里呢？锁里面会存储什么信息呢？ 同步的原理管程(Monitor)对象Java 虚拟机中的同步(Synchronization)基于进入和退出管程(Monitor)对象实现，无论是显式同步，还是隐式同步都如此。 显式同步是指有明确的monitorenter和monitorexit指令，也就是synchronized同步代码块的场景 隐式同步则是由方法调用指令读取运行时常量池中方法的ACC_SYNCHRONIZED 标志来隐式实现，也就是指方法同步的场景 1234567891011public class SynchronizedTest &#123; public synchronized void test1()&#123; System.out.println( "test1" ); &#125; public void test2()&#123; synchronized (this)&#123; System.out.println( "test2" ); &#125; &#125;&#125; Java对象头在JVM中，对象在内存中的布局分为三块区域：对象头、实例数据和对齐填充，一般而言，synchronized使用的锁对象是存储在Java对象头里的。如果对象是非数组类型，则用2字宽存储对象头，如果对象是数组则会分配3个字宽，多出来的1个字记录的是数组长度。在32位虚拟机中，一字宽等于四字节，即32bit。 Java对象头里的Mark Word里默认存储对象的HashCode，分代年龄和锁标记位。32位JVM的Mark Word的默认存储结构如下： 在运行期间Mark Word里存储的数据会随着锁标志位的变化而变化。Mark Word可能变化为存储以下4种数据： 在64位虚拟机下，Mark Word是64bit大小的，其存储结构如下： Synchronized同步方法底层原理方法级的同步是隐式，即无需通过字节码指令来控制的，它实现在方法调用和返回操作之中。JVM可以从方法常量池中的方法表结构(method_info Structure) 中的 ACC_SYNCHRONIZED 访问标志区分一个方法是否同步方法。当方法调用时，调用指令将会 检查方法的 ACC_SYNCHRONIZED 访问标志是否被设置，如果设置了，执行线程将先持有monitor（虚拟机规范中用的是管程一词）， 然后再执行方法，最后再方法完成(无论是正常完成还是非正常完成)时释放monitor。在方法执行期间，执行线程持有了monitor，其他任何线程都无法再获得同一个monitor。如果一个同步方法执行期间抛 出了异常，并且在方法内部无法处理此异常，那这个同步方法所持有的monitor将在异常抛到同步方法之外时自动释放。 Synchronized同步代码块底层原理从字节码中可知同步语句块的实现使用的是monitorenter 和 monitorexit 指令，其中monitorenter指令指向同步代码块的开始位置，monitorexit指令则指明同步代码块的结束位置，当执行monitorenter指令时，当前线程将试图获取 objectref(即对象锁) 所对应的 monitor 的持有权，当 objectref 的 monitor 的进入计数器为 0，那线程可以成功取得 monitor，并将计数器值设置为 1，取锁成功。如果当前线程已经拥有 objectref 的 monitor 的持有权，那它可以重入这个 monitor (关于重入性稍后会分析)，重入时计数器的值也会加 1。倘若其他线程已经拥有 objectref 的 monitor 的所有权，那当前线程将被阻塞，直到正在执行线程执行完毕，即monitorexit指令被执行，执行线程将释放 monitor(锁)并设置计数器值为0 ，其他线程将有机会持有 monitor 。值得注意的是编译器将会确保无论方法通过何种方式完成，方法中调用过的每条 monitorenter 指令都有执行其对应 monitorexit 指令，而无论这个方法是正常结束还是异常结束。为了保证在方法异常完成时 monitorenter 和 monitorexit 指令依然可以正确配对执行，编译器会自动产生一个异常处理器，这个异常处理器声明可处理所有的异常，它的目的就是用来执行 monitorexit 指令。从字节码中也可以看出多了一个monitorexit指令，它就是异常结束时被执行的释放monitor 的指令。 实例对象锁的线程安全当一个线程正在访问一个对象的 synchronized 实例方法，那么其他线程不能访问该对象的其他 synchronized 方法，毕竟一个对象只有一把锁，当一个线程获取了该对象的锁之后，其他线程无法获取该对象的锁，所以无法访问该对象的其他synchronized实例方法，但是其他线程还是可以访问该实例对象的其他非synchronized方法，当然如果是一个线程 A 需要访问实例对象 obj1 的 synchronized 方法 f1(当前对象锁是obj1)，另一个线程 B 需要访问实例对象 obj2 的 synchronized 方法 f2(当前对象锁是obj2)，这样是允许的，因为两个实例对象锁并不同相同，此时如果两个线程操作数据并非共享的，线程安全是有保障的，遗憾的是如果两个线程操作的是共享数据，那么线程安全就有可能无法保证了，如下代码将演示出该现象:123456789101112131415161718192021222324public class AccountingSyncBad implements Runnable&#123; static int i=0; public synchronized void increase()&#123; i++; &#125; @Override public void run() &#123; for(int j=0;j&lt;1000000;j++)&#123; increase(); &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; //new新实例 Thread t1=new Thread(new AccountingSyncBad()); //new新实例 Thread t2=new Thread(new AccountingSyncBad()); t1.start(); t2.start(); //join含义:当前线程A等待thread线程终止之后才能从thread.join()返回 t1.join(); t2.join(); System.out.println(i); &#125;&#125; 上述代码与前面不同的是我们同时创建了两个新实例AccountingSyncBad，然后启动两个不同的线程对共享变量i进行操作，但很遗憾操作结果是1452317而不是期望结果2000000，因为上述代码犯了严重的错误，虽然我们使用synchronized修饰了increase方法，但却new了两个不同的实例对象，这也就意味着存在着两个不同的实例对象锁，因此t1和t2都会进入各自的对象锁，也就是说t1和t2线程使用的是不同的锁，因此线程安全是无法保证的。 锁的升级Java早期版本中，synchronized属于重量级锁，效率低下，因为监视器锁（monitor）是依赖于底层的操作系统的Mutex Lock来实现的，而操作系统实现线程之间的切换时需要从用户态转换到核心态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高，这也是为什么早期的synchronized效率低的原因。Java SE1.6为了减少获得锁和释放锁所带来的性能消耗，引入了“偏向锁”和“轻量级锁”，所以在Java SE1.6里锁一共有四种状态，无锁状态，偏向锁状态，轻量级锁状态和重量级锁状态，它会随着竞争情况逐渐升级。锁可以升级但不能降级，意味着偏向锁升级成轻量级锁后不能降级成偏向锁。这种锁升级却不能降级的策略，目的是为了提高获得锁和释放锁的效率，下文会详细分析。 偏向锁Hotspot的作者经过以往的研究发现大多数情况下锁不仅不存在多线程竞争，而且总是由同一线程多次获得，为了让线程获得锁的代价更低而引入了偏向锁。当一个线程访问同步块并获取锁时，会在对象头和栈帧中的锁记录里存储锁偏向的线程ID，以后该线程在进入和退出同步块时不需要花费CAS操作来加锁和解锁，而只需简单的测试一下对象头的Mark Word里是否存储着指向当前线程的偏向锁，如果测试成功，表示线程已经获得了锁，如果测试失败，则需要再测试下Mark Word中偏向锁的标识是否设置成1（表示当前是偏向锁），如果没有设置，则使用CAS竞争锁，如果设置了，则尝试使用CAS将对象头的偏向锁指向当前线程。 偏向锁的撤销偏向锁使用了一种等到竞争出现才释放锁的机制，所以当其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁。偏向锁的撤销，需要等待全局安全点（在这个时间点上没有字节码正在执行），它会首先暂停拥有偏向锁的线程，然后检查持有偏向锁的线程是否活着，如果线程不处于活动状态，则将对象头设置成无锁状态，如果线程仍然活着，拥有偏向锁的栈会被执行，遍历偏向对象的锁记录，栈中的锁记录和对象头的Mark Word要么重新偏向于其他线程，要么恢复到无锁或者标记对象不适合作为偏向锁，最后唤醒暂停的线程。下图中的线程1演示了偏向锁初始化的流程，线程2演示了偏向锁撤销的流程。 关闭偏向锁偏向锁在Java 6和Java 7里是默认启用的，但是它在应用程序启动几秒钟之后才激活，如有必要可以使用JVM参数来关闭延迟-XX：BiasedLockingStartupDelay = 0。如果你确定自己应用程序里所有的锁通常情况下处于竞争状态，可以通过JVM参数关闭偏向锁-XX:-UseBiasedLocking=false，那么默认会进入轻量级锁状态。 轻量级锁轻量级锁加锁线程在执行同步块之前，JVM会先在当前线程的栈桢中创建用于存储锁记录的空间，并将对象头中的Mark Word复制到锁记录中，官方称为Displaced Mark Word。然后线程尝试使用CAS将对象头中的Mark Word替换为指向锁记录的指针。如果成功，当前线程获得锁，如果失败，表示其他线程竞争锁，当前线程便尝试使用自旋来获取锁。 轻量级锁解锁轻量级解锁时，会使用原子的CAS操作来将Displaced Mark Word替换回到对象头，如果成功，则表示没有竞争发生。如果失败，表示当前锁存在竞争，锁就会膨胀成重量级锁。下图是两个线程同时争夺锁，导致锁膨胀的流程图。 因为自旋会消耗CPU，为了避免无用的自旋（比如获得锁的线程被阻塞住了），一旦锁升级成重量级锁，就不会再恢复到轻量级锁状态。当锁处于这个状态下，其他线程试图获取锁时，都会被阻塞住，当持有锁的线程释放锁之后会唤醒这些线程，被唤醒的线程就会进行新一轮的夺锁之争。 锁的优缺点对比 Reference 方腾飞：《Java并发编程的艺术》 深入理解Java并发之synchronized实现原理]]></content>
      <tags>
        <tag>线程</tag>
        <tag>并发</tag>
        <tag>同步</tag>
        <tag>锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis基本数据结构-Dict]]></title>
    <url>%2F2018%2F11%2F03%2FRedis-DataStructure-3-Dict%2F</url>
    <content type="text"><![CDATA[Dict简介字典（dict）是Redis一个重要的基础数据结构，它是一个用于维护key和value映射关系的数据结构，与很多语言中的Map或dictionary类似。不过，这只是它在Redis中的一个用途而已，它在Redis中被使用的地方还有很多。比如Redis的数据库就是使用字典来作为底层实现的，对数据的增删查改操作都是构建在对字典的操作之上，又比如，一个Redis hash结构，当它的field较多时，便会采用dict来存储。再比如，Redis配合使用dict和skiplist来共同维护一个sorted set。字典经常作为一种数据结构内置在很多高级的编程语言里，但Redis的实现所使用的C语言中并没有内置这种数据结构，因此Redis构建了自己的字典实现。 Dict的双数组哈希表Redis中的字典由dict.h/dict结构表示：123456789101112131415typedef struct dict &#123; // 类型特定函数 dictType *type; // 私有数据 void *privatedata; // 哈希表 dictht ht[2]; // rehash 索引 // 当rehash不在进行时，值为-1 int trehashidx; /* rehashing not in progress if rehashidx == -1 */&#125;dict; 其中ht属性是一个包含两个项的数组，数组中的每个项都是一个dictht哈希表，一般情况下，字典只使用ht[0]哈希表，ht[1]哈希表只用于对ht[0]哈希表进行rehash时。除了ht[1]之外，另一个和rehash有关的属性就是rehashidx，它记录了rehash目前的进度，如果目前没有在进行rehash，那么它的值为-1。 哈希算法Redis计算哈希值和索引值的方法：123456# 使用字典设置的哈希函数，计算键key的哈希值hash = dict-&gt;type-&gt;hashFunction(key);# 使用哈希值的 sizemark 属性和和哈希值，计算出索引值# 根据情况不同，ht[x]可以是ht[0]或者ht[1]index = hash &amp; dict-&gt;ht[x].sizemark; 当字典被用作数据库的底层实现，或者哈希键的底层实现时，Redis使用MurmurHash2算法来计算键的哈希值。 键冲突键冲突解决方法是链地址法（拉链法），与Java里的hashmap一样使用单向链表把同一个索引上的多个节点连接起来。因为dictEntry节点组成的链表没有指向链表表尾的指针，所以为了速度考虑，程序总是把新节点添加到链表的表头位置（复杂度为O(1)），排在其它已有节点的前面。 Rehash哈希表的扩展与收缩是通过执行rehash（重新散列）操作来完成，操作步骤如下： 空间分配：为字典的ht[1]哈希表分配空间，这个哈希表的空间大小取决于要执行的操作，以及ht[0]当前包含的键值对数量（也即是ht[0].used属性的值）: 如果是扩展操作，那么ht[1]的大小为第一个大于等于ht[0].used*2的2ⁿ（2的n次幂）。 如果是收缩操作，那么ht[1]的大小为第一个大于等于ht[0].used的2ⁿ。 rehash：将保存在ht[0]中的所有键值对rehash到ht[1]上面：rehash指的是重新计算键的哈希值和索引值，然后将键值对放置到ht[1]哈希表的指定位置上。 替换：当ht[0]包含的所有键值对都迁移到ht[1]之后（ht[0]变为空表），释放ht[0]，将ht[1]设置为ht[0]，并在ht[1]新创建一个空白哈希表，为下一次rehash做准备。 哈希表的扩展与收缩当以下条件中的任意一个被满足时，程序会自动开始对哈希表执行扩展操作：1) 没在执行BGSAVE命令或者BGREWRITEAOF命令，而哈希表的负载因子≥1。2) 有在执行BGSAVE命令或者BGREWRITEAOF命令,并且哈希表的负载因子≥0.5。 负载因子计算为： load_factor = ht[0].used / ht[0].size; 负载因子＜0.1时，进行收缩操作。 渐进式rehashRedis的扩展或收缩需要把ht[0]所有键值对rehash到ht[1]里，这个动作并不是一次性、集中式完成的，而是分多次、渐进式完成的。这么做的原因是，当哈希表数据量庞大到一定量级时，一次性rehash操作会带来庞大的计算量，这样可能会导致服务器在一段时间内停止服务。以下是渐进式rehash的详细步骤：1) 为ht[1]分配空间，让字典同时持有ht[0]和ht[1]两个哈希表。2) 在字典中维持一个索引计数器变量rehashidx，并将它的值设置为0，表示工作正式开始。3) 在rehash进行期间，每次对字典执行增删改查操作时，程序除了执行指定的操作以外，还会顺带将ht[0]哈希表在rehashidx索引上的所有键值对rehash到ht[1]，当rehash完成之后，程序将rehashidx属性的值增1。4) 随着字典操作的不断执行，最终在某个时间节点上，ht[0]的所有键值对都会到了ht[1]，这时程序将rehashidx属性的值设为-1，表示rehash操作完成。 因为在进行渐进式rehash的过程中，字典会同时使用ht[0]和ht[1]两个哈希表，所以期间的增删查改操作都会在两个表上进行，例如查找操作时，先会在ht[0]里查找，如果没找到，再查ht[1]，诸如此类。增加操作时，那么新添加的键值对一律会被保存到ht[1]里，而ht[0]则不进行添加，保证了ht[0]包含的键值对只减不增，并随着rehash操作的执行完毕而最终变成空表。]]></content>
      <tags>
        <tag>Redis</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis基本数据结构-SkipList]]></title>
    <url>%2F2018%2F10%2F26%2FRedis-DataStructure-2-SkipList%2F</url>
    <content type="text"><![CDATA[Sorted Set有序集合键（sorted set）提供的操作非常丰富，可以满足非常多的应用场景。这也意味着，sorted set相对来说实现比较复杂。Redis使用跳跃表（skipList）作为sorted set的底层实现之一，如果一个sorted set包含的元素数量比较多，又或者sorted set中元素的成员（member）是比较长的字符串时，Redis就会使用跳跃表来作为sorted set的底层实现。 跳跃表是一种有序数据结构，能支持平均O(logN)、最坏O(N)复杂度的节点查找，在大部分情况下，跳跃表的效率可以和平衡树相媲美，所以不少程序都使用跳跃表来替代平衡树。 sorted set的数据结构有序集合（sorted set）的数据结构底层实现就是跳跃表+字典，如图： 跳跃表的数据结构一般查找问题的解法分为两个大类：一个是基于各种平衡树，一个是基于哈希表。但skiplist却比较特殊，它没法归属到这两大类里面。 skiplist，顾名思义，首先它是一个list。实际上，它是在有序链表的基础上发展起来的。 下面我们从头分析一个要求有序的线性结构的查找元素及插入新元素存在的性能问题。 最容易表达线性结构的自然是数组和链表。可是，无论是数组还是链表，在插入新元素的时候，都会存在性能问题。 如果使用数组，插入新元素的方式如下： 如果要插入一个值是3的元素，首先要知道这个元素应该插入的位置。使用二分查找可以最快定位，这一步时间复杂度是O（logN）。 插入过程中，原数组中所有大于3的元素都要右移，这一步时间复杂度是O（N）。所以总体时间复杂度是O（N）。 如果使用链表，插入新元素的方式如下： 如果要插入一个值是3的元素，首先要知道这个元素应该插入的位置。链表无法使用二分查找，只能和原链表中的节点逐一比较大小来确定位置。这一步的时间复杂度是O（N）。 插入的过程倒是很容易，直接改变节点指针的目标，时间复杂度O（1）。因此总体的时间复杂度也是O（N）。 这对于拥有几十万元素的集合来说，这两种方法显然都太慢了。 问题来了，既然数组也不行，链表也不想，那要用什么结构才好？ 我们可以利用索引的思想，提取出链表中的部分关键节点。 比如给定一个长度是7的有序链表，节点值依次是1→2→3→5→6→7→8。那么我们可以取出所有奇数值的节点作为关键字。 此时如果要插入一个值是4的新节点，不再需要和原节点8,7,6,5,3逐一比较，只需要比较关键节点7,5,3 确定了新节点在关键节点中的位置（3和5之间），就可以回到原链表，迅速定位到对应的位置插入（同样3和5之间） 当链表中有1W设置10W个节点，优化效果会很明显，比较次数就整整减少了一半！但是这样的做法只是增加了50%的额外空间，却换来了一倍的性能提高。 不过我们可以进一步思考，既然已经提取了一层关键节点作为索引，那我们为何不能从索引中进一步提取，再提取一层索引的索引呢？ 于是乎，我们有了2级索引之后，新的节点可以先和2级索引比较，确定大体范围；然后再和1级索引比较；最后再回到原链表，找到并插入对应位置。 当节点很多的时候，比较次数会减少到原来的1/4，如是者，如果我们再继续往上提取更高层的索引，保证每一层是上一层节点的一半，一直到同一层只有两个节点（因为只有一个节点没有比较的意义），那么这样一个多层链表结构，便是我们的跳跃表。 那么，跳跃表的介绍引子至此已告一段落，下面是一个正儿八经的跳跃表的概念的介绍： 我们先来看一个有序链表，如下图（最左侧的灰色节点表示一个空的头结点）： 在这样一个链表中，如果我们要查找某个数据，那么需要从头开始逐个进行比较，直到找到包含数据的那个节点，或者找到第一个比给定数据大的节点为止（没找到）。也就是说，时间复杂度为O(n)。同样，当我们要插入新数据的时候，也要经历同样的查找过程，从而确定插入位置。 假如我们每相邻两个节点增加一个指针，让指针指向下下个节点，如下图： 这样所有新增加的指针连成了一个新的链表，但它包含的节点个数只有原来的一半（上图中是7, 19, 26）。现在当我们想查找数据的时候，可以先沿着这个新链表进行查找。当碰到比待查数据大的节点时，再回到原来的链表中进行查找。比如，我们想查找23，查找的路径是沿着下图中标红的指针所指向的方向进行的： 23首先和7比较，再和19比较，比它们都大，继续向后比较。 但23和26比较的时候，比26要小，因此回到下面的链表（原链表），与22比较。 23比22要大，沿下面的指针继续向后和26比较。23比26小，说明待查数据23在原链表中不存在，而且它的插入位置应该在22和26之间。 在这个查找过程中，由于新增加的指针，我们不再需要与链表中每个节点逐个进行比较了。需要比较的节点数大概只有原来的一半。 利用同样的方式，我们可以在上层新产生的链表上，继续为每相邻的两个节点增加一个指针，从而产生第三层链表。如下图： 在这个新的三层链表结构上，如果我们还是查找23，那么沿着最上层链表首先要比较的是19，发现23比19大，接下来我们就知道只需要到19的后面去继续查找，从而一下子跳过了19前面的所有节点。可以想象，当链表足够长的时候，这种多层链表的查找方式能让我们跳过很多下层节点，大大加快查找的速度。 skiplist正是受这种多层链表的想法的启发而设计出来的。实际上，按照上面生成链表的方式，上面每一层链表的节点个数，是下面一层的节点个数的一半，这样查找过程就非常类似于一个二分查找，使得查找的时间复杂度可以降低到O(log n)。但是，这种方法在插入数据的时候有很大的问题。新插入一个节点之后，就会打乱上下相邻两层链表上节点个数严格的2:1的对应关系。如果要维持这种对应关系，就必须把新插入的节点后面的所有节点（也包括新插入的节点）重新进行调整，这会让时间复杂度重新蜕化成O(n)。删除数据也有同样的问题。 skiplist为了避免这一问题，它不要求上下相邻两层链表之间的节点个数有严格的对应关系，而是为每个节点随机出一个层数(level)。比如，一个节点随机出的层数是3，那么就把它链入到第1层到第3层这三层链表中。为了表达清楚，下图展示了如何通过一步步的插入操作从而形成一个skiplist的过程: 从上面skiplist的创建和插入过程可以看出，每一个节点的层数（level）是随机出来的，而且新插入一个节点不会影响其它节点的层数。因此，插入操作只需要修改插入节点前后的指针，而不需要对很多节点都进行调整。这就降低了插入操作的复杂度。实际上，这是skiplist的一个很重要的特性，这让它在插入性能上明显优于平衡树的方案。这在后面我们还会提到。 根据上图中的skiplist结构，我们很容易理解这种数据结构的名字的由来。skiplist，翻译成中文，可以翻译成“跳表”或“跳跃表”，指的就是除了最下面第1层链表之外，它会产生若干层稀疏的链表，这些链表里面的指针故意跳过了一些节点（而且越高层的链表跳过的节点越多）。这就使得我们在查找数据的时候能够先在高层的链表中进行查找，然后逐层降低，最终降到第1层链表来精确地确定数据位置。在这个过程中，我们跳过了一些节点，从而也就加快了查找速度。 刚刚创建的这个skiplist总共包含4层链表，现在假设我们在它里面依然查找23，下图给出了查找路径： 需要注意的是，前面演示的各个节点的插入过程，实际上在插入之前也要先经历一个类似的查找过程，在确定插入位置后，再完成插入操作。 至此，skiplist的查找和插入操作，我们已经很清楚了。而删除操作与插入操作类似，我们也很容易想象出来。这些操作我们也应该能很容易地用代码实现出来。 当然，实际应用中的skiplist每个节点应该包含key和value两部分。前面的描述中我们没有具体区分key和value，但实际上列表中是按照key进行排序的，查找过程也是根据key在比较。 但是，如果你是第一次接触skiplist，那么一定会产生一个疑问：节点插入时随机出一个层数，仅仅依靠这样一个简单的随机数操作而构建出来的多层链表结构，能保证它有一个良好的查找性能吗？为了回答这个疑问，我们需要分析skiplist的统计性能。 在分析之前，我们还需要着重指出的是，执行插入操作时计算随机数的过程，是一个很关键的过程，它对skiplist的统计特性有着很重要的影响。这并不是一个普通的服从均匀分布的随机数，它的计算过程如下： 首先，每个节点肯定都有第1层指针（每个节点都在第1层链表里）。 如果一个节点有第i层(i&gt;=1)指针（即节点已经在第1层到第i层链表中），那么它有第(i+1)层指针的概率为p。 节点最大的层数不允许超过一个最大值，记为MaxLevel。 这个计算随机层数的伪码如下所示： 123456randomLevel() level := 1 // random()返回一个[0...1)的随机数 while random() &lt; p and level &lt; MaxLevel do level := level + 1 return level randomLevel()的伪码中包含两个参数，一个是p，一个是MaxLevel。在Redis的skiplist实现中，这两个参数的取值为： 12p = 1/4MaxLevel = 32 skiplist与平衡树、哈希表的比较 skiplist和各种平衡树（如AVL、红黑树等）的元素是有序排列的，而哈希表不是有序的。因此，在哈希表上只能做单个key的查找，不适宜做范围查找。所谓范围查找，指的是查找那些大小在指定的两个值之间的所有节点。 在做范围查找的时候，平衡树比skiplist操作要复杂。在平衡树上，我们找到指定范围的小值之后，还需要以中序遍历的顺序继续寻找其它不超过大值的节点。如果不对平衡树进行一定的改造，这里的中序遍历并不容易实现。而在skiplist上进行范围查找就非常简单，只需要在找到小值之后，对第1层链表进行若干步的遍历就可以实现。 平衡树的插入和删除操作可能引发子树的调整，逻辑复杂，而skiplist的插入和删除只需要修改相邻节点的指针，操作简单又快速。 从内存占用上来说，skiplist比平衡树更灵活一些。一般来说，平衡树每个节点包含2个指针（分别指向左右子树），而skiplist每个节点包含的指针数目平均为1/(1-p)，具体取决于参数p的大小。如果像Redis里的实现一样，取p=1/4，那么平均每个节点包含1.33个指针，比平衡树更有优势。 查找单个key，skiplist和平衡树的时间复杂度都为O(log n)，大体相当；而哈希表在保持较低的哈希值冲突概率的前提下，查找时间复杂度接近O(1)，性能更高一些。所以我们平常使用的各种Map或dictionary结构，大都是基于哈希表实现的。 从算法实现难度上来比较，skiplist比平衡树要简单得多。 Redis中的sorted set我们前面提到过，Redis中的sorted set，是在skiplist, dict和ziplist基础上构建起来的: 当数据较少时，sorted set是由一个ziplist来实现的。 当数据多的时候，sorted set是由一个叫zset的数据结构来实现的，这个zset包含一个dict + 一个skiplist。dict用来查询数据到分数(score)的对应关系，而skiplist用来根据分数查询数据（可能是范围查找）。 在这里我们先来讨论一下前一种情况——基于ziplist实现的sorted set。在本系列前面关于ziplist的文章里，我们介绍过，ziplist就是由很多数据项组成的一大块连续内存。由于sorted set的每一项元素都由数据和score组成，因此，当使用zadd命令插入一个(数据, score)对的时候，底层在相应的ziplist上就插入两个数据项：数据在前，score在后。 ziplist的主要优点是节省内存，但它上面的查找操作只能按顺序查找（可以正序也可以倒序）。因此，sorted set的各个查询操作，就是在ziplist上从前向后（或从后向前）一步步查找，每一步前进两个数据项，跨域一个(数据, score)对。 随着数据的插入，sorted set底层的这个ziplist就可能会转成zset的实现（转换过程详见t_zset.c的zsetConvert）。那么到底插入多少才会转呢？ 在redis.conf中的ADVANCED CONFIG部分的两个Redis配置12zset-max-ziplist-entries 128zset-max-ziplist-value 64 这个配置的意思是说，在如下两个条件之一满足的时候，ziplist会转成zset（具体的触发条件参见t_zset.c中的zaddGenericCommand相关代码）： 当sorted set中的元素个数，即(数据, score)对的数目超过128的时候，也就是ziplist数据项超过256的时候。 当sorted set中插入的任意一个数据的长度超过了64的时候。 最后，zset结构的代码定义如下：1234typedef struct zset &#123; dict *dict; zskiplist *zsl;&#125; zset; Redis为什么用skiplist而不用平衡树？在前面我们对于skiplist和平衡树、哈希表的比较中，其实已经不难看出Redis里使用skiplist而不用平衡树的原因了。现在我们看看，对于这个问题，Redis的作者 @antirez 是怎么说的： There are a few reasons: They are not very memory intensive. It’s up to you basically. Changing parameters about the probability of a node to have a given number of levels will make then less memory intensive than btrees. A sorted set is often target of many ZRANGE or ZREVRANGE operations, that is, traversing the skip list as a linked list. With this operation the cache locality of skip lists is at least as good as with other kind of balanced trees. They are simpler to implement, debug, and so forth. For instance thanks to the skip list simplicity I received a patch (already in Redis master) with augmented skip lists implementing ZRANK in O(log(N)). It required little changes to the code. 这段话原文出处： https://news.ycombinator.com/item?id=1171423这里从内存占用、对范围查找的支持和实现难易程度这三方面总结的原因]]></content>
      <tags>
        <tag>Redis</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[死锁]]></title>
    <url>%2F2018%2F10%2F21%2Fdead-lock%2F</url>
    <content type="text"><![CDATA[“死锁”的含义所谓死锁： 是指两个或两个以上的进程在执行过程中，由于竞争资源或者由于彼此通信而造成的一种阻塞的现象，若无外力作用，它们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁，这些永远在互相等待的进程称为死锁进程。 Java代码举例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public class Deadlock &#123; private Object o1 = new Object(); private Object o2 = new Object(); public void lock1() &#123; // 获取o1对象锁 synchronized( o1 ) &#123; try &#123; System.out.println( "l1 lock o1" ); // 获取o1后先等一会儿，让Lock2有足够的时间锁住o2 Thread.sleep( 1000 ); // 接着获取o2对象锁 synchronized( o2 ) &#123; System.out.println( "l1 lock o2" ); &#125; &#125; catch( Exception e ) &#123; e.printStackTrace(); &#125; &#125; &#125; public void lock2() &#123; // 获取o2对象锁 synchronized( o2 ) &#123; try &#123; System.out.println( "l2 lock o2" ); // 获取o2后先等一会儿，让Lock1有足够的时间锁住o1 Thread.sleep( 1000 ); // 接着获取o1对象锁 synchronized( o1 ) &#123; System.out.println( "l2 lock o1" ); &#125; &#125; catch( Exception e ) &#123; e.printStackTrace(); &#125; &#125; &#125; public static void main( String[] args ) &#123; Deadlock lock = new Deadlock(); Thread t1 = new Thread( new Runnable() &#123; @Override public void run() &#123; lock.lock1(); &#125; &#125; ); Thread t2 = new Thread( new Runnable() &#123; @Override public void run() &#123; lock.lock2(); &#125; &#125; ); t1.start(); t2.start(); &#125;&#125; 创建两个对象，两条线程，用synchronized锁住对象，线程1先锁对象1后锁对象2，线程2先锁对象2后锁对象1。假设线程1先锁对象1，然后休眠1秒，线程锁对象2，之后线程1就没法锁对象2，线程2也没法锁住对象1，双方都在等待对方释放自己在等待的锁。 “死锁”产生的原因及四个必要条件“死锁”的原因可归结为 竞争资源。当系统中供多个进程共享的资源如打印机、公用队列等，其数目不足以满足进程的需要时，会引起诸进程的竞争而产生死锁。 进程间推进顺序非法。进程在运行过程中，请求和释放资源的顺序不当，也同样会导致产生进程死锁。 产生“死锁”的四个必要条件 互斥（Mutual exclusion）：存在这样一种资源，它在某个时刻只能被分配给一个执行绪（也称为线程）使用； 持有（Hold and wait）：当请求的资源已被占用从而导致执行绪阻塞时，资源占用者不但没有释放该资源，而且还可以继续请求更多资源； 不可剥夺（No preemption）：执行绪获得到的互斥资源不可被强行剥夺，换句话说，只有资源占用者自己才能释放资源； 环形等待（Circular wait）：若干执行绪以不同的次序获取互斥资源，从而形成环形等待的局面，想象在由多个执行绪组成的环形链中，每个执行绪都在等待下一个执行绪释放它持有的资源。 结合代码例子理解“死锁”的产生 互斥：两条线程各自占有的锁 持有：线程1持有线程2想要获得的锁1，线程2持有线程1想要的锁2，双方都没有 释放各自占有的对象锁，并且继续请求对方占有的锁 不可剥夺：两条线程得到互斥资源都没法被强行剥夺 环形等待：T1{O1}→→T2{O2}→→T1{O1}，{}表示被左边的线程占有{}里的资源，→→表示左边线程申请（等待）右边线程释放其占有的资源PS：环形等待可以是多个线程对多个资源的争夺 “死锁”问题定位获取java进程ID1ps aux | grep "java" 用jstack看进程堆栈12# 替换进程ID（pid）jstack -l &#123;pid&#125; | grep -A50 -B10 "deadlock" “死锁”的预防和解除理解了死锁的原因，尤其是产生死锁的四个必要条件，就可以最大可能地避免、预防和解除死锁，消除产生死锁的四个必要条件中的任何一个都可以预防和解除死锁。不难看出，在死锁的四个必要条件中，第二、三和四项条件比较容易消除。 静态分配：采用资源静态分配策略（进程资源静态分配方式是指一个进程在建立时就分配了它需要的全部资源），破坏”部分分配”条件； 可剥夺：允许进程剥夺使用其他进程占有的资源，从而破坏”不可剥夺”条件； 有序分配：采用资源有序分配法，破坏”环路”条件 参考资料“死锁”四个必要条件的合理解释]]></content>
      <tags>
        <tag>线程</tag>
        <tag>并发</tag>
        <tag>锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis基本数据结构-SDS]]></title>
    <url>%2F2018%2F10%2F14%2FRedis-DataStructure-1-String%2F</url>
    <content type="text"><![CDATA[String&emsp;&emsp;String是我们最常用的Redis基本数据结构之一。Redis没有直接使用C语言传统的字符串表示（以空字符结尾的字符数组，以下简称C字符串），而是自己构建了一种名为简单动态字符串（simple dynamic string，SDS）的抽象类型，并将SDS用作Redis的默认字符串表示。&emsp;&emsp;在Redis里边，C字符串只会作为字符串字面量用在一些无须对字符串值进行修改的地方，比如打印日志：1redisLog(REDIS_WARNING, &quot;Redis is now ready to exit, bye bye...&quot;); SDS与C字符串的区别常数级复杂度获取字符串长度&emsp;&emsp;C语言使用长度为N+1的字符串来表示长度为N的字符串，并且字符数组的最后一个元素总是空字符’\0’&emsp;&emsp;因为C字符串并不记录自身的长度信息，所以为了获取一个C字符串的长度，程序必须便利整个字符数组，对遇到的每个字符进行技术，直到遇到代表字符串结尾的空字符为止，这个操作的复杂度为O(N)&emsp;&emsp;和C字符串不同，因为SDS在len属性中记录了SDS本身的长度，所以获取一个SDS长度的复杂度仅为O(1)，这确保了获取字符串长度的操作不会成为Redis的性能瓶颈。譬如”Redis”的长度为5，程序只需要访问SDS的len属性就可以立即得到长度值为5字节 杜绝缓冲区溢出&emsp;&emsp;由于C字符串不记录自身长度，会带来另一个问题，就是容易造成缓冲区溢出。1char *strcat(char *dest, const char *src) &emsp;&emsp;假定用户在执行strcat函数时，已经为dest分配了足够多的内存，则可以容纳src字符串中的所有内容，而一旦这个假定不成立时，就会产生缓冲区溢出。&emsp;&emsp;举个例子，假设程序里有两个在内存中紧邻着的C字符串s1和s2，其中s1保存了字符串”Redis”，而s2则保存了字符串”MongoDB”，如图所示： &emsp;&emsp;如果此时要通过执行：1strcat(s1, &quot; Cluster&quot;); 将s1的内容修改为”Redis Cluster”，但如果粗心的他却忘了在执行strcat之前为s1分配足够的空间，那么在strcat函数执行之后，s1的数据将溢出到s2所在的空间中，导致s2的内容被意外地修改了，如图所示： &emsp;&emsp;SDS的空间分配策略则完全杜绝了发生这种情况的可能性：当SDS API需要对SDS的内容进行修改时，API会先检查SDS的空间是否满足修改所需的要求，如果不满足的话，API会自动把SDS的空间扩展至执行修改所需的大小，然后才执行实际的修改操作，所以使用SDS既不需要手动修改SDS的空间大小，也不会出现上述的缓冲区溢出问题。 减少修改字符串时带来的内容重分配次数&emsp;&emsp;因为C字符串并不记录自身的长度，所以一个C字符串的底层实现总是额外的多出一个字符空间用于保存空字符。因为C字符串的长度和底层数组长度之间存在着这种关联性，所以每次增长或缩短一个C字符串，都总会在保存这个C字符串的数组时引起一次内存重分配操作 &emsp;&emsp;而因为内存重分配涉及复杂的算法，并且可能需要执行系统调用，所以它通常是一个比较耗时的操作： 在一般程序中，如果修改字符串长度的情况不太常出现，那么每次修改都执行一次内存重分配是可以接受的，但Redis作为数据库，经常被用于速度要求严苛、数据被频繁修改的场合，如果每次修改字符串的长度都需要执行一次内存重分配的话，那么光是这个操作的时间就会占去修改字符串所用时间的一大部分，如果这种修改频繁地发生的话，可能还会对性能造成影响 &emsp;&emsp;为了避免C字符串的这种缺陷，SDS通过未使用空间解除了字符串长度和底层数组长度之间的关联：在SDS中，buf数组的长度不一定就是字符数加1，数组里边可以包含未使用的字节，而这些字节的数量就由SDS的free属性记录&emsp;&emsp;通过未使用空间，SDS实现了空间预分配和惰性空间释放两种优化策略： 空间预分配&emsp;&emsp;空间预分配用于优化SDS的字符串增长操作：利用额外的未使用空间进行预分配以减少内存的频繁分配，这一点类似Java中的ArrayList。&emsp;&emsp;当字符串长度小于 1M 时，扩容都是加倍现有的空间，如果超过 1M，扩容时一次只会多扩 1M 的空间。而字符串最大长度为 512M。譬如，假如SDS进行修改后变为13字节（小于1MB），那么此时SDS的buf数组的实际长度将变成13+13+1=27字节（额外的1字节用于保存空字符）。假如SDS进行修改后变为2MB（大于等于1MB），则程序将会分配1MB的未使用空间，也就是说，SDS的buf数组的实际长度将为2MB + 1MB + 1byte。 惰性空间释放&emsp;&emsp;当要缩短SDS保存的字符串时，程序并不立即使用内存充分配来回收缩短后多出来的字节，而是使用表头的free成员将这些字节记录起来以备用。 二进制安全&emsp;&emsp;SDS是二进制安全的，它可以存储任意二进制数据，因为SDS使用len属性的值而不是像C语言字符串那样以空字符（‘\0’）来标识字符串结束。 &emsp;&emsp;因为传统C字符串符合某种编码（比如ASCII），字符串不仅末尾，就连字符串里的内容也不能包含标记着结束的字符。如ASCII这种编码的操作的特点就是：遇零则止。即，当读一个字符串时，只要遇到’\0’结尾，就认为到达末尾，就忽略’\0’结尾以后的所有字符。因此，如果传统字符串保存图片、音频、视频等二进制文件，操作文件时就被截断了。 兼容部分C字符串函数&emsp;&emsp;虽然SDS的API都是二进制安全的，但它们一样遵循C字符串结尾的惯例：这些API总会将SDS保存的数据的末尾设置为空字符，并且总会在为buf数组分配空间时多分配一个字节来容纳这个空字符，这是为了让那些保存文本数据的SDS可以重用一部分&lt;string.h&gt;库定义的函数，避免不必要的代码重复。 总结]]></content>
      <tags>
        <tag>Redis</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo添加搜索功能]]></title>
    <url>%2F2018%2F10%2F14%2FHexo-Search%2F</url>
    <content type="text"><![CDATA[本文旨在记录站主基于hexo-generator-search插件实现本站的站内文章搜索功能 基本实现原理 基于hexo-generator-search生成全文内容索引xml文件 利用jQ.ajax请求xml文件并解析 jQ搜索关键字内容匹配xml内容主要的部分还是插件写的好，对应的解析函数也是改造插件作者的，网上一搜一大堆此类文章，本文仅仅意在记录本站使用该插件实现搜索的过程 安装插件1npm install --save hexo-generator-search 这个插件可以生成供搜索的索引数据，生成后的xml文件保存在自己站内目录，可以通过 http://localhost:4000/search.xml 查看 插件配置在hexo根目录底下的_config.xml里加入以下配置：1234search: path: search.xml field: post #field: post, page or all（3个可选参数） 解析函数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960var performLocalSearch = function(datas,keywords)&#123; // perform local searching var str='&lt;ul class="search-result-list"&gt;'; datas.forEach(function(data) &#123; var isMatch = true; var content_index = []; var data_title = data.title.trim().toLowerCase(); var data_content = data.content.trim().replace(/&lt;[^&gt;]+&gt;/g,"").toLowerCase(); var data_url = "/" + data.url; var index_title = -1; var index_content = -1; var first_occur = -1; // only match artiles with not empty titles and contents if(data_title != '' &amp;&amp; data_content != '') &#123; keywords.forEach(function(keyword, i) &#123; index_title = data_title.indexOf(keyword); index_content = data_content.indexOf(keyword); if( index_title &lt; 0 &amp;&amp; index_content &lt; 0 )&#123; isMatch = false; &#125; else &#123; if (index_content &lt; 0) &#123; index_content = 0; &#125; if (i == 0) &#123; first_occur = index_content; &#125; &#125; &#125;); &#125; // show search results if (isMatch) &#123; str += '&lt;li&gt;&lt;a href="'+ data_url +'" class="search-result-title" target="_blank"&gt;'+ '&gt; ' + data_title +'&lt;/a&gt;'; var content = data.content.trim().replace(/&lt;[^&gt;]+&gt;/g,""); if (first_occur &gt;= 0) &#123; // cut out characters var start = first_occur - 6; var end = first_occur + 6; if(start &lt; 0)&#123; start = 0; &#125; if(start == 0)&#123; end = 10; &#125; if(end &gt; content.length)&#123; end = content.length; &#125; var match_content = content.substr(start, end); // highlight all keywords keywords.forEach(function(keyword)&#123; var regS = new RegExp(keyword, "gi"); match_content = match_content.replace(regS, '&lt;em class=\"search-keyword\"&gt;'+keyword+'&lt;/em&gt;'); &#125;); str += '&lt;p class=\"search-result\"&gt;' + match_content +'...&lt;/p&gt;'; &#125; &#125; &#125;); return str; &#125; Search入口功能函数12345678910111213141516171819202122232425262728293031 var searchFunc = function(path, search_id, content_id) &#123; 'use strict'; $.ajax(&#123; url: path, dataType: "xml", success: function( xmlResponse ) &#123; // get the contents from search data var datas = $( "entry", xmlResponse ).map(function() &#123; return &#123; title: $( "title", this ).text(), content: $("content",this).text(), url: $( "url" , this).text() &#125;; &#125;).get(); var $input = $('#'+search_id); var $resultContent = $('#'+content_id); $input.on("input propertychange",function()&#123; var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/); if($(this).val()=="")&#123;$resultContent.html('');return;&#125; $resultContent.html(performLocalSearch(datas,keywords)); &#125;); &#125; &#125;); &#125; searchFunc("/search.xml","local-search-input","local-search-result");&#125;)(); 页面HTML代码申明html元素，id、class名字要跟解析函、Search入口功能函数里的代码对上号：12345 &lt;div id="site_search" class="bar"&gt; &lt;input type="text" id="local-search-input" name="q" results="0" placeholder="search my blog..." class="form-control"/&gt; &lt;div id="local-search-result"&gt;&lt;/div&gt; &lt;/div&gt;` 样式调整1234567891011121314151617181920212223242526272829303132ul.search-result-list &#123; padding-left: 10px;&#125;a.search-result-title &#123; font-weight: bold;&#125;p.search-result &#123; color=#555;&#125;em.search-keyword &#123; border-bottom: 1px dashed #4088b8; font-weight: bold;&#125;.form-control &#123; padding-left:10px; margin-bottom: 10px; &#125;.bar &#123; padding: 10px 10px;&#125;.bar input &#123; width:350px; height: 25px; border-radius:42px; border:2px solid #324B4E; background:#F9F0DA; transition:.3s linear; float:center;&#125;.bar input:focus &#123; width:420px;&#125;]]></content>
      <tags>
        <tag>HEXO</tag>
        <tag>搜索</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java线程池类比公司经营之道]]></title>
    <url>%2F2018%2F10%2F03%2FJava-ThreadPool-vs-Company%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;Java线程池的设计与公司经营的相似之处如果我们查看JDK源码，会发现FixedThreadPool、CachedThreadPool和SingleThreadExecutor都是通过创建一个ThreadPoolExcutor对象来实现的。我们来看一下该ThreadPoolExcutor的构造方法，并对线程池中线程的保留和新建策略做进一步的分析。 1public ThreadPoolExecutor( int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue) &emsp;&emsp;第一个参数corePoolSize代表了线程池中一定要保持的线程的数量；线程池中的线程可能发生变化，第二个参数maximumPoolSize约束了线程池中所能达到的线程的最大数量；线程有可能一直处于空闲状态，keepAliveTime代表了空闲状态的线程所能存活的时间；TimeUnit代表了时间单位；workQueue是一个缓冲队列，如果任务到达，但是还没有空闲线程可以执行该任务，那么就将该任务置于这个缓冲队列中。为了更加容易理解和记忆线程池这个几个属性的协调工作。我们利用一个精明的老板来比喻线程池。而将线程比作线程中的线程。 &emsp;&emsp;一个公司必须要保留一定数量的核心员工，不管这些员工是不是老闲着。当然，对于非常抠门的老板，这个数量可能是0，例如CachedThreadPool。核心员工的数量，就是corePoolSize。当一个公司初创时，所有的员工也就是那几个核心员工。当线程池新建时，同样只会创建与corePoolSize数量相当的线程。 &emsp;&emsp;当新的任务到达时，如果有空闲线程，马上将这些任务分配给空闲线程。如果没有的话，那么，怎么办呢？新建一个线程吗？非也，对于一个精明的老板来说，他只会把这些任务排进任务列表。手下的员工忙完手头的工作，马上就从任务列表的开头位置移出工作，并分配给空闲。这就让每名员工都不停的工作，甚至加班加点。这个任务列表就是workQueue。 &emsp;&emsp;如果更多的任务涌过来，如同这个公司的业务很好，工作多越堆越多。这个时候，就看任务列表能承受的极限了。有的老板在创立公司的时候，就抱着这种心态——任务列表可以无限长，反正我就招这么多人，客户能等就等，不能等就拉倒。但是，对于很多客户来说，如果等的时间过长，可能就放弃了。具有无限长workQueue的线程池来说，可能同样会导致某些线程等待时间过长，用户任务无响应的问题。 &emsp;&emsp;但是，如果workQueue不是无限长，那么，其容量总有可能被达到。而新的任务到达时，无法存入workQueue。这如同，这个老板既负责任（不想出现客户无限等待的情况），同时又不想放弃任何一个客户。那么，唯有增加员工数量了，这就如同线程池新建线程。但是，公司总要有个风险评估，不能让员工数量无限增长，于是，maximumPoolSize就代表了员工的最大数量。如同说，在无法两全其美的情形下，即使损失部分客户，也要控制公司的成本风险。线程池同样如此，每个线程都将消耗系统资源，这种消耗必须被控制在一定范围之内。 &emsp;&emsp;在大量任务涌入，workQueue无法缓存这些任务，而maxinumPoolSize也已经达到时，相当于一个公司达到了它的最大营运能力，就只能拒绝介绍客户任务了。线程池拒绝介绍新的任务，会抛出异常RejectedExecutionException。 &emsp;&emsp;当然，一个公司的营运既有旺季，也有淡季。上面我们所描述的情形是旺季的营运。如果淡季到了，许多员工都闲下来了。老板就会考虑裁员了。当然，老板不会马上动手，因为不能准确把握旺季和淡季的分界线。他会给空闲员工一个缓冲期，如果这个员工闲了三个月都没工作，那么证明，真的需要裁掉他了。对应到线程池中，keepAliveTime和TimeUnit限制了一个线程的最大空闲时间。相当于一个缓冲期，缓冲期一结束，就会将其销毁，以释放系统资源。当然，这些被“处理”的线程都是核心员工数量之外的，线程池总会保留corePoolSize个线程备用。 &emsp;&emsp;通过以上描述，我们应该对线程池的运作策略有了一个比较清晰的认识。总结这种策略，主要目的是基于成本考虑——尽量耗用最少的内存，来完成尽可能多的任务。]]></content>
      <tags>
        <tag>线程</tag>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo Hello World]]></title>
    <url>%2F2018%2F10%2F03%2FHexo-First-Guide%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <tags>
        <tag>HEXO</tag>
      </tags>
  </entry>
</search>
