<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[RocketMQ相关总结]]></title>
    <url>%2F2018%2F11%2F18%2FRocketMQ-Summary%2F</url>
    <content type="text"><![CDATA[总结RocketMQ相关知识点，便于回顾记忆… Rocket官网地址https://rocketmq.incubator.apache.org/ RocketMQ总体特点 能够保证严格的消息顺序 提供丰富的消息拉取模式 高效的订阅者水平扩展能力 实时的消息订阅机制 亿级消息堆积能力 核心原理数据结构主要以commitLog为消息存储的数据结构。 （1）所有数据单独储存到commit Log ，完全顺序写，随机读 （2）对最终用户展现的队列实际只储存消息在Commit Log 的位置信息，并且串行方式刷盘 （3）按照MessageId查询消息 （4）根据查询的key的hashcode%slotNum得到具体的槽位置 （5）根据slotValue（slot对应位置的值）查找到索引项列表的最后一项 （6）遍历索引项列表返回查询时间范围内的结果集 刷盘策略作为一款纯 Java 语言开发的消息引擎，RocketMQ 自主研发的存储组件，依赖 Page Cache 进行加速和堆积，意味着它的性能会受到 JVM、 GC、内核、Linux 内存管理机制、文件 IO 等因素的影响。Rocketmq中的所有消息都是持久化到硬盘的，但会使用系统PageCache加速访问，消息的落地方式是先写PageCache后刷盘，可以保证内存与磁盘都有一份数据，访问时，可以直接从内存读取。如图所示，一条消息从客户端发送出，到最终落盘持久化，每个环节都有产生延迟的风险。 《不一样的技术创新-阿里巴巴2016双十一背后的技术》一书中提到，有线上数据显示，RocketMQ 写消息链路存在偶发的高达数秒的延迟 同步刷盘同步刷盘是指，broker在收到每个消息后，都是先要保存到硬盘上，然后再给producer确认。 异步刷盘异步刷盘就是先回复确认，然后批量保存到硬盘上。异步刷盘有更好的性能，当然也有更大的丢失消息的风险。 角色关系图 架构图 架构特点所有的集群都具有水平扩展能力，无单点障碍。 NameServer以轻量级的方式提供服务发现和路由功能，每个NameServer存有全量的路由信息，提供对等的读写服务，是一个几乎无状态节点，可集群部署，节点之间无任何信息同步，支持快速扩缩容。 Broker为实际的消息队列服务器(MQ Server)，在整体架构中，可以看作是Producer与Comsumer之间的驳脚者，消息通过它从Producer接收，并存储，后转发给Consumer。以Topic为纬度支持轻量级的队列，单机可以支撑上万队列规模，支持消息推拉模型，具备多副本容错机制（2副本或3副本）、强大的削峰填谷以及上亿级消息堆积能力，同时可严格保证消息的有序性。 除此之外，Broker还提供了同城异地容灾能力，丰富的Metrics统计以及告警机制。这些都是传统消息系统无法比拟的。 Producer由用户进行分布式部署，消息由Producer通过多种负载均衡模式发送到Broker集群，发送低延时，支持快速失败。 Consumer也由用户部署，支持PUSH和PULL两种消费模式（推模式的实现也是用的拉方式），支持集群消费和广播消息，提供实时的消息订阅机制，满足大多数消费场景。 RocketMQ亮点-支持多种消费模式RocketMQ最初还未正式称为RocketMQ，一开始v1.0还是叫metaQ，经历了3代的重要演进，v3.0开始改名RocketMQ，其重要改进包括消息获取模式。 第一代，推模式，数据存储采用关系型数据库。在这种模式下，消息具有很低的延迟特性，并且很容易支持分布式事务。尤其在阿里淘宝这种高频交易场景中，具有非常广泛地应用。典型代表包括Notify、Napoli。 第二代，拉模式，自研的专有消息存储。在日志处理方面能够媲美Kafka的吞吐性能，但考虑到淘宝的应用场景，尤其是其交易链路的高可靠需求，消息引擎并没有一味的追求吞吐，而是将稳定可靠放在首位。因为采用了长连接拉模式，在消息的实时方面丝毫不逊推模式。典型代表MetaQ。 第三代，以拉模式为主，兼有推模式的高性能、低延迟消息引擎RocketMQ，在二代功能特性的基础上，为电商金融领域添加了可靠重试、基于文件存储的分布式事务等特性，并做了大量优化。从2012年开始，经历了历次双11核心交易链路检验。目前已经捐赠给Apache基金会。 不难看出，RocketMQ其实是伴随着阿里巴巴整个生态的成长，逐渐衍生出来的高性能，高可用，兼具高吞吐量和低延迟、能够同时满足电商领域和金融领域的极尽苛刻场景的消息中间件。 Broker部署方式单Master 这种方式风险较大，一旦Broker 重启或者宕机时，会导致整个服务不可用，不建议线上环境使用。 多Master模式多台Broker，全是Master 优点：配置简单，单个Master 宕机或重启维护对应用无影响，在磁盘配置为 RAID10 时，即使机器宕机不可恢复情况下，由于RAID10 磁盘非常可靠，消息也不会丢（异步刷盘丢失少量消息，同步刷盘一条不丢）。性能最高。缺点：单台机器宕机期间，这台机器上未被消费的消息在机器恢复之前不可订阅，消息实时性会受到受到影响。 HA方案：多Master/Slave对模式每个 Master 配对一个 Slave，有多对Master-Slave。 Master/Slave复制方式 同步双写写入消息时，master先写入，之后复制到slave，确认slave也存储了消息后才向producer答复返回成功。 优点：数据与服务都无单点，Master宕机情况下，消息无延迟，服务可用性与数据可用性都非常高缺点：性能比异步复制模式略低，大约低10%左右，发送单个消息的 RT 会略高。目前主宕机后，备机不能自动切换为主机，后续会支持自动切换功能。 异步复制先答复producer，再去向salve复制。 优点：即使磁盘损坏，消息丢失的非常少，且消息实时性不会受影响，因为 Master 宕机后，消费者仍然可以从 Slave 消费，此过程对应用透明。不需要人工干预。性能同多 Master 模式几乎一样。缺点：Master宕机，磁盘损坏情况，会丢失少量消息。 通过同步复制技术可以完全避免单点，同步复制势必会影响性能，适合应用于消息可靠性要求极高的场合。RocketMQ从3.0版本开始支持同步双写。 两种消息消费的交互方式的区别留意源码可以得知：consumer被分为2类：MQPullConsumer和MQPushConsumer，本质都是拉模式（pull），即consumer轮询从broker拉取消息。区别在于： push方式里，consumer把轮询过程封装了，并注册MessageListener监听器，取到消息后，唤醒MessageListener的consumeMessage()来消费，对客户端而言，感觉消息是被推送（push）过来的。 pull方式里，取消息的过程，RocketMQ交给了用户自己实现，首先通过待消费的Topic拿到MessageQueue的集合，遍历MessageQueue集合，然后针对每个MessageQueue批量取消息，一次取完后，记录该队列下一次要取的开始offset，直到取完了，再换另一个MessageQueue。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879public class Consumer &#123; // Java缓存 private static final Map&lt;MessageQueue, Long&gt; offseTable = new HashMap&lt;MessageQueue, Long&gt;(); private static final String nameServAndAddr = "172.16.235.77:9876;172.16.235.78:9876"; private static final String consumerGroupName ="ConsumerGroupName"; private static final String consumber ="Consumber"; /** * 主动拉取方式消费 * * @throws MQClientException */ public static void main(String[] args) throws MQClientException &#123; /** * 一个应用创建一个Consumer，由应用来维护此对象，可以设置为全局对象或者单例&lt;br&gt; * 注意：ConsumerGroupName需要由应用来保证唯一 ,最好使用服务的包名区分同一服务,一类Consumer集合的名称， * 这类Consumer通常消费一类消息，且消费逻辑一致 * PullConsumer：Consumer的一种，应用通常主动调用Consumer的拉取消息方法从Broker拉消息，主动权由应用控制 */ DefaultMQPullConsumer consumer = new DefaultMQPullConsumer(consumerGroupName); // //nameserver服务 consumer.setNamesrvAddr(nameServAndAddr); consumer.setInstanceName(consumber); consumer.start(); // 拉取订阅主题的队列，默认队列大小是4 Set&lt;MessageQueue&gt; mqs = consumer.fetchSubscribeMessageQueues("TopicTest1"); for (MessageQueue mq : mqs) &#123; System.out.println("Consume from the queue: " + mq); SINGLE_MQ: while (true) &#123; try &#123; PullResult pullResult = consumer.pullBlockIfNotFound( mq, null, getMessageQueueOffset(mq), 32); List&lt;MessageExt&gt; list = pullResult.getMsgFoundList(); if (list != null &amp;&amp; list.size() &lt; 100) &#123; for (MessageExt msg : list) &#123; System.out.println(new String(msg.getBody())); &#125; &#125; System.out.println(pullResult.getNextBeginOffset()); putMessageQueueOffset(mq, pullResult.getNextBeginOffset()); switch (pullResult.getPullStatus()) &#123; case FOUND: break; case NO_MATCHED_MSG: break; case NO_NEW_MSG: break SINGLE_MQ; case OFFSET_ILLEGAL: break; default: break; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; consumer.shutdown(); &#125; private static void putMessageQueueOffset(MessageQueue mq, long offset) &#123; offseTable.put(mq, offset); &#125; private static long getMessageQueueOffset(MessageQueue mq) &#123; Long offset = offseTable.get(mq); if (offset != null) &#123; System.out.println(offset); return offset; &#125; return 0; &#125; &#125; RocketMQ使用长轮询Pull方式，可保证消息非常实时，消息实时性不低于Push 长轮询Pull：建立长连接，每隔一定时间，客户端向服务端发起请求询问数据，如有则返回数据，如无则返回空，然后关闭请求。长轮询与普通轮询的不同之处在于，哪怕服务端此时没有数据，连接还是保持的，等到有数据时可以立即返回（也就模拟push），或者超时返回。长轮询好处在于可以减少无效请求，保证消息实时性获取，又不会造成积压。 推拉模式的具体选取视乎实际情况而定，在一些离线大批量数据处理系统中，消息获取的需求强调的更多是吞吐量，而非低延迟，此时拉模式可能更优。 RocketMQ 高可用保障通过可用性计算公式可以看出，要提升系统的可用性，需要在保障系统健壮性以延长平均无故障时间的基础上，进一步加强系统的故障自动恢复能力以缩短平均故障修复时间。 RocketMQ 高可用架构设计并实现了 Controller 组件，按照单主状态、异步复制状态、半同步状态以及最终的同步复制状态的有限状态机进行转换。在最终的同步复制状态下，Master 和 Slave 任一节点故障时，其它节点能够在秒级时间内切换到单主状态继续提供服务。相比于之前人工介入重启来恢复服务，RokcetMQ 高可用架构赋予了系统故障自动恢复的能力，能极大缩短平均故障恢复时间，提升系统的可用性。 下图描述了 RocketMQ 高可用架构中有限状态机的转换： 1） 第一个节点启动后，Controller 控制状态机切换为单主状态，通知启动节点以 Master 角色提供服务。2） 第二个节点启动后， Controller 控制状态机切换成异步复制状态。Master 通过异步方式向 Slave 复制数据。3） 当 Slave 的数据即将赶上 Master，Controller 控制状态机切换成半同步状态，此时命中 Master 的写请求会被 Hold 住，直到 Master以异步方式向 Slave 复制了所有差异的数据。4） 当半同步状态下 Slave 的数据完全赶上 Master 时，Controller控制状态机切换成同步复制模式，Mater 开始以同步方式向 Slave 复制数据。该状态下任一节点出现故障，其它节点能够在秒级内切换到单主状态继续提供服务。Controller 组件控制 RocketMQ 按照单主状态，异步复制状态，半同步状态，同步复制状态的顺序进行状态机切换。中间状态的停留时间与主备之间的数据差异以及网络带宽有关，但最终都会稳定在同步复制状态下。 如何保证消息有序消费？消息有序指的是一类消息消费时，能按照发送的顺序来消费。例如：一个订单产生了 3 条消息，分别是订单创建、订单付款、订单完成。消费时，要按照这个顺序消费才有意义。但同时订单之间又是可以并行消费的。 假如生产者产生了2条消息：M1、M2，要保证这两条消息的顺序，应该怎样做？你脑中想到的可能是这样： M1发送到S1后，M2发送到S2，如果要保证M1先于M2被消费，那么需要M1到达消费端后，通知S2，然后S2再将M2发送到消费端。 这个模型存在的问题是，如果M1和M2分别发送到两台Server上，就不能保证M1先达到，也就不能保证M1被先消费，那么就需要在MQ Server集群维护消息的顺序。那么如何解决？一种简单的方式就是将M1、M2发送到同一个Server上： 这样可以保证M1先于M2到达MQServer（客户端等待M1成功后再发送M2），根据先达到先被消费的原则，M1会先于M2被消费，这样就保证了消息的顺序。 这个模型，理论上可以保证消息的顺序，但在实际运用中你应该会遇到下面的问题： 只要将消息从一台服务器发往另一台服务器，就会存在网络延迟问题。如上图所示，如果发送M1耗时大于发送M2的耗时，那么M2就先被消费，仍然不能保证消息的顺序。即使M1和M2同时到达消费端，由于不清楚消费端1和消费端2的负载情况，仍然有可能出现M2先于M1被消费。如何解决这个问题？将M1和M2发往同一个消费者即可，且发送M1后，需要消费端响应成功后才能发送M2。 但又会引入另外一个问题，如果发送M1后，消费端1没有响应，那是继续发送M2呢，还是重新发送M1？一般为了保证消息一定被消费，肯定会选择重发M1到另外一个消费端2，就如下图所示。 这样的模型就严格保证消息的顺序，细心的你仍然会发现问题，消费端1没有响应Server时有两种情况，一种是M1确实没有到达，另外一种情况是消费端1已经响应，但是Server端没有收到。如果是第二种情况，重发M1，就会造成M1被重复消费。也就是我们后面要说的第二个问题，消息重复问题。 回过头来看消息顺序问题，严格的顺序消息非常容易理解，而且处理问题也比较容易，要实现严格的顺序消息，简单且可行的办法就是： 保证生产者 - MQServer - 消费者是一对一对一的关系 但是这样设计，并行度就成为了消息系统的瓶颈（吞吐量不够），也会导致更多的异常处理，比如：只要消费端出现问题，就会导致整个处理流程阻塞，我们不得不花费更多的精力来解决阻塞的问题。 但我们的最终目标是要集群的高容错性和高吞吐量。这似乎是一对不可调和的矛盾，那么阿里是如何解决的？ 有些问题，看起来很重要，但实际上我们可以通过合理的设计或者将问题分解来规避。如果硬要把时间花在解决它们身上，实际上是浪费的，效率低下的。从这个角度来看消息的顺序问题，我们可以得出两个结论： 不关注乱序的应用实际大量存在 队列无序并不意味着消息无序 最后我们从源码角度分析RocketMQ怎么实现发送顺序消息。 一般消息是通过轮询所有队列来发送的（负载均衡策略），顺序消息可以根据业务，比如说订单号相同的消息发送到同一个队列。下面的示例中，OrderId相同的消息，会发送到同一个队列： 123456789// RocketMQ默认提供了两种MessageQueueSelector实现：随机/HashSendResult sendResult = producer.send(msg, new MessageQueueSelector() &#123; @Override public MessageQueue select(List&lt;MessageQueue&gt; mqs, Message msg, Object arg) &#123; Integer id = (Integer) arg; int index = id % mqs.size(); return mqs.get(index); &#125;&#125;, orderId); 在获取到路由信息以后，会根据MessageQueueSelector实现的算法来选择一个队列，同一个OrderId获取到的队列是同一个队列。 12345678910111213private SendResult send() &#123; // 获取topic路由信息 TopicPublishInfo topicPublishInfo = this.tryToFindTopicPublishInfo(msg.getTopic()); if (topicPublishInfo != null &amp;&amp; topicPublishInfo.ok()) &#123; MessageQueue mq = null; // 根据我们的算法，选择一个发送队列 // 这里的arg = orderId mq = selector.select(topicPublishInfo.getMessageQueueList(), msg, arg); if (mq != null) &#123; return this.sendKernelImpl(msg, mq, communicationMode, sendCallback, timeout); &#125; &#125;&#125; 消息重复–如何保证幂等性上面在解决消息顺序问题时，引入了一个新的问题，就是消息重复。那么RocketMQ是怎样解决消息重复的问题呢？还是“恰好”不解决。 造成消息的重复的根本原因是：网络不可达。只要通过网络交换数据，就无法避免这个问题。所以解决这个问题的办法就是不解决，转而绕过这个问题。那么问题就变成了：如果消费端收到两条一样的消息，应该怎样处理？ 消费端处理消息的业务逻辑保持幂等性 保证每条消息都有唯一编号且保证消息处理成功与去重表的日志同时出现 第1条很好理解，只要保持幂等性，不管来多少条重复消息，最后处理的结果都一样。第2条原理就是利用一张日志表来记录已经处理成功的消息的ID，如果新到的消息ID已经在日志表中，那么就不再处理这条消息。 我们可以看到第1条的解决方式，很明显应该在消费端实现，不属于消息系统要实现的功能。第2条可以消息系统实现，也可以业务端实现。正常情况下出现重复消息的概率不一定大，且由消息系统实现的话，肯定会对消息系统的吞吐量和高可用有影响，所以最好还是由业务端自己处理消息重复的问题，这也是RocketMQ不解决消息重复的问题的原因。 RocketMQ不保证消息不重复，如果你的业务需要保证严格的不重复消息，需要你自己在业务端去重。 事务消息RocketMQ除了支持普通消息，顺序消息，另外还支持事务消息。首先讨论一下什么是事务消息以及支持事务消息的必要性。我们以一个转帐的场景为例来说明这个问题：Bob向Smith转账100块。 在单机环境下，执行事务的情况，大概是下面这个样子： 当用户增长到一定程度，Bob和Smith的账户及余额信息已经不在同一台服务器上了，那么上面的流程就变成了这样： 这时候你会发现，同样是一个转账的业务，在集群环境下，耗时居然成倍的增长，这显然是不能够接受的。那我们如何来规避这个问题？ 大事务 = 小事务 + 异步 将大事务拆分成多个小事务异步执行。这样基本上能够将跨机事务的执行效率优化到与单机一致。转账的事务就可以分解成如下两个小事务： 图中执行本地事务（Bob账户扣款）和发送异步消息应该保持同时成功或者失败中，也就是扣款成功了，发送消息一定要成功，如果扣款失败了，就不能再发送消息。那问题是：我们是先扣款还是先发送消息呢？ 首先我们看下，先发送消息，大致的示意图如下： 存在的问题是：如果消息发送成功，但是扣款失败，消费端就会消费此消息，进而向Smith账户加钱。 先发消息不行，那我们就先扣款呗，大致的示意图如下： 存在的问题跟上面类似：如果扣款成功，发送消息失败，就会出现Bob扣钱了，但是Smith账户未加钱。 可能大家会有很多的方法来解决这个问题，比如：直接将发消息放到Bob扣款的事务中去，如果发送失败，抛出异常，事务回滚。这样的处理方式也符合“恰好”不需要解决的原则。RocketMQ支持事务消息，下面我们来看看RocketMQ是怎样来实现的。 RocketMQ第一阶段发送Prepared消息时，会拿到消息的地址，第二阶段执行本地事物，第三阶段通过第一阶段拿到的地址去访问消息，并修改状态。细心的你可能又发现问题了，如果确认消息发送失败了怎么办？RocketMQ会定期扫描消息集群中的事物消息，这时候发现了Prepared消息，它会向消息发送者确认，Bob的钱到底是减了还是没减呢？如果减了是回滚还是继续发送确认消息呢？RocketMQ会根据发送端设置的策略来决定是回滚还是继续发送确认消息。这样就保证了消息发送与本地事务同时成功或同时失败。 那我们来看下RocketMQ源码，是不是这样来处理事务消息的。客户端发送事务消息的部分（完整代码请查看：rocketmq-example工程下的com.alibaba.rocketmq.example.transaction.TransactionProducer）： 12345678910111213141516// 未决事务，MQ服务器回查客户端// 也就是上文所说的，当RocketMQ发现`Prepared消息`时，会根据这个Listener实现的策略来决断事务TransactionCheckListener transactionCheckListener = new TransactionCheckListenerImpl();// 构造事务消息的生产者TransactionMQProducer producer = new TransactionMQProducer("groupName");// 设置事务决断处理类producer.setTransactionCheckListener(transactionCheckListener);// 本地事务的处理逻辑，相当于示例中检查Bob账户并扣钱的逻辑TransactionExecuterImpl tranExecuter = new TransactionExecuterImpl();producer.start()// 构造MSG，省略构造参数Message msg = new Message(......);// 发送消息SendResult sendResult = producer.sendMessageInTransaction(msg, tranExecuter, null);producer.shutdown(); 接着查看sendMessageInTransaction方法的源码，总共分为3个阶段：发送Prepared消息、执行本地事务、发送确认消息。 12345678910public TransactionSendResult sendMessageInTransaction(.....) &#123; // 逻辑代码，非实际代码 // 1.发送消息 sendResult = this.send(msg); // sendResult.getSendStatus() == SEND_OK // 2.如果消息发送成功，处理与消息关联的本地事务单元 LocalTransactionState localTransactionState = tranExecuter.executeLocalTransactionBranch(msg, arg); // 3.结束事务 this.endTransaction(sendResult, localTransactionState, localException);&#125; endTransaction方法会将请求发往broker(mq server)去更新事物消息的最终状态： 根据sendResult找到Prepared消息 根据localTransaction更新消息的最终状态 如果endTransaction方法执行失败，导致数据没有发送到broker，broker会有回查线程定时（默认1分钟）扫描每个存储事务状态的表格文件，如果是已经提交或者回滚的消息直接跳过，如果是prepared状态则会向Producer发起CheckTransaction请求，Producer会调用DefaultMQProducerImpl.checkTransactionState()方法来处理broker的定时回调请求，而checkTransactionState会调用我们的事务设置的决断方法，最后调用endTransactionOneway让broker来更新消息的最终状态。 再回到转账的例子，如果Bob的账户的余额已经减少，且消息已经发送成功，Smith端开始消费这条消息，这个时候就会出现消费失败和消费超时两个问题？解决超时问题的思路就是一直重试，直到消费端消费消息成功，整个过程中有可能会出现消息重复的问题，按照前面的思路解决即可。 这样基本上可以解决超时问题，但是如果消费失败怎么办？阿里提供给我们的解决方法是：人工解决。大家可以考虑一下，按照事务的流程，因为某种原因Smith加款失败，需要回滚整个流程。如果消息系统要实现这个回滚流程的话，系统复杂度将大大提升，且很容易出现Bug，估计出现Bug的概率会比消费失败的概率大很多。我们需要衡量是否值得花这么大的代价来解决这样一个出现概率非常小的问题，这也是大家在解决疑难问题时需要多多思考的地方。 在3.2.6版本中移除了事务消息的实现，所以此版本不支持事务消息，具体情况请参考rocketmq的issues：https://github.com/alibaba/RocketMQ/issues/65https://github.com/alibaba/RocketMQ/issues/138https://github.com/alibaba/RocketMQ/issues/156 Reference 《不一样的技术创新-阿里巴巴2016双十一背后的技术》 RocketMq知识点理解 RocketMQ总结整理]]></content>
      <tags>
        <tag>消息队列</tag>
        <tag>分布式</tag>
        <tag>异步</tag>
        <tag>削峰</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[总结ThreadLocal]]></title>
    <url>%2F2018%2F11%2F05%2FThreadLocal-Summary%2F</url>
    <content type="text"><![CDATA[ThreadLoacal简介ThreadLocal类是修饰变量的，重点是在控制变量的作用域，初衷可不是为了解决线程并发和线程冲突的，而是为了让变量的种类变的更多更丰富，方便人们使用罢了。根据变量的作用域，可以将变量分为全局变量，局部变量。简单的说，类里面定义的变量是全局变量，函数里面定义的变量是局部变量。还有一种作用域是线程作用域，线程一般是跨越几个函数的。为了在几个函数之间共用一个变量，所以才出现：线程变量，这种变量在Java中就是ThreadLocal变量。ThreadLocal变量，不同于它们的普通对应物，因为访问某个变量（通过其get 或 set 方法）的每个线程都有自己的局部变量，它独立于变量的初始化副本。ThreadLocal为每个使用该变量的线程分配一个独立的变量副本。所以每一个线程都可以独立地改变自己的副本，而不会影响其他线程所对应的副本。所以我们说，ThreadLocal为解决多线程程序的并发问题提供了一种新的思路。 ThreadLocal的用处Web开发中常见到的一个问题：多用户session问题。假设有多个用户需要获取用户信息，一个线程对应一个用户。在mybatis中，session用于操作数据库，那么设置、获取操作分别是session.set()、session.get()，如何保证每个线程都能正确操作达到想要的结果呢？ 假如我们要设置一个变量，作为各个线程共享的变量，来存储session信息，那么当我们需要让每个线程独立地设置session信息而不被其它线程打扰，要怎么做呢？很容易想到了加锁，譬如synchronized，互斥同步锁synchronized自JDK1.5经过优化后，不会很消耗资源了，但当成千上万个操作来临之时，扛高并发能力不说，数据返回延迟带来的用户体验变差又如何解决？ 那么，就上文提出的问题，引申出来，像mybatis，hibernate一类的框架是如何解决这个session问题的呢？ 来看一下，mybatis的SqlSessionManager类：123456789101112131415161718192021222324252627282930313233343536373839404142public class SqlSessionManager implements SqlSessionFactory, SqlSession &#123; private final SqlSessionFactory sqlSessionFactory; private final SqlSession sqlSessionProxy; private ThreadLocal&lt;SqlSession&gt; localSqlSession = new ThreadLocal&lt;SqlSession&gt;(); private SqlSessionManager(SqlSessionFactory sqlSessionFactory) &#123; this.sqlSessionFactory = sqlSessionFactory; this.sqlSessionProxy = (SqlSession) Proxy.newProxyInstance( SqlSessionFactory.class.getClassLoader(), new Class[]&#123;SqlSession.class&#125;, new SqlSessionInterceptor()); &#125; ... public void startManagedSession() &#123; this.localSqlSession.set(openSession()); &#125; public void startManagedSession(boolean autoCommit) &#123; this.localSqlSession.set(openSession(autoCommit)); &#125; public void startManagedSession(Connection connection) &#123; this.localSqlSession.set(openSession(connection)); &#125; ... @Override public Connection getConnection() &#123; final SqlSession sqlSession = localSqlSession.get(); if (sqlSession == null) &#123; throw new SqlSessionException("Error: Cannot get connection. No managed session is started."); &#125; return sqlSession.getConnection(); &#125; ...&#125; 留意到，mybatis里的localSqlSession就是用的ThreadLocal变量来实现。 从内存模型出发看ThreadLocal： 我们知道，在虚拟机中，堆内存就是用于存储共享数据，也就是这里所说的主内存。 每个线程将会在堆内存中开辟一块空间叫做线程的工作内存，附带一块缓存区用于存储共享数据副本。那么，共享数据在堆内存当中，线程通信就是通过主内存为中介，线程在本地内存读并且操作完共享变量操作完毕以后，把值写入主内存。 ThreadLocal被称为线程局部变量，说白了就是线程工作内存的一小块内存，用于存储数据。 那么，ThreadLocal.set()、ThreadLocal.get()方法，就相当于把数据存储于线程本地，取也是在本地内存读取。就不会像synchronized需要频繁的修改主内存的数据，再把数据复制到工作内存，也大大提高访问效率。 那么，我们再来回答上面引出的问题，mybatis为什么要用ThreadLocal来存储session？ 首先，因为线程间的数据交互是通过工作内存与主存的频繁读写完成通信，然而存储于线程本地内存，提高访问效率，避免线程阻塞造成cpu吞吐率下降。再者，在多线程中，每一个线程都各自维护session，轻易完成对线程独享资源的操作。 理解ThreadLocal的关键源码首先，要理解ThreadLocal的数据结构，我们可以看它的set/get方法：ThreadLocal.java1234567891011121314151617181920public void set(T value) &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); &#125;...void createMap(Thread t, T firstValue) &#123; t.threadLocals = new ThreadLocalMap(this, firstValue);&#125;...ThreadLocalMap getMap(Thread t) &#123; return t.threadLocals;&#125; Thread.java1ThreadLocal.ThreadLocalMap threadLocals = null; ThreadLocalMap作为ThreadLocal的静态内部类，用于存储多个ThreadLocal对象 ThreadLocal对象作为ThreadLocalMap的key来存储，我们set进去的独享数据作为value存储 留意到它里边调到的getMap(Thread)方法，得知ThreadLocalMap的获取跟当前Thread有关，仔细看threadLocals其实就是当前线程的一个ThreadLocalMap变量。也就是说，一个线程对应一个ThreadLocalMap，get()就是当前程获取自己的ThreadLocalMap。 12345678910111213public T get() &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) &#123; ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; @SuppressWarnings("unchecked") T result = (T)e.value; return result; &#125; &#125; return setInitialValue(); &#125; 线程根据使用那一小块的线程本地内存，以ThreadLocal对象作为key，去获取存储于ThreadLocalMap中的值。 ThreadLocal内存泄露引用关系图 先引用一张经典的引用关系图来说明当前线程(currentThread)以及threadLocalMap、key、threadLocal实例几个之间的引用关系： 利用这图来回顾总结一下ThreadLocal的实现：每个Thread 维护一个 ThreadLocalMap 映射表，这个映射表的 key 是 ThreadLocal实例本身，value 是真正需要存储的 Object。 也就是说 ThreadLocal 本身并不存储值，它只是作为一个 key 来让线程从 ThreadLocalMap 获取 value。值得注意的是图中的虚线，表示 ThreadLocalMap 是使用 ThreadLocal 的弱引用作为 Key 的，弱引用的对象在 GC 时会被回收。 ThreadLocal为什么会内存泄漏我们可以理解到，每个线程都会创建一块工作内存，每个线程都有一个ThreadLocalMap，而ThreadLocalMap可以有多个key，也就是说可以存储多个ThreadLocal。那么假设，开启1万个线程，每个线程创建1万个ThreadLocal，也就是每个线程维护1万个ThreadLocal小内存空间！那么，当线程执行结束以后，如果一个ThreadLocal没有外部强引用来引用它而是用弱引用来引用，那么系统 GC 的时候，这个ThreadLocal势必会被回收，这样一来，ThreadLocalMap中就会出现key为null的Entry，就没有办法访问这些key为null的Entry的value，如果当前线程不结束的话，这些key为null的Entry的value就会一直存在一条强引用链：Thread Ref -&gt; Thread -&gt; ThreaLocalMap -&gt; Entry -&gt; value永远无法回收，造成内存泄漏。 Key使用什么引用才好？如上，key对ThreadLocal使用弱引用会发生内存泄露。那么，如果使用强使用，问题是否就得以解决？若 key 使用强引用：引用的ThreadLocal的对象被回收了，但是ThreadLocalMap还持有ThreadLocal的强引用，如果没有手动删除，ThreadLocal不会被回收，导致Entry内存泄漏。那么如果 key 使用弱引用：引用的ThreadLocal的对象被回收了，由于ThreadLocalMap持有ThreadLocal的引用是弱引用，即使没有手动删除，ThreadLocal也会被回收。至于value，则在下一次ThreadLocalMap调用set,get，remove的时候会被清除。所以比较两种情况，我们可以发现：由于ThreadLocalMap的生命周期跟Thread一样长，如果都没有手动删除对应key，都会导致内存泄漏，但是使用弱引用可以多一层保障：弱引用ThreadLocal不会内存泄漏，而对应的value在下一次ThreadLocalMap调用set,get,remove的时候会被清除。因此，ThreadLocal内存泄漏的根源是：由于ThreadLocalMap的生命周期跟Thread一样长，如果没有手动删除对应key就会导致内存泄漏，而不是因为弱引用。 ThreadLocal防内存泄露最佳实践综上，我们可以理解ThreadLocal为避免内存泄露的设计大致上是： JVM利用ThreadLocalMap的Key为弱引用，来避免ThreadLocal内存泄露。 由于Key设置为弱引用，那么，当ThreadLocal存储很多Key为null的Entry的时候，而不再去调用remove、get、set方法，那么将导致内存泄漏。所以，每次使用完ThreadLocal，都调用它的remove()方法，清除数据，则可以达到回收弱引用的结果，这是最佳的使用实践。否则，在使用线程池的情况下，没有及时清理ThreadLocal，不仅是内存泄漏的问题，更严重的是可能导致业务逻辑出现问题。所以，使用ThreadLocal就跟加锁完要解锁一样，用完就清理。 Reference《并发编程（四）：ThreadLocal从源码分析总结到内存泄漏》《深入分析 ThreadLocal 内存泄漏问题》]]></content>
      <tags>
        <tag>线程</tag>
        <tag>并发</tag>
        <tag>同步</tag>
        <tag>锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[整理Volatile]]></title>
    <url>%2F2018%2F11%2F03%2FVolatile-Summary%2F</url>
    <content type="text"><![CDATA[整理volatile相关，便于回顾记忆… Volatile简介volatile是轻量级的synchronized，它在多处理器开发中保证了共享变量的“可见性”，但不像synchronized一样保证原子性。可见性的意思是当一个线程修改一个共享变量时，另外一个线程能读到这个修改的值。如果volatile变量修饰符使用恰当的话，它比synchronized的使用和执行成本更低，因为它不会引起线程上下文的切换和调度。 Volatile的特性 volatile可见性；对一个volatile的读，总可以看到对这个变量最近一次的写； volatile原子性；volatile对单个读/写具有原子性（32位Long、Double），但是复合操作除外，例如i++; JVM底层采用“内存屏障”来实现volatile语义，禁止重排序以保证有序性 理解volatile特性的一个好方法是把对volatile变量的单个读/写，看成是使用同一个锁对这些单个读/写操作做了同步。下面通过具体的示例来说明，示例代码如下。 123456789101112131415class VolatileFeaturesExample &#123; volatile long vl = 0L; // 使用volatile声明64位的long型变量 public void set(long l) &#123; vl = l; // 单个volatile变量的写 &#125; public void getAndIncrement () &#123; vl++; // 复合（多个）volatile变量的读/写 &#125; public long get() &#123; return vl; // 单个volatile变量的读 &#125;&#125; 假设有多个线程分别调用上面程序的3个方法，这个程序在语义上和下面程序等价。 12345678910111213141516class VolatileFeaturesExample &#123; long vl = 0L; // 64位的long型普通变量 public synchronized void set(long l) &#123; // 对单个的普通变量的写用同一个锁同步 vl = l; &#125; public void getAndIncrement () &#123; // 普通方法调用 long temp = get(); // 调用已同步的读方法 temp += 1L; // 普通写操作 set(temp); // 调用已同步的写方法 &#125; public synchronized long get() &#123; // 对单个的普通变量的读用同一个锁同步 return vl; &#125;&#125; 如上面示例程序所示，一个volatile变量的单个读/写操作，与一个普通变量的读/写操作都是使用同一个锁来同步，它们之间的执行效果相同。锁的happens-before规则保证释放锁和获取锁的两个线程之间的内存可见性，这意味着对一个volatile变量的读，总是能看到（任意线程）对这个volatile变量最后的写入。锁的语义决定了临界区代码的执行具有原子性。这意味着，即使是64位的long型和double型变量，只要它是volatile变量，对该变量的读/写就具有原子性。如果是多个volatile操作或类似于volatile++这种复合操作，这些操作整体上不具有原子性。 Volataile的内存语义及其实现Java语言规范对volatile的定义如下：1Java编程语言允许线程访问共享变量，为了确保共享变量能被准确和一致地更新，线程应该确保通过排他锁单独获得这个变量。 通俗点讲就是说一个变量如果用volatile修饰了，则Java可以确保所有线程看到这个变量的值是一致的，如果某个线程对volatile修饰的共享变量进行更新，那么其他线程可以立马看到这个更新，这就是所谓的线程可见性。 在了解volatile实现原理之前，我们先来看下与其实现原理相关的CPU术语与说明。下表是CPU术语的定义 Volatile是如何来保证可见性的呢？让我们在X86处理器下通过工具获取JIT编译器生成的汇编指令来查看对volatile进行写操作时，CPU会做什么事情。 Java代码如下：1instance = new Singleton(); // instance是volatile变量 转变成汇编代码，如下。10x01a3de1d: movb $0×0,0×1104800(%esi);0x01a3de24: lock addl $0×0,(%esp); 有volatile变量修饰的共享变量进行写操作的时候会多出第二行汇编代码，通过查IA-32架构软件开发者手册可知，Lock前缀的指令在多核处理器下会引发了两件事情。 1） 将当前处理器缓存行的数据写回到系统内存。2） 这个写回内存的操作会使在其他CPU里缓存了该内存地址的数据无效。为了提高处理速度，处理器不直接和内存进行通信，而是先将系统内存的数据读到内部缓存（L1，L2或其他）后再进行操作，但操作完不知道何时会写到内存。如果对声明了volatile的变量进行写操作，JVM就会向处理器发送一条Lock前缀的指令，将这个变量所在缓存行的数据写回到系统内存。但是，就算写回到内存，如果其他处理器缓存的值还是旧的，再执行计算操作就会有问题。所以，在多处理器下，为了保证各个处理器的缓存是一致的，就会实现缓存一致性协议，每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期了，当处理器发现自己缓存行对应的内存地址被修改，就会将当前处理器的缓存行设置成无效状态，当处理器对这个数据进行修改操作的时候，会重新从系统内存中把数据读到处理器缓存里。下面来具体讲解volatile的两条实现原则。 Lock前缀指令会引起处理器缓存回写到内存。Lock前缀指令导致在执行指令期间，声言处理器的LOCK#信号。在多处理器环境中，LOCK#信号确保在声言该信号期间，处理器可以独占任何共享内存[2]。但是，在最近的处理器里，LOCK＃信号一般不锁总线，而是锁缓存，毕竟锁总线开销的比较大。在8.1.4节有详细说明锁定操作对处理器缓存的影响，对于Intel486和Pentium处理器，在锁操作时，总是在总线上声言LOCK#信号。但在P6和目前的处理器中，如果访问的内存区域已经缓存在处理器内部，则不会声言LOCK#信号。相反，它会锁定这块内存区域的缓存并回写到内存，并使用缓存一致性机制来确保修改的原子性，此操作被称为“缓存锁定”，缓存一致性机制会阻止同时修改由两个以上处理器缓存的内存区域数据。 一个处理器的缓存回写到内存会导致其他处理器的缓存无效。IA-32处理器和Intel 64处理器使用MESI（修改、独占、共享、无效）控制协议去维护内部缓存和其他处理器缓存的一致性。在多核处理器系统中进行操作的时候，IA-32和Intel 64处理器能嗅探其他处理器访问系统内存和它们的内部缓存。处理器使用嗅探技术保证它的内部缓存、系统内存和其他处理器的缓存的数据在总线上保持一致。例如，在Pentium和P6 family处理器中，如果通过嗅探一个处理器来检测其他处理器打算写内存地址，而这个地址当前处于共享状态，那么正在嗅探的处理器将使它的缓存行无效，在下次访问相同内存地址时，强制执行缓存行填充。 Volatile写-读建立的happens-before关系从JSR-133开始（即从JDK5开始），volatile变量的写-读可以实现线程之间的通信。从内存语义的角度来说，volatile的写-读与锁的释放-获取有相同的内存效果：volatile写和锁的释放有相同的内存语义；volatile读与锁的获取有相同的内存语义。12345678910111213141516class VolatileExample &#123; int a = 0; volatile boolean flag = false; public void writer() &#123; a = 1; // 1 flag = true; // 2 &#125; public void reader() &#123; if (flag) &#123; // 3 int i = a; // 4 …… &#125; &#125; &#125; 假设线程A执行writer()方法之后，线程B执行reader()方法。根据happens-before规则，这个过程建立的happens-before关系可以分为3类：1) 根据程序次序规则，1 happens-before 2;3 happens-before 4。2) 根据volatile规则，2 happens-before 3。3) 根据happens-before的传递性规则，1 happens-before 4。在上图中，每一个箭头链接的两个节点，代表了一个happens-before关系。黑色箭头表示程序顺序规则；橙色箭头表示volatile规则；蓝色箭头表示组合这些规则后提供的happens-before保证。这里A线程写一个volatile变量后，B线程读同一个volatile变量。A线程在写volatile变量之前所有可见的共享变量，在B线程读同一个volatile变量后，将立即变得对B线程可见。 限制重排序重排序分为编译器重排序和处理器重排序。为了实现volatile内存语义，JMM会分别限制这两种类型的重排序类型.以下是JMM针对编译器制定的volatile重排序规则表：举例来说，第三行最后一个单元格的意思是：在程序中，当第一个操作为普通变量的读或写时，如果第二个操作为volatile写，则编译器不能重排序这两个操作。从表中可以看出： 当第二个操作是volatile写时，不管第一个操作是什么，都不能重排序。这个规则确保volatile写之前的操作不会被编译器重排序到volatile写之后。 当第一个操作是volatile读时，不管第二个操作是什么，都不能重排序。这个规则确保volatile读之后的操作不会被编译器重排序到volatile读之前。 当第一个操作是volatile写，第二个操作是volatile读时，不能重排序。 为了实现volatile的内存语义，编译器在生成字节码时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序。对于编译器来说，发现一个最优布置来最小化插入屏障的总数几乎不可能。为此，JMM采取保守策略。下面是基于保守策略的JMM内存屏障插入策略。 在每个volatile写操作的前面插入一个StoreStore屏障。 在每个volatile写操作的后面插入一个StoreLoad屏障。 在每个volatile读操作的后面插入一个LoadLoad屏障。 在每个volatile读操作的后面插入一个LoadStore屏障。 上述内存屏障插入策略非常保守，但它可以保证在任意处理器平台，任意的程序中都能得到正确的volatile内存语义。下面是保守策略下，volatile写插入内存屏障后生成的指令序列示意图:图中的StoreStore屏障可以保证在volatile写之前，其前面的所有普通写操作已经对任意处理器可见了。这是因为StoreStore屏障将保障上面所有的普通写在volatile写之前刷新到主内存。这里比较有意思的是，volatile写后面的StoreLoad屏障。此屏障的作用是避免volatile写与后面可能有的volatile读/写操作重排序。因为编译器常常无法准确判断在一个volatile写的后面是否需要插入一个StoreLoad屏障（比如，一个volatile写之后方法立即return）。为了保证能正确实现volatile的内存语义，JMM在采取了保守策略：在每个volatile写的后面，或者在每个volatile读的前面插入一个StoreLoad屏障。从整体执行效率的角度考虑，JMM最终选择了在每个volatile写的后面插入一个StoreLoad屏障。因为volatile写-读内存语义的常见使用模式是：一个写线程写volatile变量，多个读线程读同一个volatile变量。当读线程的数量大大超过写线程时，选择在volatile写之后插入StoreLoad屏障将带来可观的执行效率的提升。从这里可以看到JMM在实现上的一个特点：首先确保正确性，然后再去追求执行效率。下面是在保守策略下，volatile读插入内存屏障后生成的指令序列示意图:图中的LoadLoad屏障用来禁止处理器把上面的volatile读与下面的普通读重排序。LoadStore屏障用来禁止处理器把上面的volatile读与下面的普通写重排序。上述volatile写和volatile读的内存屏障插入策略非常保守。在实际执行时，只要不改变volatile写-读的内存语义，编译器可以根据具体情况省略不必要的屏障。下面通过具体的示例代码进行说明。1234567891011121314class VolatileBarrierExample &#123; int a; volatile int v1 = 1; volatile int v2 = 2; void readAndWrite() &#123; int i = v1; // 第一个volatile读 int j = v2; // 第二个volatile读 a = i + j; // 普通写 v1 = i + 1; // 第一个volatile写 v2 = j * 2; // 第二个 volatile写 &#125; …// 其他方法&#125; 针对readAndWrite()方法，编译器在生成字节码时可以做如下的优化。注意，最后的StoreLoad屏障不能省略。因为第二个volatile写之后，方法立即return。此时编译器可能无法准确断定后面是否会有volatile读或写，为了安全起见，编译器通常会在这里插入一个StoreLoad屏障。上面的优化针对任意处理器平台，由于不同的处理器有不同“松紧度”的处理器内存模型，内存屏障的插入还可以根据具体的处理器内存模型继续优化。以X86处理器为例，图3-21中除最后的StoreLoad屏障外，其他的屏障都会被省略。前面保守策略下的volatile读和写，在X86处理器平台可以优化成如下图所示。前文提到过，X86处理器仅会对写-读操作做重排序。X86不会对读-读、读-写和写-写操作做重排序，因此在X86处理器中会省略掉这3种操作类型对应的内存屏障。在X86中，JMM仅需在volatile写后面插入一个StoreLoad屏障即可正确实现volatile写-读的内存语义。这意味着在X86处理器中，volatile写的开销比volatile读的开销会大很多（因为执行StoreLoad屏障开销会比较大）。 Reference《Java并发编程的艺术》–方腾飞]]></content>
      <tags>
        <tag>线程</tag>
        <tag>并发</tag>
        <tag>同步</tag>
        <tag>锁</tag>
        <tag>内存模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[整理Synchronized]]></title>
    <url>%2F2018%2F11%2F03%2FSynchronized-Summary%2F</url>
    <content type="text"><![CDATA[整理volatile相关，便于回顾记忆… Synchronized简介Synchronized一直是Java多线程并发编程中用作同步的元老级角色，很多人对它的概念都是一个重量级锁，但在JDK 1.6，对synchronized进行了各种优化，为了减少获得锁和释放锁带来的性能消耗而引入了偏向锁和轻量级锁，和锁的存储结构和升级过程。Synchronized实现同步的基础在于Java中的每一个对象都可以作为，所以本质上synchronized就是一把对象锁。Synchronized可作用于类，静态方法，普通方法，及代码块，具体表现为以下3种形式： 对于普通同步方法，锁是当前实例对象 对于静态同步方法，锁是当前类的Class对象。 对于同步方法块，锁是Synchonized括号里配置的对象 当一个线程试图访问同步代码块时，它首先必须得到锁，退出或抛出异常时必须释放锁。那么锁到底存在哪里呢？锁里面会存储什么信息呢？ 同步的原理管程(Monitor)对象Java 虚拟机中的同步(Synchronization)基于进入和退出管程(Monitor)对象实现，无论是显式同步，还是隐式同步都如此。 显式同步是指有明确的monitorenter和monitorexit指令，也就是synchronized同步代码块的场景 隐式同步则是由方法调用指令读取运行时常量池中方法的ACC_SYNCHRONIZED 标志来隐式实现，也就是指方法同步的场景 1234567891011public class SynchronizedTest &#123; public synchronized void test1()&#123; System.out.println( "test1" ); &#125; public void test2()&#123; synchronized (this)&#123; System.out.println( "test2" ); &#125; &#125;&#125; Java对象头在JVM中，对象在内存中的布局分为三块区域：对象头、实例数据和对齐填充，一般而言，synchronized使用的锁对象是存储在Java对象头里的。如果对象是非数组类型，则用2字宽存储对象头，如果对象是数组则会分配3个字宽，多出来的1个字记录的是数组长度。在32位虚拟机中，一字宽等于四字节，即32bit。 Java对象头里的Mark Word里默认存储对象的HashCode，分代年龄和锁标记位。32位JVM的Mark Word的默认存储结构如下： 在运行期间Mark Word里存储的数据会随着锁标志位的变化而变化。Mark Word可能变化为存储以下4种数据： 在64位虚拟机下，Mark Word是64bit大小的，其存储结构如下： Synchronized同步方法底层原理方法级的同步是隐式，即无需通过字节码指令来控制的，它实现在方法调用和返回操作之中。JVM可以从方法常量池中的方法表结构(method_info Structure) 中的 ACC_SYNCHRONIZED 访问标志区分一个方法是否同步方法。当方法调用时，调用指令将会 检查方法的 ACC_SYNCHRONIZED 访问标志是否被设置，如果设置了，执行线程将先持有monitor（虚拟机规范中用的是管程一词）， 然后再执行方法，最后再方法完成(无论是正常完成还是非正常完成)时释放monitor。在方法执行期间，执行线程持有了monitor，其他任何线程都无法再获得同一个monitor。如果一个同步方法执行期间抛 出了异常，并且在方法内部无法处理此异常，那这个同步方法所持有的monitor将在异常抛到同步方法之外时自动释放。 Synchronized同步代码块底层原理从字节码中可知同步语句块的实现使用的是monitorenter 和 monitorexit 指令，其中monitorenter指令指向同步代码块的开始位置，monitorexit指令则指明同步代码块的结束位置，当执行monitorenter指令时，当前线程将试图获取 objectref(即对象锁) 所对应的 monitor 的持有权，当 objectref 的 monitor 的进入计数器为 0，那线程可以成功取得 monitor，并将计数器值设置为 1，取锁成功。如果当前线程已经拥有 objectref 的 monitor 的持有权，那它可以重入这个 monitor (关于重入性稍后会分析)，重入时计数器的值也会加 1。倘若其他线程已经拥有 objectref 的 monitor 的所有权，那当前线程将被阻塞，直到正在执行线程执行完毕，即monitorexit指令被执行，执行线程将释放 monitor(锁)并设置计数器值为0 ，其他线程将有机会持有 monitor 。值得注意的是编译器将会确保无论方法通过何种方式完成，方法中调用过的每条 monitorenter 指令都有执行其对应 monitorexit 指令，而无论这个方法是正常结束还是异常结束。为了保证在方法异常完成时 monitorenter 和 monitorexit 指令依然可以正确配对执行，编译器会自动产生一个异常处理器，这个异常处理器声明可处理所有的异常，它的目的就是用来执行 monitorexit 指令。从字节码中也可以看出多了一个monitorexit指令，它就是异常结束时被执行的释放monitor 的指令。 实例对象锁的线程安全当一个线程正在访问一个对象的 synchronized 实例方法，那么其他线程不能访问该对象的其他 synchronized 方法，毕竟一个对象只有一把锁，当一个线程获取了该对象的锁之后，其他线程无法获取该对象的锁，所以无法访问该对象的其他synchronized实例方法，但是其他线程还是可以访问该实例对象的其他非synchronized方法，当然如果是一个线程 A 需要访问实例对象 obj1 的 synchronized 方法 f1(当前对象锁是obj1)，另一个线程 B 需要访问实例对象 obj2 的 synchronized 方法 f2(当前对象锁是obj2)，这样是允许的，因为两个实例对象锁并不同相同，此时如果两个线程操作数据并非共享的，线程安全是有保障的，遗憾的是如果两个线程操作的是共享数据，那么线程安全就有可能无法保证了，如下代码将演示出该现象:123456789101112131415161718192021222324public class AccountingSyncBad implements Runnable&#123; static int i=0; public synchronized void increase()&#123; i++; &#125; @Override public void run() &#123; for(int j=0;j&lt;1000000;j++)&#123; increase(); &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; //new新实例 Thread t1=new Thread(new AccountingSyncBad()); //new新实例 Thread t2=new Thread(new AccountingSyncBad()); t1.start(); t2.start(); //join含义:当前线程A等待thread线程终止之后才能从thread.join()返回 t1.join(); t2.join(); System.out.println(i); &#125;&#125; 上述代码与前面不同的是我们同时创建了两个新实例AccountingSyncBad，然后启动两个不同的线程对共享变量i进行操作，但很遗憾操作结果是1452317而不是期望结果2000000，因为上述代码犯了严重的错误，虽然我们使用synchronized修饰了increase方法，但却new了两个不同的实例对象，这也就意味着存在着两个不同的实例对象锁，因此t1和t2都会进入各自的对象锁，也就是说t1和t2线程使用的是不同的锁，因此线程安全是无法保证的。 锁的升级Java早期版本中，synchronized属于重量级锁，效率低下，因为监视器锁（monitor）是依赖于底层的操作系统的Mutex Lock来实现的，而操作系统实现线程之间的切换时需要从用户态转换到核心态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高，这也是为什么早期的synchronized效率低的原因。Java SE1.6为了减少获得锁和释放锁所带来的性能消耗，引入了“偏向锁”和“轻量级锁”，所以在Java SE1.6里锁一共有四种状态，无锁状态，偏向锁状态，轻量级锁状态和重量级锁状态，它会随着竞争情况逐渐升级。锁可以升级但不能降级，意味着偏向锁升级成轻量级锁后不能降级成偏向锁。这种锁升级却不能降级的策略，目的是为了提高获得锁和释放锁的效率，下文会详细分析。 偏向锁Hotspot的作者经过以往的研究发现大多数情况下锁不仅不存在多线程竞争，而且总是由同一线程多次获得，为了让线程获得锁的代价更低而引入了偏向锁。当一个线程访问同步块并获取锁时，会在对象头和栈帧中的锁记录里存储锁偏向的线程ID，以后该线程在进入和退出同步块时不需要花费CAS操作来加锁和解锁，而只需简单的测试一下对象头的Mark Word里是否存储着指向当前线程的偏向锁，如果测试成功，表示线程已经获得了锁，如果测试失败，则需要再测试下Mark Word中偏向锁的标识是否设置成1（表示当前是偏向锁），如果没有设置，则使用CAS竞争锁，如果设置了，则尝试使用CAS将对象头的偏向锁指向当前线程。 偏向锁的撤销偏向锁使用了一种等到竞争出现才释放锁的机制，所以当其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁。偏向锁的撤销，需要等待全局安全点（在这个时间点上没有字节码正在执行），它会首先暂停拥有偏向锁的线程，然后检查持有偏向锁的线程是否活着，如果线程不处于活动状态，则将对象头设置成无锁状态，如果线程仍然活着，拥有偏向锁的栈会被执行，遍历偏向对象的锁记录，栈中的锁记录和对象头的Mark Word要么重新偏向于其他线程，要么恢复到无锁或者标记对象不适合作为偏向锁，最后唤醒暂停的线程。下图中的线程1演示了偏向锁初始化的流程，线程2演示了偏向锁撤销的流程。 关闭偏向锁偏向锁在Java 6和Java 7里是默认启用的，但是它在应用程序启动几秒钟之后才激活，如有必要可以使用JVM参数来关闭延迟-XX：BiasedLockingStartupDelay = 0。如果你确定自己应用程序里所有的锁通常情况下处于竞争状态，可以通过JVM参数关闭偏向锁-XX:-UseBiasedLocking=false，那么默认会进入轻量级锁状态。 轻量级锁轻量级锁加锁线程在执行同步块之前，JVM会先在当前线程的栈桢中创建用于存储锁记录的空间，并将对象头中的Mark Word复制到锁记录中，官方称为Displaced Mark Word。然后线程尝试使用CAS将对象头中的Mark Word替换为指向锁记录的指针。如果成功，当前线程获得锁，如果失败，表示其他线程竞争锁，当前线程便尝试使用自旋来获取锁。 轻量级锁解锁轻量级解锁时，会使用原子的CAS操作来将Displaced Mark Word替换回到对象头，如果成功，则表示没有竞争发生。如果失败，表示当前锁存在竞争，锁就会膨胀成重量级锁。下图是两个线程同时争夺锁，导致锁膨胀的流程图。 因为自旋会消耗CPU，为了避免无用的自旋（比如获得锁的线程被阻塞住了），一旦锁升级成重量级锁，就不会再恢复到轻量级锁状态。当锁处于这个状态下，其他线程试图获取锁时，都会被阻塞住，当持有锁的线程释放锁之后会唤醒这些线程，被唤醒的线程就会进行新一轮的夺锁之争。 锁的优缺点对比 Reference 方腾飞：《Java并发编程的艺术》 深入理解Java并发之synchronized实现原理]]></content>
      <tags>
        <tag>线程</tag>
        <tag>并发</tag>
        <tag>同步</tag>
        <tag>锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis基本数据结构-Dict]]></title>
    <url>%2F2018%2F11%2F03%2FRedis-DataStructure-3-Dict%2F</url>
    <content type="text"><![CDATA[Dict简介字典（dict）是Redis一个重要的基础数据结构，它是一个用于维护key和value映射关系的数据结构，与很多语言中的Map或dictionary类似。不过，这只是它在Redis中的一个用途而已，它在Redis中被使用的地方还有很多。比如Redis的数据库就是使用字典来作为底层实现的，对数据的增删查改操作都是构建在对字典的操作之上，又比如，一个Redis hash结构，当它的field较多时，便会采用dict来存储。再比如，Redis配合使用dict和skiplist来共同维护一个sorted set。字典经常作为一种数据结构内置在很多高级的编程语言里，但Redis的实现所使用的C语言中并没有内置这种数据结构，因此Redis构建了自己的字典实现。 Dict的双数组哈希表Redis中的字典由dict.h/dict结构表示：123456789101112131415typedef struct dict &#123; // 类型特定函数 dictType *type; // 私有数据 void *privatedata; // 哈希表 dictht ht[2]; // rehash 索引 // 当rehash不在进行时，值为-1 int trehashidx; /* rehashing not in progress if rehashidx == -1 */&#125;dict; 其中ht属性是一个包含两个项的数组，数组中的每个项都是一个dictht哈希表，一般情况下，字典只使用ht[0]哈希表，ht[1]哈希表只用于对ht[0]哈希表进行rehash时。除了ht[1]之外，另一个和rehash有关的属性就是rehashidx，它记录了rehash目前的进度，如果目前没有在进行rehash，那么它的值为-1。 哈希算法Redis计算哈希值和索引值的方法：123456# 使用字典设置的哈希函数，计算键key的哈希值hash = dict-&gt;type-&gt;hashFunction(key);# 使用哈希值的 sizemark 属性和和哈希值，计算出索引值# 根据情况不同，ht[x]可以是ht[0]或者ht[1]index = hash &amp; dict-&gt;ht[x].sizemark; 当字典被用作数据库的底层实现，或者哈希键的底层实现时，Redis使用MurmurHash2算法来计算键的哈希值。 键冲突键冲突解决方法是链地址法（拉链法），与Java里的hashmap一样使用单向链表把同一个索引上的多个节点连接起来。因为dictEntry节点组成的链表没有指向链表表尾的指针，所以为了速度考虑，程序总是把新节点添加到链表的表头位置（复杂度为O(1)），排在其它已有节点的前面。 Rehash哈希表的扩展与收缩是通过执行rehash（重新散列）操作来完成，操作步骤如下： 空间分配：为字典的ht[1]哈希表分配空间，这个哈希表的空间大小取决于要执行的操作，以及ht[0]当前包含的键值对数量（也即是ht[0].used属性的值）: 如果是扩展操作，那么ht[1]的大小为第一个大于等于ht[0].used*2的2ⁿ（2的n次幂）。 如果是收缩操作，那么ht[1]的大小为第一个大于等于ht[0].used的2ⁿ。 rehash：将保存在ht[0]中的所有键值对rehash到ht[1]上面：rehash指的是重新计算键的哈希值和索引值，然后将键值对放置到ht[1]哈希表的指定位置上。 替换：当ht[0]包含的所有键值对都迁移到ht[1]之后（ht[0]变为空表），释放ht[0]，将ht[1]设置为ht[0]，并在ht[1]新创建一个空白哈希表，为下一次rehash做准备。 哈希表的扩展与收缩当以下条件中的任意一个被满足时，程序会自动开始对哈希表执行扩展操作：1) 没在执行BGSAVE命令或者BGREWRITEAOF命令，而哈希表的负载因子≥1。2) 有在执行BGSAVE命令或者BGREWRITEAOF命令,并且哈希表的负载因子≥0.5。 负载因子计算为： load_factor = ht[0].used / ht[0].size; 负载因子＜0.1时，进行收缩操作。 渐进式rehashRedis的扩展或收缩需要把ht[0]所有键值对rehash到ht[1]里，这个动作并不是一次性、集中式完成的，而是分多次、渐进式完成的。这么做的原因是，当哈希表数据量庞大到一定量级时，一次性rehash操作会带来庞大的计算量，这样可能会导致服务器在一段时间内停止服务。以下是渐进式rehash的详细步骤：1) 为ht[1]分配空间，让字典同时持有ht[0]和ht[1]两个哈希表。2) 在字典中维持一个索引计数器变量rehashidx，并将它的值设置为0，表示工作正式开始。3) 在rehash进行期间，每次对字典执行增删改查操作时，程序除了执行指定的操作以外，还会顺带将ht[0]哈希表在rehashidx索引上的所有键值对rehash到ht[1]，当rehash完成之后，程序将rehashidx属性的值增1。4) 随着字典操作的不断执行，最终在某个时间节点上，ht[0]的所有键值对都会到了ht[1]，这时程序将rehashidx属性的值设为-1，表示rehash操作完成。 因为在进行渐进式rehash的过程中，字典会同时使用ht[0]和ht[1]两个哈希表，所以期间的增删查改操作都会在两个表上进行，例如查找操作时，先会在ht[0]里查找，如果没找到，再查ht[1]，诸如此类。增加操作时，那么新添加的键值对一律会被保存到ht[1]里，而ht[0]则不进行添加，保证了ht[0]包含的键值对只减不增，并随着rehash操作的执行完毕而最终变成空表。]]></content>
      <tags>
        <tag>Redis</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis基本数据结构-SkipList]]></title>
    <url>%2F2018%2F10%2F26%2FRedis-DataStructure-2-SkipList%2F</url>
    <content type="text"><![CDATA[Sorted Set有序集合键（sorted set）提供的操作非常丰富，可以满足非常多的应用场景。这也意味着，sorted set相对来说实现比较复杂。Redis使用跳跃表（skipList）作为sorted set的底层实现之一，如果一个sorted set包含的元素数量比较多，又或者sorted set中元素的成员（member）是比较长的字符串时，Redis就会使用跳跃表来作为sorted set的底层实现。 跳跃表是一种有序数据结构，能支持平均O(logN)、最坏O(N)复杂度的节点查找，在大部分情况下，跳跃表的效率可以和平衡树相媲美，所以不少程序都使用跳跃表来替代平衡树。 sorted set的数据结构有序集合（sorted set）的数据结构底层实现就是跳跃表+字典，如图： 跳跃表的数据结构一般查找问题的解法分为两个大类：一个是基于各种平衡树，一个是基于哈希表。但skiplist却比较特殊，它没法归属到这两大类里面。 skiplist，顾名思义，首先它是一个list。实际上，它是在有序链表的基础上发展起来的。 下面我们从头分析一个要求有序的线性结构的查找元素及插入新元素存在的性能问题。 最容易表达线性结构的自然是数组和链表。可是，无论是数组还是链表，在插入新元素的时候，都会存在性能问题。 如果使用数组，插入新元素的方式如下： 如果要插入一个值是3的元素，首先要知道这个元素应该插入的位置。使用二分查找可以最快定位，这一步时间复杂度是O（logN）。 插入过程中，原数组中所有大于3的元素都要右移，这一步时间复杂度是O（N）。所以总体时间复杂度是O（N）。 如果使用链表，插入新元素的方式如下： 如果要插入一个值是3的元素，首先要知道这个元素应该插入的位置。链表无法使用二分查找，只能和原链表中的节点逐一比较大小来确定位置。这一步的时间复杂度是O（N）。 插入的过程倒是很容易，直接改变节点指针的目标，时间复杂度O（1）。因此总体的时间复杂度也是O（N）。 这对于拥有几十万元素的集合来说，这两种方法显然都太慢了。 问题来了，既然数组也不行，链表也不想，那要用什么结构才好？ 我们可以利用索引的思想，提取出链表中的部分关键节点。 比如给定一个长度是7的有序链表，节点值依次是1→2→3→5→6→7→8。那么我们可以取出所有奇数值的节点作为关键字。 此时如果要插入一个值是4的新节点，不再需要和原节点8,7,6,5,3逐一比较，只需要比较关键节点7,5,3 确定了新节点在关键节点中的位置（3和5之间），就可以回到原链表，迅速定位到对应的位置插入（同样3和5之间） 当链表中有1W设置10W个节点，优化效果会很明显，比较次数就整整减少了一半！但是这样的做法只是增加了50%的额外空间，却换来了一倍的性能提高。 不过我们可以进一步思考，既然已经提取了一层关键节点作为索引，那我们为何不能从索引中进一步提取，再提取一层索引的索引呢？ 于是乎，我们有了2级索引之后，新的节点可以先和2级索引比较，确定大体范围；然后再和1级索引比较；最后再回到原链表，找到并插入对应位置。 当节点很多的时候，比较次数会减少到原来的1/4，如是者，如果我们再继续往上提取更高层的索引，保证每一层是上一层节点的一半，一直到同一层只有两个节点（因为只有一个节点没有比较的意义），那么这样一个多层链表结构，便是我们的跳跃表。 那么，跳跃表的介绍引子至此已告一段落，下面是一个正儿八经的跳跃表的概念的介绍： 我们先来看一个有序链表，如下图（最左侧的灰色节点表示一个空的头结点）： 在这样一个链表中，如果我们要查找某个数据，那么需要从头开始逐个进行比较，直到找到包含数据的那个节点，或者找到第一个比给定数据大的节点为止（没找到）。也就是说，时间复杂度为O(n)。同样，当我们要插入新数据的时候，也要经历同样的查找过程，从而确定插入位置。 假如我们每相邻两个节点增加一个指针，让指针指向下下个节点，如下图： 这样所有新增加的指针连成了一个新的链表，但它包含的节点个数只有原来的一半（上图中是7, 19, 26）。现在当我们想查找数据的时候，可以先沿着这个新链表进行查找。当碰到比待查数据大的节点时，再回到原来的链表中进行查找。比如，我们想查找23，查找的路径是沿着下图中标红的指针所指向的方向进行的： 23首先和7比较，再和19比较，比它们都大，继续向后比较。 但23和26比较的时候，比26要小，因此回到下面的链表（原链表），与22比较。 23比22要大，沿下面的指针继续向后和26比较。23比26小，说明待查数据23在原链表中不存在，而且它的插入位置应该在22和26之间。 在这个查找过程中，由于新增加的指针，我们不再需要与链表中每个节点逐个进行比较了。需要比较的节点数大概只有原来的一半。 利用同样的方式，我们可以在上层新产生的链表上，继续为每相邻的两个节点增加一个指针，从而产生第三层链表。如下图： 在这个新的三层链表结构上，如果我们还是查找23，那么沿着最上层链表首先要比较的是19，发现23比19大，接下来我们就知道只需要到19的后面去继续查找，从而一下子跳过了19前面的所有节点。可以想象，当链表足够长的时候，这种多层链表的查找方式能让我们跳过很多下层节点，大大加快查找的速度。 skiplist正是受这种多层链表的想法的启发而设计出来的。实际上，按照上面生成链表的方式，上面每一层链表的节点个数，是下面一层的节点个数的一半，这样查找过程就非常类似于一个二分查找，使得查找的时间复杂度可以降低到O(log n)。但是，这种方法在插入数据的时候有很大的问题。新插入一个节点之后，就会打乱上下相邻两层链表上节点个数严格的2:1的对应关系。如果要维持这种对应关系，就必须把新插入的节点后面的所有节点（也包括新插入的节点）重新进行调整，这会让时间复杂度重新蜕化成O(n)。删除数据也有同样的问题。 skiplist为了避免这一问题，它不要求上下相邻两层链表之间的节点个数有严格的对应关系，而是为每个节点随机出一个层数(level)。比如，一个节点随机出的层数是3，那么就把它链入到第1层到第3层这三层链表中。为了表达清楚，下图展示了如何通过一步步的插入操作从而形成一个skiplist的过程: 从上面skiplist的创建和插入过程可以看出，每一个节点的层数（level）是随机出来的，而且新插入一个节点不会影响其它节点的层数。因此，插入操作只需要修改插入节点前后的指针，而不需要对很多节点都进行调整。这就降低了插入操作的复杂度。实际上，这是skiplist的一个很重要的特性，这让它在插入性能上明显优于平衡树的方案。这在后面我们还会提到。 根据上图中的skiplist结构，我们很容易理解这种数据结构的名字的由来。skiplist，翻译成中文，可以翻译成“跳表”或“跳跃表”，指的就是除了最下面第1层链表之外，它会产生若干层稀疏的链表，这些链表里面的指针故意跳过了一些节点（而且越高层的链表跳过的节点越多）。这就使得我们在查找数据的时候能够先在高层的链表中进行查找，然后逐层降低，最终降到第1层链表来精确地确定数据位置。在这个过程中，我们跳过了一些节点，从而也就加快了查找速度。 刚刚创建的这个skiplist总共包含4层链表，现在假设我们在它里面依然查找23，下图给出了查找路径： 需要注意的是，前面演示的各个节点的插入过程，实际上在插入之前也要先经历一个类似的查找过程，在确定插入位置后，再完成插入操作。 至此，skiplist的查找和插入操作，我们已经很清楚了。而删除操作与插入操作类似，我们也很容易想象出来。这些操作我们也应该能很容易地用代码实现出来。 当然，实际应用中的skiplist每个节点应该包含key和value两部分。前面的描述中我们没有具体区分key和value，但实际上列表中是按照key进行排序的，查找过程也是根据key在比较。 但是，如果你是第一次接触skiplist，那么一定会产生一个疑问：节点插入时随机出一个层数，仅仅依靠这样一个简单的随机数操作而构建出来的多层链表结构，能保证它有一个良好的查找性能吗？为了回答这个疑问，我们需要分析skiplist的统计性能。 在分析之前，我们还需要着重指出的是，执行插入操作时计算随机数的过程，是一个很关键的过程，它对skiplist的统计特性有着很重要的影响。这并不是一个普通的服从均匀分布的随机数，它的计算过程如下： 首先，每个节点肯定都有第1层指针（每个节点都在第1层链表里）。 如果一个节点有第i层(i&gt;=1)指针（即节点已经在第1层到第i层链表中），那么它有第(i+1)层指针的概率为p。 节点最大的层数不允许超过一个最大值，记为MaxLevel。 这个计算随机层数的伪码如下所示： 123456randomLevel() level := 1 // random()返回一个[0...1)的随机数 while random() &lt; p and level &lt; MaxLevel do level := level + 1 return level randomLevel()的伪码中包含两个参数，一个是p，一个是MaxLevel。在Redis的skiplist实现中，这两个参数的取值为： 12p = 1/4MaxLevel = 32 skiplist与平衡树、哈希表的比较 skiplist和各种平衡树（如AVL、红黑树等）的元素是有序排列的，而哈希表不是有序的。因此，在哈希表上只能做单个key的查找，不适宜做范围查找。所谓范围查找，指的是查找那些大小在指定的两个值之间的所有节点。 在做范围查找的时候，平衡树比skiplist操作要复杂。在平衡树上，我们找到指定范围的小值之后，还需要以中序遍历的顺序继续寻找其它不超过大值的节点。如果不对平衡树进行一定的改造，这里的中序遍历并不容易实现。而在skiplist上进行范围查找就非常简单，只需要在找到小值之后，对第1层链表进行若干步的遍历就可以实现。 平衡树的插入和删除操作可能引发子树的调整，逻辑复杂，而skiplist的插入和删除只需要修改相邻节点的指针，操作简单又快速。 从内存占用上来说，skiplist比平衡树更灵活一些。一般来说，平衡树每个节点包含2个指针（分别指向左右子树），而skiplist每个节点包含的指针数目平均为1/(1-p)，具体取决于参数p的大小。如果像Redis里的实现一样，取p=1/4，那么平均每个节点包含1.33个指针，比平衡树更有优势。 查找单个key，skiplist和平衡树的时间复杂度都为O(log n)，大体相当；而哈希表在保持较低的哈希值冲突概率的前提下，查找时间复杂度接近O(1)，性能更高一些。所以我们平常使用的各种Map或dictionary结构，大都是基于哈希表实现的。 从算法实现难度上来比较，skiplist比平衡树要简单得多。 Redis中的sorted set我们前面提到过，Redis中的sorted set，是在skiplist, dict和ziplist基础上构建起来的: 当数据较少时，sorted set是由一个ziplist来实现的。 当数据多的时候，sorted set是由一个叫zset的数据结构来实现的，这个zset包含一个dict + 一个skiplist。dict用来查询数据到分数(score)的对应关系，而skiplist用来根据分数查询数据（可能是范围查找）。 在这里我们先来讨论一下前一种情况——基于ziplist实现的sorted set。在本系列前面关于ziplist的文章里，我们介绍过，ziplist就是由很多数据项组成的一大块连续内存。由于sorted set的每一项元素都由数据和score组成，因此，当使用zadd命令插入一个(数据, score)对的时候，底层在相应的ziplist上就插入两个数据项：数据在前，score在后。 ziplist的主要优点是节省内存，但它上面的查找操作只能按顺序查找（可以正序也可以倒序）。因此，sorted set的各个查询操作，就是在ziplist上从前向后（或从后向前）一步步查找，每一步前进两个数据项，跨域一个(数据, score)对。 随着数据的插入，sorted set底层的这个ziplist就可能会转成zset的实现（转换过程详见t_zset.c的zsetConvert）。那么到底插入多少才会转呢？ 在redis.conf中的ADVANCED CONFIG部分的两个Redis配置12zset-max-ziplist-entries 128zset-max-ziplist-value 64 这个配置的意思是说，在如下两个条件之一满足的时候，ziplist会转成zset（具体的触发条件参见t_zset.c中的zaddGenericCommand相关代码）： 当sorted set中的元素个数，即(数据, score)对的数目超过128的时候，也就是ziplist数据项超过256的时候。 当sorted set中插入的任意一个数据的长度超过了64的时候。 最后，zset结构的代码定义如下：1234typedef struct zset &#123; dict *dict; zskiplist *zsl;&#125; zset; Redis为什么用skiplist而不用平衡树？在前面我们对于skiplist和平衡树、哈希表的比较中，其实已经不难看出Redis里使用skiplist而不用平衡树的原因了。现在我们看看，对于这个问题，Redis的作者 @antirez 是怎么说的： There are a few reasons: They are not very memory intensive. It’s up to you basically. Changing parameters about the probability of a node to have a given number of levels will make then less memory intensive than btrees. A sorted set is often target of many ZRANGE or ZREVRANGE operations, that is, traversing the skip list as a linked list. With this operation the cache locality of skip lists is at least as good as with other kind of balanced trees. They are simpler to implement, debug, and so forth. For instance thanks to the skip list simplicity I received a patch (already in Redis master) with augmented skip lists implementing ZRANK in O(log(N)). It required little changes to the code. 这段话原文出处： https://news.ycombinator.com/item?id=1171423这里从内存占用、对范围查找的支持和实现难易程度这三方面总结的原因]]></content>
      <tags>
        <tag>Redis</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[死锁]]></title>
    <url>%2F2018%2F10%2F21%2Fdead-lock%2F</url>
    <content type="text"><![CDATA[“死锁”的含义所谓死锁： 是指两个或两个以上的进程在执行过程中，由于竞争资源或者由于彼此通信而造成的一种阻塞的现象，若无外力作用，它们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁，这些永远在互相等待的进程称为死锁进程。 Java代码举例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public class Deadlock &#123; private Object o1 = new Object(); private Object o2 = new Object(); public void lock1() &#123; // 获取o1对象锁 synchronized( o1 ) &#123; try &#123; System.out.println( "l1 lock o1" ); // 获取o1后先等一会儿，让Lock2有足够的时间锁住o2 Thread.sleep( 1000 ); // 接着获取o2对象锁 synchronized( o2 ) &#123; System.out.println( "l1 lock o2" ); &#125; &#125; catch( Exception e ) &#123; e.printStackTrace(); &#125; &#125; &#125; public void lock2() &#123; // 获取o2对象锁 synchronized( o2 ) &#123; try &#123; System.out.println( "l2 lock o2" ); // 获取o2后先等一会儿，让Lock1有足够的时间锁住o1 Thread.sleep( 1000 ); // 接着获取o1对象锁 synchronized( o1 ) &#123; System.out.println( "l2 lock o1" ); &#125; &#125; catch( Exception e ) &#123; e.printStackTrace(); &#125; &#125; &#125; public static void main( String[] args ) &#123; Deadlock lock = new Deadlock(); Thread t1 = new Thread( new Runnable() &#123; @Override public void run() &#123; lock.lock1(); &#125; &#125; ); Thread t2 = new Thread( new Runnable() &#123; @Override public void run() &#123; lock.lock2(); &#125; &#125; ); t1.start(); t2.start(); &#125;&#125; 创建两个对象，两条线程，用synchronized锁住对象，线程1先锁对象1后锁对象2，线程2先锁对象2后锁对象1。假设线程1先锁对象1，然后休眠1秒，线程锁对象2，之后线程1就没法锁对象2，线程2也没法锁住对象1，双方都在等待对方释放自己在等待的锁。 “死锁”产生的原因及四个必要条件“死锁”的原因可归结为 竞争资源。当系统中供多个进程共享的资源如打印机、公用队列等，其数目不足以满足进程的需要时，会引起诸进程的竞争而产生死锁。 进程间推进顺序非法。进程在运行过程中，请求和释放资源的顺序不当，也同样会导致产生进程死锁。 产生“死锁”的四个必要条件 互斥（Mutual exclusion）：存在这样一种资源，它在某个时刻只能被分配给一个执行绪（也称为线程）使用； 持有（Hold and wait）：当请求的资源已被占用从而导致执行绪阻塞时，资源占用者不但没有释放该资源，而且还可以继续请求更多资源； 不可剥夺（No preemption）：执行绪获得到的互斥资源不可被强行剥夺，换句话说，只有资源占用者自己才能释放资源； 环形等待（Circular wait）：若干执行绪以不同的次序获取互斥资源，从而形成环形等待的局面，想象在由多个执行绪组成的环形链中，每个执行绪都在等待下一个执行绪释放它持有的资源。 结合代码例子理解“死锁”的产生 互斥：两条线程各自占有的锁 持有：线程1持有线程2想要获得的锁1，线程2持有线程1想要的锁2，双方都没有 释放各自占有的对象锁，并且继续请求对方占有的锁 不可剥夺：两条线程得到互斥资源都没法被强行剥夺 环形等待：T1{O1}→→T2{O2}→→T1{O1}，{}表示被左边的线程占有{}里的资源，→→表示左边线程申请（等待）右边线程释放其占有的资源PS：环形等待可以是多个线程对多个资源的争夺 “死锁”问题定位获取java进程ID1ps aux | grep "java" 用jstack看进程堆栈12# 替换进程ID（pid）jstack -l &#123;pid&#125; | grep -A50 -B10 "deadlock" “死锁”的预防和解除理解了死锁的原因，尤其是产生死锁的四个必要条件，就可以最大可能地避免、预防和解除死锁，消除产生死锁的四个必要条件中的任何一个都可以预防和解除死锁。不难看出，在死锁的四个必要条件中，第二、三和四项条件比较容易消除。 静态分配：采用资源静态分配策略（进程资源静态分配方式是指一个进程在建立时就分配了它需要的全部资源），破坏”部分分配”条件； 可剥夺：允许进程剥夺使用其他进程占有的资源，从而破坏”不可剥夺”条件； 有序分配：采用资源有序分配法，破坏”环路”条件 参考资料“死锁”四个必要条件的合理解释]]></content>
      <tags>
        <tag>线程</tag>
        <tag>并发</tag>
        <tag>锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis基本数据结构-SDS]]></title>
    <url>%2F2018%2F10%2F14%2FRedis-DataStructure-1-String%2F</url>
    <content type="text"><![CDATA[String&emsp;&emsp;String是我们最常用的Redis基本数据结构之一。Redis没有直接使用C语言传统的字符串表示（以空字符结尾的字符数组，以下简称C字符串），而是自己构建了一种名为简单动态字符串（simple dynamic string，SDS）的抽象类型，并将SDS用作Redis的默认字符串表示。&emsp;&emsp;在Redis里边，C字符串只会作为字符串字面量用在一些无须对字符串值进行修改的地方，比如打印日志：1redisLog(REDIS_WARNING, &quot;Redis is now ready to exit, bye bye...&quot;); SDS与C字符串的区别常数级复杂度获取字符串长度&emsp;&emsp;C语言使用长度为N+1的字符串来表示长度为N的字符串，并且字符数组的最后一个元素总是空字符’\0’&emsp;&emsp;因为C字符串并不记录自身的长度信息，所以为了获取一个C字符串的长度，程序必须便利整个字符数组，对遇到的每个字符进行技术，直到遇到代表字符串结尾的空字符为止，这个操作的复杂度为O(N)&emsp;&emsp;和C字符串不同，因为SDS在len属性中记录了SDS本身的长度，所以获取一个SDS长度的复杂度仅为O(1)，这确保了获取字符串长度的操作不会成为Redis的性能瓶颈。譬如”Redis”的长度为5，程序只需要访问SDS的len属性就可以立即得到长度值为5字节 杜绝缓冲区溢出&emsp;&emsp;由于C字符串不记录自身长度，会带来另一个问题，就是容易造成缓冲区溢出。1char *strcat(char *dest, const char *src) &emsp;&emsp;假定用户在执行strcat函数时，已经为dest分配了足够多的内存，则可以容纳src字符串中的所有内容，而一旦这个假定不成立时，就会产生缓冲区溢出。&emsp;&emsp;举个例子，假设程序里有两个在内存中紧邻着的C字符串s1和s2，其中s1保存了字符串”Redis”，而s2则保存了字符串”MongoDB”，如图所示： &emsp;&emsp;如果此时要通过执行：1strcat(s1, &quot; Cluster&quot;); 将s1的内容修改为”Redis Cluster”，但如果粗心的他却忘了在执行strcat之前为s1分配足够的空间，那么在strcat函数执行之后，s1的数据将溢出到s2所在的空间中，导致s2的内容被意外地修改了，如图所示： &emsp;&emsp;SDS的空间分配策略则完全杜绝了发生这种情况的可能性：当SDS API需要对SDS的内容进行修改时，API会先检查SDS的空间是否满足修改所需的要求，如果不满足的话，API会自动把SDS的空间扩展至执行修改所需的大小，然后才执行实际的修改操作，所以使用SDS既不需要手动修改SDS的空间大小，也不会出现上述的缓冲区溢出问题。 减少修改字符串时带来的内容重分配次数&emsp;&emsp;因为C字符串并不记录自身的长度，所以一个C字符串的底层实现总是额外的多出一个字符空间用于保存空字符。因为C字符串的长度和底层数组长度之间存在着这种关联性，所以每次增长或缩短一个C字符串，都总会在保存这个C字符串的数组时引起一次内存重分配操作 &emsp;&emsp;而因为内存重分配涉及复杂的算法，并且可能需要执行系统调用，所以它通常是一个比较耗时的操作： 在一般程序中，如果修改字符串长度的情况不太常出现，那么每次修改都执行一次内存重分配是可以接受的，但Redis作为数据库，经常被用于速度要求严苛、数据被频繁修改的场合，如果每次修改字符串的长度都需要执行一次内存重分配的话，那么光是这个操作的时间就会占去修改字符串所用时间的一大部分，如果这种修改频繁地发生的话，可能还会对性能造成影响 &emsp;&emsp;为了避免C字符串的这种缺陷，SDS通过未使用空间解除了字符串长度和底层数组长度之间的关联：在SDS中，buf数组的长度不一定就是字符数加1，数组里边可以包含未使用的字节，而这些字节的数量就由SDS的free属性记录&emsp;&emsp;通过未使用空间，SDS实现了空间预分配和惰性空间释放两种优化策略： 空间预分配&emsp;&emsp;空间预分配用于优化SDS的字符串增长操作：利用额外的未使用空间进行预分配以减少内存的频繁分配，这一点类似Java中的ArrayList。&emsp;&emsp;当字符串长度小于 1M 时，扩容都是加倍现有的空间，如果超过 1M，扩容时一次只会多扩 1M 的空间。而字符串最大长度为 512M。譬如，假如SDS进行修改后变为13字节（小于1MB），那么此时SDS的buf数组的实际长度将变成13+13+1=27字节（额外的1字节用于保存空字符）。假如SDS进行修改后变为2MB（大于等于1MB），则程序将会分配1MB的未使用空间，也就是说，SDS的buf数组的实际长度将为2MB + 1MB + 1byte。 惰性空间释放&emsp;&emsp;当要缩短SDS保存的字符串时，程序并不立即使用内存充分配来回收缩短后多出来的字节，而是使用表头的free成员将这些字节记录起来以备用。 二进制安全&emsp;&emsp;SDS是二进制安全的，它可以存储任意二进制数据，因为SDS使用len属性的值而不是像C语言字符串那样以空字符（‘\0’）来标识字符串结束。 &emsp;&emsp;因为传统C字符串符合某种编码（比如ASCII），字符串不仅末尾，就连字符串里的内容也不能包含标记着结束的字符。如ASCII这种编码的操作的特点就是：遇零则止。即，当读一个字符串时，只要遇到’\0’结尾，就认为到达末尾，就忽略’\0’结尾以后的所有字符。因此，如果传统字符串保存图片、音频、视频等二进制文件，操作文件时就被截断了。 兼容部分C字符串函数&emsp;&emsp;虽然SDS的API都是二进制安全的，但它们一样遵循C字符串结尾的惯例：这些API总会将SDS保存的数据的末尾设置为空字符，并且总会在为buf数组分配空间时多分配一个字节来容纳这个空字符，这是为了让那些保存文本数据的SDS可以重用一部分&lt;string.h&gt;库定义的函数，避免不必要的代码重复。 总结]]></content>
      <tags>
        <tag>Redis</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo添加搜索功能]]></title>
    <url>%2F2018%2F10%2F14%2FHexo-Search%2F</url>
    <content type="text"><![CDATA[本文旨在记录站主基于hexo-generator-search插件实现本站的站内文章搜索功能 基本实现原理 基于hexo-generator-search生成全文内容索引xml文件 利用jQ.ajax请求xml文件并解析 jQ搜索关键字内容匹配xml内容主要的部分还是插件写的好，对应的解析函数也是改造插件作者的，网上一搜一大堆此类文章，本文仅仅意在记录本站使用该插件实现搜索的过程 安装插件1npm install --save hexo-generator-search 这个插件可以生成供搜索的索引数据，生成后的xml文件保存在自己站内目录，可以通过 http://localhost:4000/search.xml 查看 插件配置在hexo根目录底下的_config.xml里加入以下配置：1234search: path: search.xml field: post #field: post, page or all（3个可选参数） 解析函数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960var performLocalSearch = function(datas,keywords)&#123; // perform local searching var str='&lt;ul class="search-result-list"&gt;'; datas.forEach(function(data) &#123; var isMatch = true; var content_index = []; var data_title = data.title.trim().toLowerCase(); var data_content = data.content.trim().replace(/&lt;[^&gt;]+&gt;/g,"").toLowerCase(); var data_url = "/" + data.url; var index_title = -1; var index_content = -1; var first_occur = -1; // only match artiles with not empty titles and contents if(data_title != '' &amp;&amp; data_content != '') &#123; keywords.forEach(function(keyword, i) &#123; index_title = data_title.indexOf(keyword); index_content = data_content.indexOf(keyword); if( index_title &lt; 0 &amp;&amp; index_content &lt; 0 )&#123; isMatch = false; &#125; else &#123; if (index_content &lt; 0) &#123; index_content = 0; &#125; if (i == 0) &#123; first_occur = index_content; &#125; &#125; &#125;); &#125; // show search results if (isMatch) &#123; str += '&lt;li&gt;&lt;a href="'+ data_url +'" class="search-result-title" target="_blank"&gt;'+ '&gt; ' + data_title +'&lt;/a&gt;'; var content = data.content.trim().replace(/&lt;[^&gt;]+&gt;/g,""); if (first_occur &gt;= 0) &#123; // cut out characters var start = first_occur - 6; var end = first_occur + 6; if(start &lt; 0)&#123; start = 0; &#125; if(start == 0)&#123; end = 10; &#125; if(end &gt; content.length)&#123; end = content.length; &#125; var match_content = content.substr(start, end); // highlight all keywords keywords.forEach(function(keyword)&#123; var regS = new RegExp(keyword, "gi"); match_content = match_content.replace(regS, '&lt;em class=\"search-keyword\"&gt;'+keyword+'&lt;/em&gt;'); &#125;); str += '&lt;p class=\"search-result\"&gt;' + match_content +'...&lt;/p&gt;'; &#125; &#125; &#125;); return str; &#125; Search入口功能函数12345678910111213141516171819202122232425262728293031 var searchFunc = function(path, search_id, content_id) &#123; 'use strict'; $.ajax(&#123; url: path, dataType: "xml", success: function( xmlResponse ) &#123; // get the contents from search data var datas = $( "entry", xmlResponse ).map(function() &#123; return &#123; title: $( "title", this ).text(), content: $("content",this).text(), url: $( "url" , this).text() &#125;; &#125;).get(); var $input = $('#'+search_id); var $resultContent = $('#'+content_id); $input.on("input propertychange",function()&#123; var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/); if($(this).val()=="")&#123;$resultContent.html('');return;&#125; $resultContent.html(performLocalSearch(datas,keywords)); &#125;); &#125; &#125;); &#125; searchFunc("/search.xml","local-search-input","local-search-result");&#125;)(); 页面HTML代码申明html元素，id、class名字要跟解析函、Search入口功能函数里的代码对上号：12345 &lt;div id="site_search" class="bar"&gt; &lt;input type="text" id="local-search-input" name="q" results="0" placeholder="search my blog..." class="form-control"/&gt; &lt;div id="local-search-result"&gt;&lt;/div&gt; &lt;/div&gt;` 样式调整1234567891011121314151617181920212223242526272829303132ul.search-result-list &#123; padding-left: 10px;&#125;a.search-result-title &#123; font-weight: bold;&#125;p.search-result &#123; color=#555;&#125;em.search-keyword &#123; border-bottom: 1px dashed #4088b8; font-weight: bold;&#125;.form-control &#123; padding-left:10px; margin-bottom: 10px; &#125;.bar &#123; padding: 10px 10px;&#125;.bar input &#123; width:350px; height: 25px; border-radius:42px; border:2px solid #324B4E; background:#F9F0DA; transition:.3s linear; float:center;&#125;.bar input:focus &#123; width:420px;&#125;]]></content>
      <tags>
        <tag>HEXO</tag>
        <tag>搜索</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java线程池类比公司经营之道]]></title>
    <url>%2F2018%2F10%2F03%2FJava-ThreadPool-vs-Company%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;Java线程池的设计与公司经营的相似之处如果我们查看JDK源码，会发现FixedThreadPool、CachedThreadPool和SingleThreadExecutor都是通过创建一个ThreadPoolExcutor对象来实现的。我们来看一下该ThreadPoolExcutor的构造方法，并对线程池中线程的保留和新建策略做进一步的分析。 1public ThreadPoolExecutor( int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue) &emsp;&emsp;第一个参数corePoolSize代表了线程池中一定要保持的线程的数量；线程池中的线程可能发生变化，第二个参数maximumPoolSize约束了线程池中所能达到的线程的最大数量；线程有可能一直处于空闲状态，keepAliveTime代表了空闲状态的线程所能存活的时间；TimeUnit代表了时间单位；workQueue是一个缓冲队列，如果任务到达，但是还没有空闲线程可以执行该任务，那么就将该任务置于这个缓冲队列中。为了更加容易理解和记忆线程池这个几个属性的协调工作。我们利用一个精明的老板来比喻线程池。而将线程比作线程中的线程。 &emsp;&emsp;一个公司必须要保留一定数量的核心员工，不管这些员工是不是老闲着。当然，对于非常抠门的老板，这个数量可能是0，例如CachedThreadPool。核心员工的数量，就是corePoolSize。当一个公司初创时，所有的员工也就是那几个核心员工。当线程池新建时，同样只会创建与corePoolSize数量相当的线程。 &emsp;&emsp;当新的任务到达时，如果有空闲线程，马上将这些任务分配给空闲线程。如果没有的话，那么，怎么办呢？新建一个线程吗？非也，对于一个精明的老板来说，他只会把这些任务排进任务列表。手下的员工忙完手头的工作，马上就从任务列表的开头位置移出工作，并分配给空闲。这就让每名员工都不停的工作，甚至加班加点。这个任务列表就是workQueue。 &emsp;&emsp;如果更多的任务涌过来，如同这个公司的业务很好，工作多越堆越多。这个时候，就看任务列表能承受的极限了。有的老板在创立公司的时候，就抱着这种心态——任务列表可以无限长，反正我就招这么多人，客户能等就等，不能等就拉倒。但是，对于很多客户来说，如果等的时间过长，可能就放弃了。具有无限长workQueue的线程池来说，可能同样会导致某些线程等待时间过长，用户任务无响应的问题。 &emsp;&emsp;但是，如果workQueue不是无限长，那么，其容量总有可能被达到。而新的任务到达时，无法存入workQueue。这如同，这个老板既负责任（不想出现客户无限等待的情况），同时又不想放弃任何一个客户。那么，唯有增加员工数量了，这就如同线程池新建线程。但是，公司总要有个风险评估，不能让员工数量无限增长，于是，maximumPoolSize就代表了员工的最大数量。如同说，在无法两全其美的情形下，即使损失部分客户，也要控制公司的成本风险。线程池同样如此，每个线程都将消耗系统资源，这种消耗必须被控制在一定范围之内。 &emsp;&emsp;在大量任务涌入，workQueue无法缓存这些任务，而maxinumPoolSize也已经达到时，相当于一个公司达到了它的最大营运能力，就只能拒绝介绍客户任务了。线程池拒绝介绍新的任务，会抛出异常RejectedExecutionException。 &emsp;&emsp;当然，一个公司的营运既有旺季，也有淡季。上面我们所描述的情形是旺季的营运。如果淡季到了，许多员工都闲下来了。老板就会考虑裁员了。当然，老板不会马上动手，因为不能准确把握旺季和淡季的分界线。他会给空闲员工一个缓冲期，如果这个员工闲了三个月都没工作，那么证明，真的需要裁掉他了。对应到线程池中，keepAliveTime和TimeUnit限制了一个线程的最大空闲时间。相当于一个缓冲期，缓冲期一结束，就会将其销毁，以释放系统资源。当然，这些被“处理”的线程都是核心员工数量之外的，线程池总会保留corePoolSize个线程备用。 &emsp;&emsp;通过以上描述，我们应该对线程池的运作策略有了一个比较清晰的认识。总结这种策略，主要目的是基于成本考虑——尽量耗用最少的内存，来完成尽可能多的任务。]]></content>
      <tags>
        <tag>线程</tag>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo Hello World]]></title>
    <url>%2F2018%2F10%2F03%2FHexo-First-Guide%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <tags>
        <tag>HEXO</tag>
      </tags>
  </entry>
</search>
